<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Apache Mesos</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Fundamentals</li><li class="chapter-item expanded "><a href="architecture.html"><strong aria-hidden="true">1.</strong> Mesos Architecture providing an overview of Mesos concepts</a></li><li class="chapter-item expanded "><a href="presentations.html"><strong aria-hidden="true">2.</strong> Video and Slides of Mesos Presentations</a></li><li class="chapter-item expanded "><a href="versioning.html"><strong aria-hidden="true">3.</strong> Mesos Release and Support Policy</a></li><li class="chapter-item expanded affix "><li class="part-title">Build / Installation</li><li class="chapter-item expanded "><a href="building.html"><strong aria-hidden="true">4.</strong> Building for basic instructions on compiling and installing Mesos.</a></li><li class="chapter-item expanded "><a href="binary-packages.html"><strong aria-hidden="true">5.</strong> Binary Packages for how to use Mesos binary packages.</a></li><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">6.</strong> Configuration for build configuration options.</a></li><li class="chapter-item expanded "><a href="cmake.html"><strong aria-hidden="true">7.</strong> CMake for details about using the new CMake build system.</a></li><li class="chapter-item expanded "><a href="windows.html"><strong aria-hidden="true">8.</strong> Windows Support for the state of Windows support in Mesos.</a></li><li class="chapter-item expanded affix "><li class="part-title">Administration</li><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">9.</strong> Configuration for command-line arguments.</a></li><li class="chapter-item expanded "><a href="high-availability.html"><strong aria-hidden="true">10.</strong> High Availability Master Setup</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="replicated-log-internals.html"><strong aria-hidden="true">10.1.</strong> Replicated Log for information on the Mesos replicated log.</a></li></ol></li><li class="chapter-item expanded "><a href="agent-recovery.html"><strong aria-hidden="true">11.</strong> Fault Tolerant Agent Setup</a></li><li class="chapter-item expanded "><a href="framework-rate-limiting.html"><strong aria-hidden="true">12.</strong> Framework Rate Limiting</a></li><li class="chapter-item expanded "><a href="maintenance.html"><strong aria-hidden="true">13.</strong> Maintenance for performing maintenance on a Mesos cluster.</a></li><li class="chapter-item expanded "><a href="upgrades.html"><strong aria-hidden="true">14.</strong> Upgrades for upgrading a Mesos cluster.</a></li><li class="chapter-item expanded "><a href="downgrades.html"><strong aria-hidden="true">15.</strong> Downgrades for downgrading a Mesos cluster.</a></li><li class="chapter-item expanded "><a href="logging.html"><strong aria-hidden="true">16.</strong> Logging</a></li><li class="chapter-item expanded "><a href="monitoring.html"><strong aria-hidden="true">17.</strong> Monitoring / Metrics</a></li><li class="chapter-item expanded "><a href="cli.html"><strong aria-hidden="true">18.</strong> Debugging using the new CLI</a></li><li class="chapter-item expanded "><a href="operational-guide.html"><strong aria-hidden="true">19.</strong> Operational Guide</a></li><li class="chapter-item expanded "><a href="fetcher.html"><strong aria-hidden="true">20.</strong> Fetcher Cache Configuration</a></li><li class="chapter-item expanded "><a href="fault-domains.html"><strong aria-hidden="true">21.</strong> Fault Domains</a></li><li class="chapter-item expanded "><a href="performance-profiling.html"><strong aria-hidden="true">22.</strong> Performance Profiling for debugging performance issues in Mesos.</a></li><li class="chapter-item expanded "><a href="memory-profiling.html"><strong aria-hidden="true">23.</strong> Memory Profiling for debugging potential memory leaks in Mesos.</a></li><li class="chapter-item expanded affix "><li class="part-title">Resource Management</li><li class="chapter-item expanded "><a href="attributes-resources.html"><strong aria-hidden="true">24.</strong> Attributes and Resources for how to describe the agents that comprise a cluster.</a></li><li class="chapter-item expanded "><a href="roles.html"><strong aria-hidden="true">25.</strong> Using Resource Roles</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="weights.html"><strong aria-hidden="true">25.1.</strong> Resource Role Weights for fair sharing.</a></li><li class="chapter-item expanded "><a href="quota.html"><strong aria-hidden="true">25.2.</strong> Resource Role Quota for how to configure Mesos to provide guaranteed resource allocations for use by a role.</a></li><li class="chapter-item expanded "><a href="reservation.html"><strong aria-hidden="true">25.3.</strong> Reservations for how operators and frameworks can reserve resources on individual agents for use by a role.</a></li><li class="chapter-item expanded "><a href="shared-resources.html"><strong aria-hidden="true">25.4.</strong> Shared Resources for how to share persistent volumes between tasks managed by different executors on the same agent.</a></li></ol></li><li class="chapter-item expanded "><a href="oversubscription.html"><strong aria-hidden="true">26.</strong> Oversubscription for how to configure Mesos to take advantage of unused resources to launch “best-effort” tasks.</a></li><li class="chapter-item expanded affix "><li class="part-title">Security</li><li class="chapter-item expanded "><a href="authentication.html"><strong aria-hidden="true">27.</strong> Authentication</a></li><li class="chapter-item expanded "><a href="authorization.html"><strong aria-hidden="true">28.</strong> Authorization</a></li><li class="chapter-item expanded "><a href="ssl.html"><strong aria-hidden="true">29.</strong> SSL</a></li><li class="chapter-item expanded "><a href="secrets.html"><strong aria-hidden="true">30.</strong> Secrets for managing secrets within Mesos.</a></li><li class="chapter-item expanded affix "><li class="part-title">Containerization</li><li class="chapter-item expanded "><a href="containerizers.html"><strong aria-hidden="true">31.</strong> Containerizer Overview</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="containerizer-internals.html"><strong aria-hidden="true">31.1.</strong> Containerizer Internals for implementation details of containerizers.</a></li><li class="chapter-item expanded "><a href="docker-containerizer.html"><strong aria-hidden="true">31.2.</strong> Docker Containerizer for launching a Docker image as a Task, or as an Executor.</a></li><li class="chapter-item expanded "><a href="mesos-containerizer.html"><strong aria-hidden="true">31.3.</strong> Mesos Containerizer default containerizer, supports both Linux and POSIX systems.</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="container-image.html"><strong aria-hidden="true">31.3.1.</strong> Container Images for supporting container images in Mesos containerizer.</a></li><li class="chapter-item expanded "><a href="isolators/docker-volume.html"><strong aria-hidden="true">31.3.2.</strong> Docker Volume Support</a></li><li class="chapter-item expanded "><a href="gpu-support.html"><strong aria-hidden="true">31.3.3.</strong> Nvidia GPU Support for how to run Mesos with Nvidia GPU support.</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="sandbox.html"><strong aria-hidden="true">32.</strong> Container Sandboxes</a></li><li class="chapter-item expanded "><a href="container-volume.html"><strong aria-hidden="true">33.</strong> Container Volumes</a></li><li class="chapter-item expanded "><a href="nested-container-and-task-group.html"><strong aria-hidden="true">34.</strong> Nested Container and Task Group (Pod)</a></li><li class="chapter-item expanded "><a href="standalone-containers.html"><strong aria-hidden="true">35.</strong> Standalone Containers</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Apache Mesos</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/AVENTER-UG/mesos-docs/" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="mesos-architecture"><a class="header" href="#mesos-architecture">Mesos Architecture</a></h1>
<p><img src="images/architecture3.jpg" alt="Mesos Architecture" /></p>
<p>The above figure shows the main components of Mesos.  Mesos consists of a <em>master</em> daemon that manages <em>agent</em> daemons running on each cluster node, and <em>Mesos frameworks</em> that run <em>tasks</em> on these agents.</p>
<p>The master enables fine-grained sharing of resources (CPU, RAM, ...) across
frameworks by making them <em>resource offers</em>. Each resource offer contains a list
of <code>&lt;agent ID, resource1: amount1, resource2: amount2, ...&gt;</code> (NOTE: as
keyword 'slave' is deprecated in favor of 'agent', driver-based frameworks will
still receive offers with slave ID, whereas frameworks using the v1 HTTP API receive offers with agent ID). The master decides <em>how many</em> resources to offer to each framework according to a given organizational policy, such as fair sharing or strict priority. To support a diverse set of policies, the master employs a modular architecture that makes it easy to add new allocation modules via a plugin mechanism.</p>
<p>A framework running on top of Mesos consists of two components: a <em>scheduler</em> that registers with the master to be offered resources, and an <em>executor</em> process that is launched on agent nodes to run the framework's tasks (see the <a href="app-framework-development-guide.html">App/Framework development guide</a> for more details about framework schedulers and executors). While the master determines <strong>how many</strong> resources are offered to each framework, the frameworks' schedulers select <strong>which</strong> of the offered resources to use. When a framework accepts offered resources, it passes to Mesos a description of the tasks it wants to run on them. In turn, Mesos launches the tasks on the corresponding agents.</p>
<h2 id="example-of-resource-offer"><a class="header" href="#example-of-resource-offer">Example of resource offer</a></h2>
<p>The figure below shows an example of how a framework gets scheduled to run a task.</p>
<p><img src="images/architecture-example.jpg" alt="Mesos Architecture" /></p>
<p>Let's walk through the events in the figure.</p>
<ol>
<li>Agent 1 reports to the master that it has 4 CPUs and 4 GB of memory free. The master then invokes the allocation policy module, which tells it that framework 1 should be offered all available resources.</li>
<li>The master sends a resource offer describing what is available on agent 1 to framework 1.</li>
<li>The framework's scheduler replies to the master with information about two tasks to run on the agent, using &lt;2 CPUs, 1 GB RAM&gt; for the first task, and &lt;1 CPUs, 2 GB RAM&gt; for the second task.</li>
<li>Finally, the master sends the tasks to the agent, which allocates appropriate resources to the framework's executor, which in turn launches the two tasks (depicted with dotted-line borders in the figure). Because 1 CPU and 1 GB of RAM are still unallocated, the allocation module may now offer them to framework 2.</li>
</ol>
<p>In addition, this resource offer process repeats when tasks finish and new resources become free.</p>
<p>While the thin interface provided by Mesos allows it to scale and allows the frameworks to evolve independently, one question remains: how can the constraints of a framework be satisfied without Mesos knowing about these constraints? For example, how can a framework achieve data locality without Mesos knowing which nodes store the data required by the framework? Mesos answers these questions by simply giving frameworks the ability to <strong>reject</strong> offers. A framework will reject the offers that do not satisfy its constraints and accept the ones that do.  In particular, we have found that a simple policy called delay scheduling, in which frameworks wait for a limited time to acquire nodes storing the input data, yields nearly optimal data locality.</p>
<p>You can also read much more about the Mesos architecture in this <a href="https://www.usenix.org/conference/nsdi11/mesos-platform-fine-grained-resource-sharing-data-center">technical paper</a>.</p>
<h1 id="video-and-slides-of-mesos-presentations"><a class="header" href="#video-and-slides-of-mesos-presentations">Video and Slides of Mesos Presentations</a></h1>
<p><em>(Listed in reverse chronological order)</em></p>
<h2 id="mesoscon-north-america-2018"><a class="header" href="#mesoscon-north-america-2018">MesosCon North America 2018</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PL-cRvJ6sAbfjvQCLT3ktrpnVwrJKTodfm">Video playlist</a> + <a href="https://mesoscon18.sched.com/">Slides</a></p>
<h2 id="jolt-running-distributed-fault-tolerant-tests-at-scale-using-mesos"><a class="header" href="#jolt-running-distributed-fault-tolerant-tests-at-scale-using-mesos">Jolt: Running Distributed, Fault-Tolerant Tests at Scale using Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=2uGwlVs8Cpw">Video</a>
Sunil Shah, Kyle Kelly, and Timmy Zhu Presented November 1, 2017 at <a href="https://www.meetup.com/Bay-Area-Mesos-User-Group/events/244469969/">Bay Area Mesos User Group Meetup</a></p>
<h2 id="mesoscon-europe-2017"><a class="header" href="#mesoscon-europe-2017">MesosCon Europe 2017</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6rSBqPhTh_lmeMmxn6AOSjf">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2017/mesoscon-europe/program/slides">Slides</a></p>
<h2 id="mesoscon-north-america-2017"><a class="header" href="#mesoscon-north-america-2017">MesosCon North America 2017</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6qAEnkhkh5tGI6oX_xXD3X4">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2017/mesoscon-north-america/program/slides">Slides</a></p>
<h2 id="mesoscon-asia-2017"><a class="header" href="#mesoscon-asia-2017">MesosCon Asia 2017</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6rZfzCL_b-W9yxcJQhZ0RUg">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2017/mesoscon-asia/program/slides">Slides</a></p>
<h2 id="mesoscon-asia-2016"><a class="header" href="#mesoscon-asia-2016">MesosCon Asia 2016</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6pLSHrXSg7IYgzSlkOh132K">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2016/mesoscon-asia/program/slides">Slides</a></p>
<h2 id="mesoscon-europe-2016"><a class="header" href="#mesoscon-europe-2016">MesosCon Europe 2016</a></h2>
<p><a href="http://events17.linuxfoundation.org/events/archive/2016/mesoscon-europe/program/slides">Slides</a></p>
<h2 id="mesoscon-north-america-2016"><a class="header" href="#mesoscon-north-america-2016">MesosCon North America 2016</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLGeM09tlguZQVL7ZsfNMffX9h1rGNVqnC">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2016/mesoscon-north-america/program/slides">Slides</a></p>
<h2 id="mesoscon-europe-2015"><a class="header" href="#mesoscon-europe-2015">MesosCon Europe 2015</a></h2>
<p><a href="https://www.youtube.com/watch?v=K-x7yOy8Ymk&amp;list=PLGeM09tlguZS6MhlSZDbf-gANWdKgje0I">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2015/mesoscon-europe/program/slides">Slides</a></p>
<h2 id="mesoscon-north-america-2015"><a class="header" href="#mesoscon-north-america-2015">MesosCon North America 2015</a></h2>
<p><a href="https://www.youtube.com/watch?v=aV6pdWveN7s&amp;list=PLVjgeV_avap2arug3vIz8c6l72rvh9poV">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2015/mesoscon-north-america/program/slides">Slides</a></p>
<h2 id="building-and-deploying-applications-to-apache-mesos"><a class="header" href="#building-and-deploying-applications-to-apache-mesos">Building and Deploying Applications to Apache Mesos</a></h2>
<p><a href="https://www.slideshare.net/charmalloc/buildingdeployingapplicationsmesos">Slides</a>
Joe Stein
Presented February 26, 2015 at <a href="http://www.meetup.com/DigitalOcean_Community/events/220580767/">DigitalOcean Community Meetup</a></p>
<h2 id="mesoscon-2014"><a class="header" href="#mesoscon-2014">MesosCon 2014</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLDVc2EaAVPg9kp8cFzjR1Yxj96I4U5EGN">Video playlist</a></p>
<h2 id="datacenter-computing-with-apache-mesos"><a class="header" href="#datacenter-computing-with-apache-mesos">Datacenter Computing with Apache Mesos</a></h2>
<p><a href="http://www.slideshare.net/pacoid/datacenter-computing-with-apache-mesos">Slides</a>
Paco Nathan
Presented April 15, 2014 at <a href="http://www.meetup.com/bigdatadc/events/172610652/">Big Data DC Meetup</a></p>
<h2 id="apache-spark-at-viadeo-running-on-mesos"><a class="header" href="#apache-spark-at-viadeo-running-on-mesos">Apache Spark at Viadeo (Running on Mesos)</a></h2>
<p><a href="http://www.youtube.com/watch?v=shaZslr49vQ&amp;t=16m55s">Video</a> + <a href="https://speakerdeck.com/ecepoi/apache-spark-at-viadeo">Slides</a>
Eugen Cepoi
Presented April 9, 2014 at Paris Hadoop User Group</p>
<h2 id="mesos-hubspot-and-singularity"><a class="header" href="#mesos-hubspot-and-singularity">Mesos, HubSpot, and Singularity</a></h2>
<p><a href="https://www.youtube.com/watch?v=ROn14csiikw">Video</a>
Tom Petr
Presented April 3rd, 2014 at @TwitterOSS #conf</p>
<h2 id="building-distributed-frameworks-on-mesos"><a class="header" href="#building-distributed-frameworks-on-mesos">Building Distributed Frameworks on Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=n5GT7OFSh58">Video</a>
Benjamin Hindman
Presented March 25th, 2014 at <a href="https://www.eventbrite.com/e/aurora-and-mesosframeworksmeetup-tickets-10850994617">Aurora and Mesos Frameworks Meetup</a></p>
<h2 id="introduction-to-apache-aurora"><a class="header" href="#introduction-to-apache-aurora">Introduction to Apache Aurora</a></h2>
<p><a href="https://www.youtube.com/watch?v=asd_h6VzaJc">Video</a>
Bill Farner
Presented March 25th, 2014 at <a href="https://www.eventbrite.com/e/aurora-and-mesosframeworksmeetup-tickets-10850994617">Aurora and Mesos Frameworks Meetup</a></p>
<h2 id="improving-resource-efficiency-with-apache-mesos"><a class="header" href="#improving-resource-efficiency-with-apache-mesos">Improving Resource Efficiency with Apache Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=YpmElyi94AA">Video</a>
Christina Delimitrou
Presented April 3rd, 2014 at @TwitterOSS #conf</p>
<h2 id="apache-mesos-as-an-sdk-for-building-distributed-frameworks"><a class="header" href="#apache-mesos-as-an-sdk-for-building-distributed-frameworks">Apache Mesos as an SDK for Building Distributed Frameworks</a></h2>
<p><a href="http://www.slideshare.net/pacoid/strata-sc-2014-apache-mesos-as-an-sdk-for-building-distributed-frameworks">Slides</a>
Paco Nathan
Presented February 13th, 2014 at <a href="http://strataconf.com/">Strata</a></p>
<h2 id="run-your-data-center-like-googles-with-apache-mesos"><a class="header" href="#run-your-data-center-like-googles-with-apache-mesos">Run your Data Center like Google's with Apache Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=2YWVGMuMTrg">Video and Demo</a>
Abhishek Parolkar
Presented November 14th, 2013 at <a href="http://www.cloudexpoasia.com/">Cloud Expo Asia 2013</a></p>
<h2 id="datacenter-management-with-mesos"><a class="header" href="#datacenter-management-with-mesos">Datacenter Management with Mesos</a></h2>
<p><a href="http://www.youtube.com/watch?v=YB1VW0LKzJ4">Video</a>
Benjamin Hindman
Presented August 29th, 2013 at <a href="http://ampcamp.berkeley.edu/3/">AMP Camp</a></p>
<h2 id="building-a-framework-on-mesos-a-case-study-with-jenkins"><a class="header" href="#building-a-framework-on-mesos-a-case-study-with-jenkins">Building a Framework on Mesos: A Case Study with Jenkins</a></h2>
<p><a href="http://www.youtube.com/watch?v=TPXw_lMTJVk">Video</a>
Vinod Kone
Presented July 25, 2013 at <a href="http://www.meetup.com/Distributed-data-processing-with-Mesos/events/128585772/">SF Mesos Meetup</a></p>
<h2 id="hadoop-on-mesos"><a class="header" href="#hadoop-on-mesos">Hadoop on Mesos</a></h2>
<p><a href="http://www.youtube.com/watch?v=SFj5EMw8THk">Video</a>
Brenden Matthews
Presented July 25, 2013 at <a href="http://www.meetup.com/Distributed-data-processing-with-Mesos/events/128585772/">SF Mesos Meetup</a></p>
<h2 id="introduction-to-apache-mesos"><a class="header" href="#introduction-to-apache-mesos">Introduction to Apache Mesos</a></h2>
<p><a href="https://speakerdeck.com/benh/apache-mesos-nyc-meetup">Slides</a>
Benjamin Hindman
Presented August 20, 2013 at <a href="https://mesos-nyc-aug2013.eventbrite.com/">NYC Mesos Meetup</a></p>
<h2 id="chronos-a-distributed-fault-tolerant-and-highly-available-job-orchestration-framework-for-mesos"><a class="header" href="#chronos-a-distributed-fault-tolerant-and-highly-available-job-orchestration-framework-for-mesos">Chronos: A Distributed, Fault-Tolerant and Highly Available Job Orchestration Framework for Mesos</a></h2>
<p><a href="https://speakerdeck.com/mesos/chronos-august-2013-nyc-meetup">Slides</a>
Florian Leibert
Presented August 20, 2013 at <a href="https://mesos-nyc-aug2013.eventbrite.com/">NYC Mesos Meetup</a></p>
<h2 id="airbnb-tech-talk"><a class="header" href="#airbnb-tech-talk">Airbnb Tech Talk</a></h2>
<p><a href="http://www.youtube.com/watch?v=Hal00g8o1iY">Video</a>
Benjamin Hindman Presented September 6, 2012 at <a href="http://airbnb.com">Airbnb</a></p>
<h2 id="managing-twitter-clusters-with-mesos"><a class="header" href="#managing-twitter-clusters-with-mesos">Managing Twitter Clusters with Mesos</a></h2>
<p><a href="http://www.youtube.com/watch?v=37OMbAjnJn0">Video</a>
Benjamin Hindman Presented August 22, 2012 at <a href="http://ampcamp.berkeley.edu">AMP Camp</a></p>
<h2 id="mesos-a-platform-for-fine-grained-resource-sharing-in-datacenters"><a class="header" href="#mesos-a-platform-for-fine-grained-resource-sharing-in-datacenters">Mesos: A Platform for Fine-Grained Resource Sharing in Datacenters</a></h2>
<p><a href="http://www.youtube.com/watch?v=dB8IDu7g9Nc">Video</a>
Matei Zaharia
Presented March 2011 at <a href="http://berkeley.edu">UC Berkeley</a></p>
<h2 id="mesos-efficiently-sharing-the-datacenter"><a class="header" href="#mesos-efficiently-sharing-the-datacenter">Mesos: Efficiently Sharing the Datacenter</a></h2>
<p><a href="http://vimeo.com/17821090">Video</a>
Benjamin Hindman
Presented November 8, 2010 at <a href="http://linkedin.com">LinkedIn</a></p>
<h2 id="mesos-a-resource-management-platform-for-hadoop-and-big-data-clusters"><a class="header" href="#mesos-a-resource-management-platform-for-hadoop-and-big-data-clusters">Mesos: A Resource Management Platform for Hadoop and Big Data Clusters</a></h2>
<p><a href="http://www.youtube.com/watch?v=lE3jR6nM3bw">Video</a>
Matei Zaharia
Presented Summer 2010 at <a href="http://yahoo.com">Yahoo</a></p>
<h1 id="apache-mesos---paid-training"><a class="header" href="#apache-mesos---paid-training">Apache Mesos - Paid Training</a></h1>
<h2 id="automated-machine-learning-pipeline-with-mesos"><a class="header" href="#automated-machine-learning-pipeline-with-mesos">Automated Machine Learning Pipeline with Mesos</a></h2>
<p><a href="https://www.packtpub.com/big-data-and-business-intelligence/automated-machine-learning-pipeline-mesos-integrated-course">Video</a>
Karl Whitford
Packt (November 2017)</p>
<h2 id="docker-apache-mesos--dcos-run-and-manage-cloud-datacenter-a-hrefhttpswwwpacktpubcomnetworking-and-serversdocker-apache-mesos-dcos-run-and-manage-cloud-datacenter-videovideoa"><a class="header" href="#docker-apache-mesos--dcos-run-and-manage-cloud-datacenter-a-hrefhttpswwwpacktpubcomnetworking-and-serversdocker-apache-mesos-dcos-run-and-manage-cloud-datacenter-videovideoa">Docker, Apache Mesos &amp; DCOS: Run and manage cloud datacenter (<a href="https://www.packtpub.com/networking-and-servers/docker-apache-mesos-dcos-run-and-manage-cloud-datacenter-video">Video</a>)</a></h2>
<p>Manuj Aggarwal
Packt (January 2018) </p>
<h1 id="mesos-release-and-support-policy"><a class="header" href="#mesos-release-and-support-policy">Mesos Release and Support policy</a></h1>
<p>The Mesos versioning and release policy gives operators and developers clear guidelines on:</p>
<ul>
<li>Making modifications to the existing APIs without affecting backward compatibility.</li>
<li>How long a Mesos API will be supported.</li>
<li>Upgrading a Mesos installation across release versions.</li>
</ul>
<p>This document describes the release strategy for Mesos post 1.0.0 release.</p>
<h2 id="release-schedule"><a class="header" href="#release-schedule">Release Schedule</a></h2>
<p>Mesos releases are time-based, though we do make limited adjustments to the release schedule to accommodate feature development. This gives users and developers a predictable cadence to consume and produce features, while ensuring that each release can include the developments that users are waiting for.</p>
<p>If a feature is not ready by the time a release is cut, that feature should be disabled. This means that features should be developed in such a way that they are opt-in by default and can be easily disabled (e.g., flag).</p>
<p>A new Mesos release is cut approximately every <strong>3 months</strong>. The versioning scheme is <a href="http://semver.org">SemVer</a>. Typically, the minor release version is incremented by 1 (e.g., 1.1, 1.2, 1.3 etc) for every release, unless it is a major release.</p>
<p>Every (minor) release is a stable release and recommended for production use. This means a release candidate will go through rigorous testing (unit tests, integration tests, benchmark tests, cluster tests, scalability, etc.) before being officially released. In the rare case that a regular release is not deemed stable, a patch release will be released that will stabilize it.</p>
<p>At any given time, 3 releases are supported: the latest release and the two prior. Support means fixing of <em>critical issues</em> that affect the release. Once an issue is deemed critical, it will be fixed in only those <strong>affected</strong> releases that are still <strong>supported</strong>. This is called a patch release and increments the patch version by 1 (e.g., 1.2.1). Once a release reaches End Of Life (i.e., support period has ended), no more patch releases will be made for that release. Note that this is not related to backwards compatibility guarantees and deprecation periods (discussed later).</p>
<p>Which issues are considered critical?</p>
<ul>
<li>Security fixes</li>
<li>Compatibility regressions</li>
<li>Functional regressions</li>
<li>Performance regressions</li>
<li>Fixes for 3rd party integration (e.g., Docker remote API)</li>
</ul>
<p>Whether an issue is considered critical or not is sometimes subjective. In some cases it is obvious and sometimes it is fuzzy. Users should work with committers to figure out the criticality of an issue and get agreement and commitment for support.</p>
<p>Patch releases are normally done <strong>once per month</strong>.</p>
<p>If a particular issue is affecting a user and the user cannot wait until the next scheduled patch release, they can request an off-schedule patch release for a specific supported version. This should be done by sending an email to the dev list.</p>
<h2 id="upgrades"><a class="header" href="#upgrades">Upgrades</a></h2>
<p>All stable releases will be loosely compatible. Loose compatibility means:</p>
<ul>
<li>Master or agent can be upgraded to a new release version as long as they or the ecosystem components (scheduler, executor, zookeeper, service discovery layer, monitoring etc) do not depend on deprecated features (e.g., deprecated flags, deprecated metrics).</li>
<li>There should be no unexpected effect on externally visible behavior that is not deprecated. See API compatibility section for what should be expected for Mesos APIs.</li>
</ul>
<blockquote>
<p>NOTE: The compatibility guarantees do not apply to modules yet. See Modules section below for details.</p>
</blockquote>
<p>This means users should be able to upgrade (as long as they are not depending on deprecated / removed features) Mesos master or agent from a stable release version N directly to another stable release version M without having to go through intermediate release versions. For the purposes of upgrades, a stable release means the release with the latest patch version. For example, among 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1 releases 1.2.1, 1.3.0 and 1.4.1 are considered stable and so a user should be able to upgrade from 1.2.1 directly to 1.4.1. Look at the API compatability section below for how frameworks can do seamless upgrades.</p>
<p>The deprecation period for any given feature will be <strong>6 months</strong>. Having a set period allows Mesos developers to not indefinitely accrue technical debt and allows users time to plan for upgrades.</p>
<p>The detailed information about upgrading to a particular Mesos version would be posted <a href="upgrades.html">here</a>.</p>
<h2 id="api-versioning"><a class="header" href="#api-versioning">API versioning</a></h2>
<p>The Mesos APIs (constituting Scheduler, Executor, Internal, Operator/Admin APIs) will have a version in the URL. The versioned URL will have a prefix of <strong><code>/api/vN</code></strong> where &quot;N&quot; is the version of the API. The &quot;/api&quot; prefix is chosen to distinguish API resources from Web UI paths.</p>
<p>Examples:</p>
<ul>
<li>http://localhost:5050/api/v1/scheduler :  Scheduler HTTP API hosted by the master.</li>
<li>http://localhost:5051/api/v1/executor  :  Executor HTTP API hosted by the agent.</li>
</ul>
<p>A given Mesos installation might host multiple versions of the same API i.e., Scheduler API v1 and/or v2 etc.</p>
<h3 id="api-version-vs-release-version"><a class="header" href="#api-version-vs-release-version">API version vs Release version</a></h3>
<ul>
<li>To keep things simple, the stable version of the API will correspond to the major release version of Mesos.
<ul>
<li>For example, v1 of the API will be supported by Mesos release versions 1.0.0, 1.4.0, 1.20.0 etc.</li>
</ul>
</li>
<li>vN version of the API might also be supported by release versions of N-1 series but the vN API is not considered stable until the last release version of N-1 series.</li>
<li>For example, v2 of the API might be introduced in Mesos 1.12.0 release but it is only considered stable in Mesos 1.21.0 release if it is the last release of &quot;1&quot; series. Note that all Mesos 1.x.y versions will still support v1 of the API.</li>
<li>The API version is only bumped if we need to make a backwards <a href="versioning.html#api-compatibility">incompatible</a> API change. We will strive to support a given API version for at least a year.</li>
<li>The deprecation clock for vN-1 API will start as soon as we release &quot;N.0.0&quot; version of Mesos. We will strive to give enough time (e.g., 6 months) for frameworks/operators to upgrade to vN API before we stop supporting vN-1 API.</li>
</ul>
<p><a name="api-compatibility"></a></p>
<h3 id="api-compatibility"><a class="header" href="#api-compatibility">API Compatibility</a></h3>
<p>The API compatibility is determined by the corresponding protobuf guarantees.</p>
<p>As an example, the following are considered &quot;backwards compatible&quot; changes for Scheduler API:</p>
<ul>
<li>Adding new types of Calls i.e., new types of HTTP requests to &quot;/scheduler&quot;.</li>
<li>Adding new optional fields to existing requests to &quot;/scheduler&quot;.</li>
<li>Adding new types of Events i.e., new types of chunks streamed on &quot;/scheduler&quot;.</li>
<li>Adding new header fields to chunked response streamed on &quot;/scheduler&quot;.</li>
<li>Adding new fields (or changing the order of fields) to chunks' body streamed on &quot;/scheduler&quot;.</li>
<li>Adding new API resources (e.g., &quot;/foobar&quot;).</li>
</ul>
<p>The following are considered backwards incompatible changes for Scheduler API:</p>
<ul>
<li>Adding new required fields to existing requests to &quot;/scheduler&quot;.</li>
<li>Renaming/removing fields from existing requests to &quot;/scheduler&quot;.</li>
<li>Renaming/removing fields from chunks streamed on &quot;/scheduler&quot;.</li>
<li>Renaming/removing existing Calls.</li>
</ul>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<h3 id="release-branches"><a class="header" href="#release-branches">Release branches</a></h3>
<p>For regular releases, the work is done on the master branch. There are no feature branches but there will be release branches.</p>
<p>When it is time to cut a minor release, a new branch (e.g., 1.2.x) is created off the master branch. We chose 'x' instead of patch release number to disambiguate branch names from tag names. Then the first RC (-rc1) is tagged on the release branch. Subsequent RCs, in case the previous RCs fail testing, should be tagged on the release branch.</p>
<p>Patch releases are also based off the release branches. Typically the fix for an issue that is affecting supported releases lands on the master branch and is then backported to the release branch(es). In rare cases, the fix might directly go into a release branch without landing on master (e.g.,  fix / issue is not applicable to master).</p>
<p>Having a branch for each minor release reduces the amount of work a release manager needs to do when it is time to do a release. It is the responsibility of the committer of a fix to commit it to all the affecting release branches. This is important because the committer has more context about the issue / fix at the time of the commit than a release manager at the time of release. The release manager of a minor release will be responsible for all its patch releases as well. Just like the master branch, history rewrites are not allowed in the release branch (i.e., no git push --force).</p>
<h3 id="api-protobufs"><a class="header" href="#api-protobufs">API protobufs</a></h3>
<p>Most APIs in Mesos accept protobuf messages with a corresponding JSON field mapping. To support multiple versions of the API, we decoupled the versioned protobufs backing the API from the &quot;internal&quot; protobufs used by the Mesos code.</p>
<p>For example, the protobufs for the v1 Scheduler API are located at:</p>
<pre><code>include/mesos/v1/scheduler/scheduler.proto

package mesos.v1.scheduler;
option java_package = &quot;org.apache.mesos.v1.scheduler&quot;;
option java_outer_classname = &quot;Protos&quot;;
...
</code></pre>
<p>The corresponding internal protobufs for the Scheduler API are located at:</p>
<pre><code>include/mesos/scheduler/scheduler.proto

package mesos.scheduler;
option java_package = &quot;org.apache.mesos.scheduler&quot;;
option java_outer_classname = &quot;Protos&quot;;
...
</code></pre>
<p>The users of the API send requests (and receive responses) based on the versioned protobufs. We implemented <a href="https://github.com/apache/mesos/blob/master/src/internal/evolve.hpp">evolve</a>/<a href="https://github.com/apache/mesos/blob/master/src/internal/devolve.hpp">devolve</a> converters that can convert protobufs from any supported version to the internal protobuf and vice versa.</p>
<p>Internally, message passing between various Mesos components would use the internal unversioned protobufs. When sending response (if any) back to the user of the API, the unversioned protobuf would be converted back to a versioned protobuf.</p>
<h1 id="building"><a class="header" href="#building">Building</a></h1>
<h2 id="downloading-mesos"><a class="header" href="#downloading-mesos">Downloading Mesos</a></h2>
<p>There are different ways you can get Mesos:</p>
<p>1. Download the latest stable release from <a href="http://mesos.apache.org/downloads/">Apache</a> (<em><strong>Recommended</strong></em>)</p>
<pre><code>$ wget https://downloads.apache.org/mesos/1.11.0/mesos-1.11.0.tar.gz
$ tar -zxf mesos-1.11.0.tar.gz
</code></pre>
<p>2. Clone the Mesos git <a href="https://gitbox.apache.org/repos/asf/mesos.git">repository</a> (<em><strong>Advanced Users Only</strong></em>)</p>
<pre><code>$ git clone https://gitbox.apache.org/repos/asf/mesos.git
</code></pre>
<p><em>NOTE: If you have problems running the above commands, you may need to first run through the <em><strong>System Requirements</strong></em> section below to install the <code>wget</code>, <code>tar</code>, and <code>git</code> utilities for your system.</em></p>
<h2 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h2>
<p>Mesos runs on Linux (64 Bit) and Mac OS X (64 Bit). To build Mesos from source, GCC 4.8.1+ or Clang 3.5+ is required.</p>
<p>On Linux, a kernel version &gt;= 2.6.28 is required at both build time and run time. For full support of process isolation under Linux a recent kernel &gt;= 3.10 is required.</p>
<p>The Mesos agent also runs on Windows. To build Mesos from source, follow the instructions in the <a href="windows.html">Windows</a> section.</p>
<p>Make sure your hostname is resolvable via DNS or via <code>/etc/hosts</code> to allow full support of Docker's host-networking capabilities, needed for some of the Mesos tests. When in doubt, please validate that <code>/etc/hosts</code> contains your hostname.</p>
<h3 id="ubuntu-1404"><a class="header" href="#ubuntu-1404">Ubuntu 14.04</a></h3>
<p>Following are the instructions for stock Ubuntu 14.04. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Update the packages.
$ sudo apt-get update

# Install a few utility tools.
$ sudo apt-get install -y tar wget git

# Install the latest OpenJDK.
$ sudo apt-get install -y openjdk-7-jdk

# Install autotools (Only necessary if building from git repository).
$ sudo apt-get install -y autoconf libtool

# Install other Mesos dependencies.
$ sudo apt-get -y install build-essential python-dev python-six python-virtualenv libcurl4-nss-dev libsasl2-dev libsasl2-modules maven libapr1-dev libsvn-dev
</code></pre>
<h3 id="ubuntu-1604"><a class="header" href="#ubuntu-1604">Ubuntu 16.04</a></h3>
<p>Following are the instructions for stock Ubuntu 16.04. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Update the packages.
$ sudo apt-get update

# Install a few utility tools.
$ sudo apt-get install -y tar wget git

# Install the latest OpenJDK.
$ sudo apt-get install -y openjdk-8-jdk

# Install autotools (Only necessary if building from git repository).
$ sudo apt-get install -y autoconf libtool

# Install other Mesos dependencies.
$ sudo apt-get -y install build-essential python-dev python-six python-virtualenv libcurl4-nss-dev libsasl2-dev libsasl2-modules maven libapr1-dev libsvn-dev zlib1g-dev iputils-ping
</code></pre>
<h3 id="mac-os-x-1011-el-capitan-macos-1012-sierra"><a class="header" href="#mac-os-x-1011-el-capitan-macos-1012-sierra">Mac OS X 10.11 (El Capitan), macOS 10.12 (Sierra)</a></h3>
<p>Following are the instructions for Mac OS X El Capitan. When building Mesos with the Apple-provided toolchain, the Command Line Tools from XCode &gt;= 8.0 are required; XCode 8 requires Mac OS X 10.11.5 or newer.</p>
<pre><code># Install Python 3: https://www.python.org/downloads/

# Install Command Line Tools. The Command Line Tools from XCode &gt;= 8.0 are required.
$ xcode-select --install

# Install Homebrew.
$ ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;

# Install Java.
$ brew install Caskroom/cask/java

# Install libraries.
$ brew install wget git autoconf automake libtool subversion maven xz

# Install Python dependencies.
$ sudo easy_install pip
$ pip install virtualenv
</code></pre>
<p>When compiling on macOS 10.12, the following is needed:</p>
<pre><code># There is an incompatiblity with the system installed svn and apr headers.
# We need the svn and apr headers from a brew installation of subversion.
# You may need to unlink the existing version of subversion installed via
# brew in order to configure correctly.
$ brew unlink subversion # (If already installed)
$ brew install subversion

# When configuring, the svn and apr headers from brew will be automatically
# detected, so no need to explicitly point to them.
# If the build fails due to compiler warnings, `--disable-werror` can be passed
# to configure to not treat warnings as errors.
$ ../configure

# Lastly, you may encounter the following error when the libprocess tests run:
$ ./libprocess-tests
Failed to obtain the IP address for '&lt;hostname&gt;'; the DNS service may not be able to resolve it: nodename nor servname provided, or not known

# If so, turn on 'Remote Login' within System Preferences &gt; Sharing to resolve the issue.
</code></pre>
<p><em>NOTE: When upgrading from Yosemite to El Capitan, make sure to rerun <code>xcode-select --install</code> after the upgrade.</em></p>
<h3 id="centos-66"><a class="header" href="#centos-66">CentOS 6.6</a></h3>
<p>Following are the instructions for stock CentOS 6.6. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Install a recent kernel for full support of process isolation.
$ sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
$ sudo rpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm
$ sudo yum --enablerepo=elrepo-kernel install -y kernel-lt

# Make the just installed kernel the one booted by default, and reboot.
$ sudo sed -i 's/default=1/default=0/g' /boot/grub/grub.conf
$ sudo reboot

# Install a few utility tools. This also forces an update of `nss`,
# which is necessary for the Java bindings to build properly.
$ sudo yum install -y tar wget git which nss

# 'Mesos &gt; 0.21.0' requires a C++ compiler with full C++11 support,
# (e.g. GCC &gt; 4.8) which is available via 'devtoolset-2'.
# Fetch the Scientific Linux CERN devtoolset repo file.
$ sudo wget -O /etc/yum.repos.d/slc6-devtoolset.repo http://linuxsoft.cern.ch/cern/devtoolset/slc6-devtoolset.repo

# Import the CERN GPG key.
$ sudo rpm --import http://linuxsoft.cern.ch/cern/centos/7/os/x86_64/RPM-GPG-KEY-cern

# Fetch the Apache Maven repo file.
$ sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo

# 'Mesos &gt; 0.21.0' requires 'subversion &gt; 1.8' devel package, which is
# not available in the default repositories.
# Create a WANdisco SVN repo file to install the correct version:
$ sudo bash -c 'cat &gt; /etc/yum.repos.d/wandisco-svn.repo &lt;&lt;EOF
[WANdiscoSVN]
name=WANdisco SVN Repo 1.8
enabled=1
baseurl=http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/$basearch/
gpgcheck=1
gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco
EOF'

# Install essential development tools.
$ sudo yum groupinstall -y &quot;Development Tools&quot;

# Install 'devtoolset-2-toolchain' which includes GCC 4.8.2 and related packages.
# Installing 'devtoolset-3' might be a better choice since `perf` might
# conflict with the version of `elfutils` included in devtoolset-2.
$ sudo yum install -y devtoolset-2-toolchain

# Install other Mesos dependencies.
$ sudo yum install -y apache-maven python-devel python-six python-virtualenv java-1.7.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel

# Enter a shell with 'devtoolset-2' enabled.
$ scl enable devtoolset-2 bash
$ g++ --version  # Make sure you've got GCC &gt; 4.8!

# Process isolation is using cgroups that are managed by 'cgconfig'.
# The 'cgconfig' service is not started by default on CentOS 6.6.
# Also the default configuration does not attach the 'perf_event' subsystem.
# To do this, add 'perf_event = /cgroup/perf_event;' to the entries in '/etc/cgconfig.conf'.
$ sudo yum install -y libcgroup
$ sudo service cgconfig start
</code></pre>
<h3 id="centos-71"><a class="header" href="#centos-71">CentOS 7.1</a></h3>
<p>Following are the instructions for stock CentOS 7.1. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Install a few utility tools
$ sudo yum install -y tar wget git

# Fetch the Apache Maven repo file.
$ sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo

# Install the EPEL repo so that we can pull in 'libserf-1' as part of our
# subversion install below.
$ sudo yum install -y epel-release

# 'Mesos &gt; 0.21.0' requires 'subversion &gt; 1.8' devel package,
# which is not available in the default repositories.
# Create a WANdisco SVN repo file to install the correct version:
$ sudo bash -c 'cat &gt; /etc/yum.repos.d/wandisco-svn.repo &lt;&lt;EOF
[WANdiscoSVN]
name=WANdisco SVN Repo 1.9
enabled=1
baseurl=http://opensource.wandisco.com/centos/7/svn-1.9/RPMS/\$basearch/
gpgcheck=1
gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco
EOF'

# Parts of Mesos require systemd in order to operate. However, Mesos
# only supports versions of systemd that contain the 'Delegate' flag.
# This flag was first introduced in 'systemd version 218', which is
# lower than the default version installed by centos. Luckily, centos
# 7.1 has a patched 'systemd &lt; 218' that contains the 'Delegate' flag.
# Explicity update systemd to this patched version.
$ sudo yum update systemd

# Install essential development tools.
$ sudo yum groupinstall -y &quot;Development Tools&quot;

# Install other Mesos dependencies.
$ sudo yum install -y apache-maven python-devel python-six python-virtualenv java-1.8.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel
</code></pre>
<h3 id="windows"><a class="header" href="#windows">Windows</a></h3>
<p>Follow the instructions in the <a href="windows.html">Windows</a> section.</p>
<h2 id="building-mesos-posix"><a class="header" href="#building-mesos-posix">Building Mesos (Posix)</a></h2>
<pre><code># Change working directory.
$ cd mesos

# Bootstrap (Only required if building from git repository).
$ ./bootstrap

# Configure and build.
$ mkdir build
$ cd build
$ ../configure
$ make
</code></pre>
<p>In order to speed up the build and reduce verbosity of the logs, you can append <code>-j &lt;number of cores&gt; V=0</code> to <code>make</code>.</p>
<pre><code># Run test suite.
$ make check

# Install (Optional).
$ make install
</code></pre>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<p>Mesos comes bundled with example frameworks written in C++, Java and Python.
The framework binaries will only be available after running <code>make check</code>, as
described in the <em><strong>Building Mesos</strong></em> section above.</p>
<pre><code># Change into build directory.
$ cd build

# Start Mesos master (ensure work directory exists and has proper permissions).
$ ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos

# Start Mesos agent (ensure work directory exists and has proper permissions).
$ ./bin/mesos-agent.sh --master=127.0.0.1:5050 --work_dir=/var/lib/mesos

# Visit the Mesos web page.
$ http://127.0.0.1:5050

# Run C++ framework (exits after successfully running some tasks).
$ ./src/test-framework --master=127.0.0.1:5050

# Run Java framework (exits after successfully running some tasks).
$ ./src/examples/java/test-framework 127.0.0.1:5050

# Run Python framework (exits after successfully running some tasks).
$ ./src/examples/python/test-framework 127.0.0.1:5050
</code></pre>
<p><em>Note: These examples assume you are running Mesos on your local machine.
Following them will not allow you to access the Mesos web page in a production
environment (e.g. on AWS). For that you will need to specify the actual IP of
your host when launching the Mesos master and ensure your firewall settings
allow access to port 5050 from the outside world.</em></p>
<h1 id="binary-packages"><a class="header" href="#binary-packages">Binary Packages</a></h1>
<h2 id="downloading-the-mesos-rpm"><a class="header" href="#downloading-the-mesos-rpm">Downloading the Mesos RPM</a></h2>
<p>Download and install the latest stable CentOS7 RPM binary from the <a href="http://rpm.aventer.biz/CentOS/7/x86_64/">Repository</a>:</p>
<pre><code>$ cat &gt; /tmp/aventer.repo &lt;&lt;EOF
#aventer-mesos-el - packages by mesos from aventer
[aventer-rel]
name=AVENTER stable repository $releasever
baseurl=http://rpm.aventer.biz/CentOS/$releasever/$basearch/
enabled=1
gpgkey=https://www.aventer.biz/CentOS/support_aventer.asc
EOF

$ sudo mv /tmp/aventer.repo /etc/yum.repos.d/aventer.repo

$ sudo yum update

$ sudo yum install mesos
</code></pre>
<p>The above instructions show how to install the latest version of Mesos for RHEL 7.
Substitute <code>baseurl</code> the with the appropriate URL for your operating system.</p>
<h2 id="start-mesos-master-and-agent"><a class="header" href="#start-mesos-master-and-agent">Start Mesos Master and Agent.</a></h2>
<p>The RPM installation creates the directory <code>/var/lib/mesos</code> that can be used as a work directory.</p>
<p>Start the Mesos master with the following command:</p>
<pre><code>$ mesos-master --work_dir=/var/lib/mesos
</code></pre>
<p>On a different terminal, start the Mesos agent, and associate it with the Mesos master started above:</p>
<pre><code>$ mesos-agent --work_dir=/var/lib/mesos --master=127.0.0.1:5050
</code></pre>
<p>This is the simplest way to try out Mesos after downloading the RPM. For more complex and production
setup instructions refer to the <a href="http://mesos.apache.org/documentation/latest/#administration">Administration</a> section of the docs.</p>
<h1 id="mesos-runtime-configuration"><a class="header" href="#mesos-runtime-configuration">Mesos Runtime Configuration</a></h1>
<p>The Mesos master and agent can take a variety of configuration options
through command-line arguments or environment variables. A list of the
available options can be seen by running <code>mesos-master --help</code> or
<code>mesos-agent --help</code>. Each option can be set in two ways:</p>
<ul>
<li>
<p>By passing it to the binary using <code>--option_name=value</code>, either
specifying the value directly, or specifying a file in which the value
resides (<code>--option_name=file://path/to/file</code>). The path can be
absolute or relative to the current working directory.</p>
</li>
<li>
<p>By setting the environment variable <code>MESOS_OPTION_NAME</code> (the option
name with a <code>MESOS_</code> prefix added to it).</p>
</li>
</ul>
<p>Configuration values are searched for first in the environment, then
on the command-line.</p>
<p>Additionally, this documentation lists only a recent snapshot of the options in
Mesos. A definitive source for which flags your version of Mesos supports can be
found by running the binary with the flag <code>--help</code>, for example <code>mesos-master --help</code>.</p>
<h2 id="master-and-agent-options"><a class="header" href="#master-and-agent-options">Master and Agent Options</a></h2>
<p><em>These are options common to both the Mesos master and agent.</em></p>
<p>See <a href="configuration/master-and-agent.html">configuration/master-and-agent.md</a>.</p>
<h2 id="master-options"><a class="header" href="#master-options">Master Options</a></h2>
<p>See <a href="configuration/master.html">configuration/master.md</a>.</p>
<h2 id="agent-options"><a class="header" href="#agent-options">Agent Options</a></h2>
<p>See <a href="configuration/agent.html">configuration/agent.md</a>.</p>
<h2 id="libprocess-options"><a class="header" href="#libprocess-options">Libprocess Options</a></h2>
<p>See <a href="configuration/libprocess.html">configuration/libprocess.md</a>.</p>
<h1 id="mesos-build-configuration"><a class="header" href="#mesos-build-configuration">Mesos Build Configuration</a></h1>
<h2 id="autotools-options"><a class="header" href="#autotools-options">Autotools Options</a></h2>
<p>If you have special compilation requirements, please refer to <code>./configure --help</code> when configuring Mesos.</p>
<p>See <a href="configuration/autotools.html">configuration/autotools.md</a>.</p>
<h2 id="cmake-options"><a class="header" href="#cmake-options">CMake Options</a></h2>
<p>See <a href="configuration/cmake.html">configuration/cmake.md</a>.</p>
<h1 id="install-cmake-37"><a class="header" href="#install-cmake-37">Install CMake 3.7+</a></h1>
<h2 id="linux"><a class="header" href="#linux">Linux</a></h2>
<p>Install the latest version of CMake from <a href="https://cmake.org/download/">CMake.org</a>.
A self-extracting tarball is available to make this process painless.</p>
<p>Currently, few of the common Linux flavors package a sufficient CMake
version. Ubuntu versions 12.04 and 14.04 package CMake 2;
Ubuntu 16.04 packages CMake 3.5. If you already installed cmake from packages,
you may remove it via: <code>apt-get purge cmake</code>.</p>
<p>The standard CentOS package is CMake 2, and unfortunately even the <code>cmake3</code>
package in EPEL is only CMake 3.6, you may remove them via:
<code>yum remove cmake cmake3</code>.</p>
<h2 id="mac-os-x"><a class="header" href="#mac-os-x">Mac OS X</a></h2>
<p>HomeBrew's CMake version is sufficient: <code>brew install cmake</code>.</p>
<h2 id="windows-1"><a class="header" href="#windows-1">Windows</a></h2>
<p>Download and install the MSI from <a href="https://cmake.org/download/">CMake.org</a>.</p>
<p><strong>NOTE:</strong> Windows needs CMake 3.8+, rather than 3.7+.</p>
<h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<p>The most basic way to build with CMake, with no configuration, is fairly
straightforward:</p>
<pre><code>mkdir build
cd build
cmake ..
cmake --build .
</code></pre>
<p>The last step, <code>cmake --build .</code> can also take a <code>--target</code> command to build any
particular target (e.g. <code>mesos-tests</code>, or <code>tests</code> to build <code>mesos-tests</code>,
<code>libprocess-tests</code>, and <code>stout-tests</code>): <code>cmake --build . --target tests</code>. To
send arbitrary flags to the native build system underneath (e.g. <code>make</code>), append
the command with <code>-- &lt;flags to be passed&gt;</code>: <code>cmake --build . -- -j4</code>.</p>
<p>Also, <code>cmake --build</code> can be substituted by your build system of choice. For
instance, the default CMake generator on Linux produces GNU Makefiles, so after
configuring with <code>cmake ..</code>, you can just run <code>make tests</code> in the <code>build</code> folder
like usual. Similarly, if you configure with <code>-G Ninja</code> to use the Ninja
generator, you can then run <code>ninja tests</code> to build the <code>tests</code> target with
Ninja.</p>
<h1 id="installable-build"><a class="header" href="#installable-build">Installable build</a></h1>
<p>This example will build Mesos and install it into a custom prefix:</p>
<pre><code>mkdir build &amp;&amp; cd build
cmake -DCMAKE_INSTALL_PREFIX=/home/current_user/mesos
cmake --build . --target install
</code></pre>
<p>To additionally install <code>mesos-tests</code> executable and related test helpers
(this can be used to run Mesos tests against the installed binaries),
one can enable the <code>MESOS_INSTALL_TESTS</code> option.</p>
<p>To produce a set of binaries and libraries that will work after being
copied/moved to a different location, use <code>MESOS_FINAL_PREFIX</code>.</p>
<p>The example below employs both <code>MESOS_FINAL_PREFIX</code> and <code>MESOS_INSTALL_TESTS</code>.
On a build system:</p>
<pre><code>mkdir build &amp;&amp; cd build
cmake -DMESOS_FINAL_PREFIX=/opt/mesos -DCMAKE_INSTALL_PREFIX=/home/current_user/mesos -DMESOS_INSTALL_TESTS=ON
cmake --build . --target install
tar -czf mesos.tar.gz mesos -C /home/current_user
</code></pre>
<p>On a target system:</p>
<pre><code>sudo tar -xf mesos.tar.gz -C /opt
# Run tests against Mesos installation
sudo /opt/mesos/bin/mesos-tests
# Start Mesos agent
sudo /opt/mesos/bin/mesos-agent --work-dir=/var/lib/mesos ...
</code></pre>
<h1 id="supported-options"><a class="header" href="#supported-options">Supported options</a></h1>
<p>See <a href="configuration/cmake.html">configuration options</a>.</p>
<h1 id="examples-1"><a class="header" href="#examples-1">Examples</a></h1>
<p>See <a href="cmake-examples.html">CMake By Example</a>.</p>
<h1 id="documentation"><a class="header" href="#documentation">Documentation</a></h1>
<p>The <a href="https://cmake.org/cmake/help/latest/">CMake documentation</a> is written as a reference module. The most commonly
used sections are:</p>
<ul>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html">buildsystem overview</a></li>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-commands.7.html">commands</a></li>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-properties.7.html">properties</a></li>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-variables.7.html">variables</a></li>
</ul>
<p>The wiki also has a set of <a href="https://cmake.org/Wiki/CMake_Useful_Variables">useful variables</a>.</p>
<h1 id="dependency-graph"><a class="header" href="#dependency-graph">Dependency graph</a></h1>
<p>Like any build system, CMake has a dependency graph. The difference is
that targets in CMake's dependency graph are <em>much richer</em> compared to other
build systems. CMake targets have the notion of 'interfaces', where build
properties are saved as part of the target, and these properties can be
inherited transitively within the graph.</p>
<p>For example, say there is a library <code>mylib</code>, and anything which links it must
include its headers, located in <code>mylib/include</code>. When building the library, some
private headers must also be included, but not when linking to it. When
compiling the executable <code>myprogram</code>, <code>mylib</code>'s public headers must be included,
but not its private headers. There is no manual step to add <code>mylib/include</code> to
<code>myprogram</code> (and any other program which links to <code>mylib</code>), it is instead
deduced from the public interface property of <code>mylib</code>. This is represented by
the following code:</p>
<pre><code># A new library with a single source file (headers are found automatically).
add_library(mylib mylib.cpp)

# The folder of private headers, not exposed to consumers of `mylib`.
target_include_directories(mylib PRIVATE mylib/private)

# The folder of public headers, added to the compilation of any consumer.
target_include_directories(mylib PUBLIC mylib/include)

# A new exectuable with a single source file.
add_executable(myprogram main.cpp)

# The creation of the link dependency `myprogram` -&gt; `mylib`.
target_link_libraries(myprogram mylib)

# There is no additional step to add `mylib/include` to `myprogram`.
</code></pre>
<p>This same notion applies to practically every build property:
compile definitions via <a href="https://cmake.org/cmake/help/latest/command/target_compile_definitions.html"><code>target_compile_definitions</code></a>,
include directories via <a href="https://cmake.org/cmake/help/latest/command/target_include_directories.html"><code>target_include_directories</code></a>,
link libraries via <a href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html"><code>target_link_libraries</code></a>,
compile options via <a href="https://cmake.org/cmake/help/latest/command/target_compile_options.html"><code>target_compile_options</code></a>,
and compile features via <a href="https://cmake.org/cmake/help/latest/command/target_compile_features.html"><code>target_compile_features</code></a>.</p>
<p>All of these commands also take an optional argument of
<code>&lt;INTERFACE|PUBLIC|PRIVATE&gt;</code>, which constrains their transitivity in the graph.
That is, a <code>PRIVATE</code> include directory is recorded for the target, but not
shared transitively to anything depending on the target, <code>PUBLIC</code> is used
for both the target and dependencies on it, and <code>INTERFACE</code> is used only
for dependencies.</p>
<p>Notably missing from this list are link directories. CMake explicitly prefers
finding and using the absolute paths to libraries, obsoleting link directories.</p>
<h1 id="common-mistakes"><a class="header" href="#common-mistakes">Common mistakes</a></h1>
<h2 id="booleans"><a class="header" href="#booleans">Booleans</a></h2>
<p>CMake treats <code>ON</code>, <code>OFF</code>, <code>TRUE</code>, <code>FALSE</code>, <code>1</code>, and <code>0</code> all as true/false
booleans. Furthermore, variables of the form <code>&lt;target&gt;-NOTFOUND</code> are also
treated as false (this is used for finding packages).</p>
<p>In Mesos, we prefer the boolean types <code>TRUE</code> and <code>FALSE</code>.</p>
<p>See <a href="https://cmake.org/cmake/help/latest/command/if.html"><code>if</code></a> for more info.</p>
<h2 id="conditionals"><a class="header" href="#conditionals">Conditionals</a></h2>
<p>For historical reasons, CMake conditionals such as <code>if</code> and <code>elseif</code>
automatically interpolate variable names. It is therefore dangerous to
interpolate them manually, because if <code>${FOO}</code> evaluates to <code>BAR</code>, and <code>BAR</code> is
another variable name, then <code>if (${FOO})</code> becomes <code>if (BAR)</code>, and <code>BAR</code> is then
evaluated again by the <code>if</code>. Stick to <code>if (FOO)</code> to check the value of <code>${FOO}</code>.
Do not use <code>if (${FOO})</code>.</p>
<p>Also see the CMake policies
<a href="https://cmake.org/cmake/help/latest/policy/CMP0012.html">CMP0012</a> and
<a href="https://cmake.org/cmake/help/latest/policy/CMP0054.html">CMP0054</a>.</p>
<h2 id="definitions"><a class="header" href="#definitions">Definitions</a></h2>
<p>When using <code>add_definitions()</code> (which should be used rarely, as it is for
&quot;global&quot; compile definitions), the flags must be prefixed with <code>-D</code> to be
treated as preprocessor definitions. However, when using
<code>target_compile_definitions()</code> (which should be preferred, as it is
for specific targets), the flags do not need the prefix.</p>
<h1 id="style"><a class="header" href="#style">Style</a></h1>
<p>In general, wrap at 80 lines, and use a two-space indent. When wrapping
arguments, put the command on a separate line and arguments on subsequent lines:</p>
<pre><code>target_link_libraries(
  program PRIVATE
  alpha
  beta
  gamma)
</code></pre>
<p>Otherwise keep it together:</p>
<pre><code>target_link_libraries(program PUBLIC library)
</code></pre>
<p>Always keep the trailing parenthesis with the last argument.</p>
<p>Use a single space between conditionals and their open parenthesis, e.g.
<code>if (FOO)</code>, but not for commands, e.g. <code>add_executable(program)</code>.</p>
<p>CAPITALIZE the declaration and use of custom functions and macros (e.g.
<code>EXTERNAL</code> and <code>PATCH_CMD</code>), and do not capitalize the use of CMake built-in
(including modules) functions and macros. CAPITALIZE variables.</p>
<h1 id="cmake-anti-patterns"><a class="header" href="#cmake-anti-patterns">CMake anti-patterns</a></h1>
<p>Because CMake handles much more of the grunt work for you than other build
systems, there are unfortunately a lot of CMake <a href="http://voices.canonical.com/jussi.pakkanen/2013/03/26/a-list-of-common-cmake-antipatterns/">anti-patterns</a> you should
look out for when writing new CMake code. These are some common problems
that should be avoided when writing new CMake code:</p>
<h2 id="superfluous-use-of-add_dependencies"><a class="header" href="#superfluous-use-of-add_dependencies">Superfluous use of <code>add_dependencies</code></a></h2>
<p>When you've linked library <code>a</code> to library <code>b</code> with <code>target_link_libraries(a b)</code>,
the CMake graph is already updated with the dependency information. It is
redundant to use <code>add_dependencies(a b)</code> to (re)specify the dependency. In fact,
this command should <em>rarely</em> be used.</p>
<p>The exceptions to this are:</p>
<ol>
<li>Setting a dependency from an imported library to a target added via
<code>ExternalProject_Add</code>.</li>
<li>Setting a dependency on Mesos modules since no explicit linking is done.</li>
<li>Setting a dependency between executables (e.g. the <code>mesos-agent</code> requiring the
<code>mesos-containerizer</code> executable). In general, runtime dependencies need
to be setup with <code>add_dependency</code>, but never link dependencies.</li>
</ol>
<h2 id="use-of-link_libraries-or-link_directories"><a class="header" href="#use-of-link_libraries-or-link_directories">Use of <code>link_libraries</code> or <code>link_directories</code></a></h2>
<p>Neither of these commands should ever be used. The only appropriate command used
to link libraries is <a href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html"><code>target_link_libraries</code></a>, which records the information
in the CMake dependency graph. Furthermore, imported third-party libraries
should have correct locations recorded in their respective targets, so the use
of <code>link_directories</code> should never be necessary. The
<a href="https://cmake.org/cmake/help/latest/command/link_directories.html">official documentation</a> states:</p>
<blockquote>
<p>Note that this command is rarely necessary. Library locations returned by
<code>find_package()</code> and <code>find_library()</code> are absolute paths. Pass these absolute
library file paths directly to the <code>target_link_libraries()</code> command. CMake
will ensure the linker finds them.</p>
</blockquote>
<p>The difference is that the former sets global (or directory level) side effects,
and the latter sets specific target information stored in the graph.</p>
<h2 id="use-of-include_directories"><a class="header" href="#use-of-include_directories">Use of <code>include_directories</code></a></h2>
<p>This is similar to the above: the <a href="https://cmake.org/cmake/help/latest/command/target_include_directories.html"><code>target_include_directories</code></a> should always
be preferred so that the include directory information remains localized to the
appropriate targets.</p>
<h2 id="adding-anything-to-endif-"><a class="header" href="#adding-anything-to-endif-">Adding anything to <code>endif ()</code></a></h2>
<p>Old versions of CMake expected the style <code>if (FOO) ... endif (FOO)</code>, where the
<code>endif</code> contained the same expression as the <code>if</code> command. However, this is
tortuously redundant, so leave the parentheses in <code>endif ()</code> empty. This goes
for other endings too, such as <code>endforeach ()</code>, <code>endwhile ()</code>, <code>endmacro ()</code> and
<code>endfunction ()</code>.</p>
<h2 id="specifying-header-files-superfluously"><a class="header" href="#specifying-header-files-superfluously">Specifying header files superfluously</a></h2>
<p>One of the distinct advantages of using CMake for C and C++ projects is that
adding header files to the source list for a target is unnecessary. CMake is
designed to parse the source files (<code>.c</code>, <code>.cpp</code>, etc.) and determine their
required headers automatically. The exception to this is headers generated as
part of the build (such as protobuf or the JNI headers).</p>
<h2 id="checking-cmake_build_type"><a class="header" href="#checking-cmake_build_type">Checking <code>CMAKE_BUILD_TYPE</code></a></h2>
<p>See the <a href="cmake-examples.html#building-debug-or-release-configurations">&quot;Building debug or release configurations&quot;</a>
example for more information. In short, not all generators respect the variable
<code>CMAKE_BUILD_TYPE</code> at configuration time, and thus it must not be used in CMake
logic. A usable alternative (where supported) is a <a href="https://cmake.org/cmake/help/latest/manual/cmake-generator-expressions.7.html#logical-expressions">generator expression</a> such
as <code>$&lt;$&lt;CONFIG:Debug&gt;:DEBUG_MODE&gt;</code>.</p>
<h1 id="remaining-hacks"><a class="header" href="#remaining-hacks">Remaining hacks</a></h1>
<h2 id="3rdparty_dependencies"><a class="header" href="#3rdparty_dependencies"><code>3RDPARTY_DEPENDENCIES</code></a></h2>
<p>Until Mesos on Windows is stable, we keep some dependencies in an external
repository, <a href="https://github.com/mesos/3rdparty">3rdparty</a>. When
all dependencies are bundled with Mesos, this extra repository will no longer be
necessary. Until then, the CMake variable <code>3RDPARTY_DEPENDENCIES</code> points by
default to this URL, but it can also point to the on-disk location of a local
clone of the repo. With this option you can avoid pulling from GitHub for every
clean build. Note that this must be an absolute path with forward slashes, e.g.
<code>-D3RDPARTY_DEPENDENCIES=C:/3rdparty</code>, otherwise it will fail on Windows.</p>
<h2 id="external"><a class="header" href="#external"><code>EXTERNAL</code></a></h2>
<p>The CMake function <code>EXTERNAL</code> defines a few variables that make it easy for us
to track the directory structure of a dependency. In particular, if our
library's name is <code>boost</code>, we invoke:</p>
<pre><code>EXTERNAL(boost ${BOOST_VERSION} ${CMAKE_CURRENT_BINARY_DIR})
</code></pre>
<p>Which will define the following variables as side-effects in the current scope:</p>
<ul>
<li><code>BOOST_TARGET</code>     (a target folder name to put dep in e.g., <code>boost-1.53.0</code>)</li>
<li><code>BOOST_CMAKE_ROOT</code> (where to have CMake put the uncompressed source, e.g.,
<code>build/3rdparty/boost-1.53.0</code>)</li>
<li><code>BOOST_ROOT</code>       (where the code goes in various stages of build, e.g.,
<code>build/.../boost-1.53.0/src</code>, which might contain folders
<code>build-1.53.0-build</code>, <code>-lib</code>, and so on, for each build
step that dependency has)</li>
</ul>
<p>The implementation is in <code>3rdparty/cmake/External.cmake</code>.</p>
<p>This is not to be confused with the CMake module <a href="https://cmake.org/cmake/help/latest/module/ExternalProject.html">ExternalProject</a>, from which
we use <code>ExternalProject_Add</code> to download, extract, configure, and build our
dependencies.</p>
<h2 id="cmake_noop"><a class="header" href="#cmake_noop"><code>CMAKE_NOOP</code></a></h2>
<p>This is a CMake variable we define in <code>3rdparty/CMakeLists.txt</code> so that we can
cancel steps of <code>ExternalProject</code>. <code>ExternalProject</code>'s default behavior is to
attempt to configure, build, and install a project using CMake. So when one of
these steps must be skipped, we use set it to <code>CMAKE_NOOP</code> so that nothing
is run instead.</p>
<h2 id="cmake_forward_args"><a class="header" href="#cmake_forward_args"><code>CMAKE_FORWARD_ARGS</code></a></h2>
<p>The <code>CMAKE_FORWARD_ARGS</code> variable defined in <code>3rdparty/CMakeLists.txt</code> is sent
as the <code>CMAKE_ARGS</code> argument to the <code>ExternalProject_Add</code> macro (along with any
per-project arguments), and is used when the external project is configured as a
CMake project. If either the <code>CONFIGURE_COMMAND</code> or <code>BUILD_COMMAND</code> arguments of
<code>ExternalProject_Add</code> are used, then the <code>CMAKE_ARGS</code> argument will be ignored.
This variable ensures that compilation configurations are properly propagated to
third-party dependencies, such as compiler flags.</p>
<h3 id="cmake_ssl_forward_args"><a class="header" href="#cmake_ssl_forward_args"><code>CMAKE_SSL_FORWARD_ARGS</code></a></h3>
<p>The <code>CMAKE_SSL_FORWARD_ARGS</code> variable defined in <code>3rdparty/CMakeLists.txt</code>
is like <code>CMAKE_FORWARD_ARGS</code>, but only used for specific external projects
that find and link against OpenSSL.</p>
<h2 id="library_linkage"><a class="header" href="#library_linkage"><code>LIBRARY_LINKAGE</code></a></h2>
<p>This variable is a shortcut used in <code>3rdparty/CMakeLists.txt</code>. It is set to
<code>SHARED</code> when <code>BUILD_SHARED_LIBS</code> is true, and otherwise it is set to <code>STATIC</code>.
The <code>SHARED</code> and <code>STATIC</code> keywords are used to declare how a library should be
built; however, if left out then the type is deduced automatically from
<code>BUILD_SHARED_LIBS</code>.</p>
<h2 id="make_include_dir"><a class="header" href="#make_include_dir"><code>MAKE_INCLUDE_DIR</code></a></h2>
<p>This function works around a <a href="https://gitlab.kitware.com/cmake/cmake/issues/15052">CMake issue</a> with setting include
directories of imported libraries built with <code>ExternalProject_Add</code>. We have to
call this for each <code>IMPORTED</code> third-party dependency which has set
<code>INTERFACE_INCLUDE_DIRECTORIES</code>, just to make CMake happy. An example is Glog:</p>
<pre><code>MAKE_INCLUDE_DIR(glog)
</code></pre>
<h2 id="get_byproducts"><a class="header" href="#get_byproducts"><code>GET_BYPRODUCTS</code></a></h2>
<p>This function works around a <a href="https://cmake.org/pipermail/cmake/2015-April/060234.html">CMake issue</a> with the Ninja
generator where it does not understand imported libraries, and instead needs
<code>BUILD_BYPRODUCTS</code> explicitly set. This simply allows us to use
<code>ExternalProject_Add</code> and Ninja. For Glog, it looks like this:</p>
<pre><code>GET_BYPRODUCTS(glog)
</code></pre>
<p>Also see the CMake policy <a href="https://cmake.org/cmake/help/latest/policy/CMP0058.html">CMP0058</a>.</p>
<h2 id="patch_cmd"><a class="header" href="#patch_cmd"><code>PATCH_CMD</code></a></h2>
<p>The CMake function <code>PATCH_CMD</code> generates a patch command given a patch file.
If the path is not absolute, it's resolved to the current source directory.
It stores the command in the variable name supplied. This is used to easily
patch third-party dependencies. For Glog, it looks like this:</p>
<pre><code>PATCH_CMD(GLOG_PATCH_CMD glog-${GLOG_VERSION}.patch)
ExternalProject_Add(
  ${GLOG_TARGET}
  ...
  PATCH_COMMAND     ${GLOG_PATCH_CMD})
</code></pre>
<p>The implementation is in <code>3rdparty/cmake/PatchCommand.cmake</code>.</p>
<h3 id="windows-patchexe"><a class="header" href="#windows-patchexe">Windows <code>patch.exe</code></a></h3>
<p>While using <code>patch</code> on Linux is straightforward, doing the same on Windows takes
a bit of work. <code>PATH_CMD</code> encapsulates this:</p>
<ul>
<li>Checks the cache variable <code>PATCHEXE_PATH</code> for <code>patch.exe</code>.</li>
<li>Searches for <code>patch.exe</code> in its default locations.</li>
<li>Copies <code>patch.exe</code> and a custom manifest to the temporary directory.</li>
<li>Applies the manifest to avoid the UAC prompt.</li>
<li>Uses the patched <code>patch.exe</code>.</li>
</ul>
<p>As such, <code>PATCH_CMD</code> lets us apply patches as we do on Linux, without requiring
an administrative prompt.</p>
<p>Note that on Windows, the patch file must have CRLF line endings. A file with LF
line endings will cause the error: &quot;Assertion failed, hunk, file patch.c, line
343&quot;. For this reason, it is required to checkout the Mesos repo with <code>git config core.autocrlf true</code>.</p>
<h1 id="windows-2"><a class="header" href="#windows-2">Windows</a></h1>
<p>Mesos 1.0.0 introduced experimental support for Windows.</p>
<h2 id="building-mesos"><a class="header" href="#building-mesos">Building Mesos</a></h2>
<h3 id="system-requirements-1"><a class="header" href="#system-requirements-1">System Requirements</a></h3>
<ol>
<li>
<p>Install the latest <a href="https://www.visualstudio.com/downloads/">Visual Studio 2017</a>:
The &quot;Community&quot; edition is sufficient (and free of charge).
During installation, choose the &quot;Desktop development with C++&quot; workload.</p>
</li>
<li>
<p>Install <a href="https://cmake.org/download/">CMake 3.8.0</a> or later.
During installation, choose to &quot;Add CMake to the system PATH for all users&quot;.</p>
</li>
<li>
<p>Install <a href="http://gnuwin32.sourceforge.net/packages/patch.htm">GNU patch for Windows</a>.</p>
</li>
<li>
<p>If building from source, install <a href="https://git-scm.com/download/win">Git</a>.</p>
</li>
<li>
<p>Make sure there are no spaces in your build directory.
For example, <code>C:/Program Files (x86)/mesos</code> is an invalid build directory.</p>
</li>
<li>
<p>If developing Mesos, install <a href="https://www.python.org/downloads/">Python 3</a>
(not Python 2), in order to use our <code>support</code> scripts (e.g.
to post and apply patches, or lint source code).</p>
</li>
</ol>
<h3 id="build-instructions"><a class="header" href="#build-instructions">Build Instructions</a></h3>
<p>Following are the instructions for Windows 10.</p>
<pre><code># Clone (or extract) Mesos.
git clone https://gitbox.apache.org/repos/asf/mesos.git
cd mesos

# Configure using CMake for an out-of-tree build.
mkdir build
cd build
cmake .. -G &quot;Visual Studio 15 2017 Win64&quot; -T &quot;host=x64&quot;

# Build Mesos.
# To build just the Mesos agent, add `--target mesos-agent`.
cmake --build .

# The Windows agent exposes new isolators that must be used as with
# the `--isolation` flag. To get started point the agent to a working
# master, using eiher an IP address or zookeeper information.
.\src\mesos-agent.exe --master=&lt;master&gt; --work_dir=&lt;work folder&gt; --launcher_dir=&lt;repository&gt;\build\src
</code></pre>
<h2 id="running-mesos"><a class="header" href="#running-mesos">Running Mesos</a></h2>
<p>If you deploy the executables to another machine, you must also
install the <a href="https://aka.ms/vs/15/release/VC_redist.x64.exe">Microsoft Visual C++ Redistributable for Visual Studio 2017</a>.</p>
<h2 id="known-limitations"><a class="header" href="#known-limitations">Known Limitations</a></h2>
<p>The current implementation is known to have the following limitations:</p>
<ul>
<li>
<p>Only the agent should be run on Windows. The Mesos master can be
launched, but only for testing as the master does not support
high-availability setups on Windows.</p>
</li>
<li>
<p>While Mesos supports NTFS long paths internally, tasks which do not support
long paths must be run on agent whose <code>--work_dir</code> is a short path.</p>
</li>
<li>
<p>The minimum versions of Windows supported are: Windows 10 Creators Update (AKA
version 1703, build number 15063), and <a href="https://docs.microsoft.com/en-us/windows-server/get-started/get-started-with-1709">Windows Server, version 1709</a>.
It is likely that this will increase, due to evolving Windows container
support and developer features which ease porting.</p>
</li>
<li>
<p>The ability to <a href="https://blogs.windows.com/buildingapps/2016/12/02/symlinks-windows-10/">create symlinks</a> as a non-admin user requires
Developer Mode to be enabled. Otherwise the agent will need to be
run under an administrator.</p>
</li>
</ul>
<h2 id="build-configuration-examples"><a class="header" href="#build-configuration-examples">Build Configuration Examples</a></h2>
<h3 id="building-with-ninja"><a class="header" href="#building-with-ninja">Building with Ninja</a></h3>
<p>Instead of using MSBuild, it is also possible to build Mesos on
Windows using <a href="https://ninja-build.org/">Ninja</a>, which can result in
significantly faster builds. To use Ninja, you need to download it and
ensure <code>ninja.exe</code> is in your <code>PATH</code>.</p>
<ul>
<li>Download the <a href="https://github.com/ninja-build/ninja/releases">Windows binary</a>.</li>
<li>Unzip it and place <code>ninja.exe</code> in your <code>PATH</code>.</li>
<li>Open an &quot;x64 Native Tools Command Prompt for VS 2017&quot; to set your
environment.</li>
<li>In that command prompt, type <code>powershell</code> to use a better shell.</li>
<li>Similar to above, configure CMake with <code>cmake .. -G Ninja</code>.</li>
<li>Now you can use <code>ninja</code> to build the various targets.</li>
<li>You may want to use <code>ninja -v</code> to make it verbose, as it's otherwise
very quiet.</li>
</ul>
<p>Note that with Ninja it is imperative to open the correct developer
command prompt so that the 64-bit build tools are used, as Ninja does
not otherwise know how to find them.</p>
<h3 id="building-with-java"><a class="header" href="#building-with-java">Building with Java</a></h3>
<p>This enables more unit tests, but we do not yet officially produce
<code>mesos-master</code>.</p>
<p>When building with Java on Windows, you must add the <a href="https://maven.apache.org/guides/getting-started/windows-prerequisites.html">Maven</a> build tool to
your path. The <code>JAVA_HOME</code> environment variable must also be manually set.
An installation of the Java SDK can be found form <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">Oracle</a>.</p>
<p>As of this writing, Java 9 is not yet supported, but Java 8 has been tested.</p>
<p>The Java build defaults to <code>OFF</code> because it is slow. To build the Java
components on Windows, turn it <code>ON</code>:</p>
<pre><code class="language-powershell">mkdir build; cd build
$env:PATH += &quot;;C:\...\apache-maven-3.3.9\bin\&quot;
$env:JAVA_HOME = &quot;C:\Program Files\Java\jdk1.8.0_144&quot;
cmake .. -DENABLE_JAVA=ON -G &quot;Visual Studio 15 2017 Win64&quot; -T &quot;host=x64&quot;
cmake --build . --target mesos-java
</code></pre>
<p>Note that the <code>mesos-java</code> library does not have to be manually built; as
<code>libmesos</code> will link it when Java is enabled.</p>
<p>Unfortunately, on Windows the <code>FindJNI</code> CMake module will populate <code>JAVA_JVM_LIBRARY</code> with
the path to the static <code>jvm.lib</code>, but this variable must point to the shared
library, <code>jvm.dll</code>, as it is loaded at runtime. Set it correctly like this:</p>
<pre><code>$env:JAVA_JVM_LIBRARY = &quot;C:\Program Files\Java\jdk1.8.0_144\jre\bin\server\jvm.dll&quot;
</code></pre>
<p>The library may still fail to load at runtime with the following error:</p>
<blockquote>
<p>&quot;The specified module could not be found.&quot;</p>
</blockquote>
<p>If this is the case, and the path to <code>jvm.dll</code> is verified to be correct, then
the error message actually indicates that the dependencies of <code>jvm.dll</code> could
not be found. On Windows, the DLL search path includes the environment variable
<code>PATH</code>, so add the <code>bin</code> folder which contains <code>server\jvm.dll</code> to <code>PATH</code>:</p>
<pre><code>$env:PATH += &quot;;C:\Program Files\Java\jdk1.8.0_144\jre\bin&quot;
</code></pre>
<h3 id="building-with-openssl"><a class="header" href="#building-with-openssl">Building with OpenSSL</a></h3>
<p>When building with OpenSSL on Windows, you must build or install a distribution
of OpenSSL for Windows. A commonly chosen distribution is
<a href="https://slproweb.com/products/Win32OpenSSL.html">Shining Light Productions' OpenSSL</a>.</p>
<p>As of this writing, OpenSSL 1.1.x is supported.</p>
<p>Use <code>-DENABLE_SSL=ON</code> to build with OpenSSL.</p>
<p>Note that it will link to OpenSSL dynamically, so if the built executables are
deployed elsewhere, that machine also needs OpenSSL installed.</p>
<p>Beware that the OpenSSL installation, nor Mesos itself, comes with a certificate
bundle, and so it is likely that certificate verification will fail.</p>
<h1 id="mesos-runtime-configuration-1"><a class="header" href="#mesos-runtime-configuration-1">Mesos Runtime Configuration</a></h1>
<p>The Mesos master and agent can take a variety of configuration options
through command-line arguments or environment variables. A list of the
available options can be seen by running <code>mesos-master --help</code> or
<code>mesos-agent --help</code>. Each option can be set in two ways:</p>
<ul>
<li>
<p>By passing it to the binary using <code>--option_name=value</code>, either
specifying the value directly, or specifying a file in which the value
resides (<code>--option_name=file://path/to/file</code>). The path can be
absolute or relative to the current working directory.</p>
</li>
<li>
<p>By setting the environment variable <code>MESOS_OPTION_NAME</code> (the option
name with a <code>MESOS_</code> prefix added to it).</p>
</li>
</ul>
<p>Configuration values are searched for first in the environment, then
on the command-line.</p>
<p>Additionally, this documentation lists only a recent snapshot of the options in
Mesos. A definitive source for which flags your version of Mesos supports can be
found by running the binary with the flag <code>--help</code>, for example <code>mesos-master --help</code>.</p>
<h2 id="master-and-agent-options-1"><a class="header" href="#master-and-agent-options-1">Master and Agent Options</a></h2>
<p><em>These are options common to both the Mesos master and agent.</em></p>
<p>See <a href="configuration/master-and-agent.html">configuration/master-and-agent.md</a>.</p>
<h2 id="master-options-1"><a class="header" href="#master-options-1">Master Options</a></h2>
<p>See <a href="configuration/master.html">configuration/master.md</a>.</p>
<h2 id="agent-options-1"><a class="header" href="#agent-options-1">Agent Options</a></h2>
<p>See <a href="configuration/agent.html">configuration/agent.md</a>.</p>
<h2 id="libprocess-options-1"><a class="header" href="#libprocess-options-1">Libprocess Options</a></h2>
<p>See <a href="configuration/libprocess.html">configuration/libprocess.md</a>.</p>
<h1 id="mesos-build-configuration-1"><a class="header" href="#mesos-build-configuration-1">Mesos Build Configuration</a></h1>
<h2 id="autotools-options-1"><a class="header" href="#autotools-options-1">Autotools Options</a></h2>
<p>If you have special compilation requirements, please refer to <code>./configure --help</code> when configuring Mesos.</p>
<p>See <a href="configuration/autotools.html">configuration/autotools.md</a>.</p>
<h2 id="cmake-options-1"><a class="header" href="#cmake-options-1">CMake Options</a></h2>
<p>See <a href="configuration/cmake.html">configuration/cmake.md</a>.</p>
<h1 id="mesos-high-availability-mode"><a class="header" href="#mesos-high-availability-mode">Mesos High-Availability Mode</a></h1>
<p>If the Mesos master is unavailable, existing tasks can continue to execute, but new resources cannot be allocated and new tasks cannot be launched. To reduce the chance of this situation occurring, Mesos has a high-availability mode that uses multiple Mesos masters: one active master (called the <em>leader</em> or leading master) and several <em>backups</em> in case it fails. The masters elect the leader, with <a href="http://zookeeper.apache.org/">Apache ZooKeeper</a> both coordinating the election and handling leader detection by masters, agents, and scheduler drivers. More information regarding <a href="https://zookeeper.apache.org/doc/current/recipes.html#sc_leaderElection">how leader election works</a> is available on the Apache Zookeeper website.</p>
<p>This document describes how to configure Mesos to run in high-availability mode. For more information on developing highly available frameworks, see a <a href="high-availability-framework-guide.html">companion document</a>.</p>
<p><strong>Note</strong>: This document assumes you know how to start, run, and work with ZooKeeper, whose client library is included in the standard Mesos build.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>To put Mesos into high-availability mode:</p>
<ol>
<li>
<p>Ensure that the ZooKeeper cluster is up and running.</p>
</li>
<li>
<p>Provide the znode path to all masters, agents, and framework schedulers as follows:</p>
<ul>
<li>
<p>Start the mesos-master binaries using the <code>--zk</code> flag, e.g. <code>--zk=zk://host1:port1,host2:port2,.../path</code></p>
</li>
<li>
<p>Start the mesos-agent binaries with <code>--master=zk://host1:port1,host2:port2,.../path</code></p>
</li>
<li>
<p>Start any framework schedulers using the same <code>zk</code> path as in the last two steps. The SchedulerDriver must be constructed with this path, as shown in the <a href="app-framework-development-guide.html">Framework Development Guide</a>.</p>
</li>
</ul>
</li>
</ol>
<p>From now on, the Mesos masters and agents all communicate with ZooKeeper to find out which master is the current leading master. This is in addition to the usual communication between the leading master and the agents.</p>
<p>In addition to ZooKeeper, one can get the location of the leading master by sending an HTTP request to <a href="endpoints/master/redirect.html">/redirect</a> endpoint on any master.</p>
<p>For HTTP endpoints that only work at the leading master, requests made to endpoints at a non-leading master will result in either a <code>307 Temporary Redirect</code> (with the location of the leading master) or <code>503 Service Unavailable</code> (if the master does not know who the current leader is).</p>
<p>Refer to the <a href="app-framework-development-guide.html">Scheduler API</a> for how to deal with leadership changes.</p>
<h2 id="component-disconnection-handling"><a class="header" href="#component-disconnection-handling">Component Disconnection Handling</a></h2>
<p>When a network partition disconnects a component (master, agent, or scheduler driver) from ZooKeeper, the component's Master Detector induces a timeout event. This notifies the component that it has no leading master. Depending on the component, the following happens. (Note that while a component is disconnected from ZooKeeper, a master may still be in communication with agents or schedulers and vice versa.)</p>
<ul>
<li>
<p>Agents disconnected from ZooKeeper no longer know which master is the leader. They ignore messages from masters to ensure they don't act on a non-leader's decisions. When an agent reconnects to ZooKeeper, ZooKeeper informs it of the current leader and the agent stops ignoring messages from the leader.</p>
</li>
<li>
<p>Masters enter leaderless state irrespective of whether they are a leader or not before the disconnection.</p>
<ul>
<li>
<p>If the leader was disconnected from ZooKeeper, it aborts its process. The user/developer/administrator can then start a new master instance which will try to reconnect to ZooKeeper.</p>
<ul>
<li>Note that many production deployments of Mesos use a process supervisor (such as systemd or supervisord) that is configured to automatically restart the Mesos master if the process aborts unexpectedly.</li>
</ul>
</li>
<li>
<p>Otherwise, the disconnected backup waits to reconnect with ZooKeeper and possibly get elected as the new leading master.</p>
</li>
</ul>
</li>
<li>
<p>Scheduler drivers disconnected from the leading master notify the scheduler about their disconnection from the leader.</p>
</li>
</ul>
<p>When a network partition disconnects an agent from the leader:</p>
<ul>
<li>
<p>The agent fails health checks from the leader.</p>
</li>
<li>
<p>The leader marks the agent as deactivated and sends its tasks to the LOST state. The  <a href="app-framework-development-guide.html">Framework Development Guide</a> describes these various task states.</p>
</li>
<li>
<p>Deactivated agents may not reregister with the leader and are told to shut down upon any post-deactivation communication.</p>
</li>
</ul>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<p>For monitoring the current number of masters in the cluster communicating with each other to form a quorum, see the monitoring guide's <a href="monitoring.html#replicated-log">Replicated Log</a> on <code>registrar/log/ensemble_size</code>.
For creating alerts covering failures in leader election, have a look at the monitoring guide's <a href="monitoring.html#basic-alerts">Basic Alerts</a> on <code>master/elected</code>.</p>
<h2 id="implementation-details-1"><a class="header" href="#implementation-details-1">Implementation Details</a></h2>
<p>Mesos implements two levels of ZooKeeper leader election abstractions, one in <code>src/zookeeper</code> and the other in <code>src/master</code> (look for <code>contender|detector.hpp|cpp</code>).</p>
<ul>
<li>
<p>The lower level <code>LeaderContender</code> and <code>LeaderDetector</code> implement a generic ZooKeeper election algorithm loosely modeled after this
<a href="http://zookeeper.apache.org/doc/current/recipes.html#sc_leaderElection">recipe</a> (sans herd effect handling due to the master group's small size, which is often 3).</p>
</li>
<li>
<p>The higher level <code>MasterContender</code> and <code>MasterDetector</code> wrap around ZooKeeper's contender and detector abstractions as adapters to provide/interpret the ZooKeeper data.</p>
</li>
<li>
<p>Each Mesos master simultaneously uses both a contender and a detector to try to elect themselves and detect who the current leader is. A separate detector is necessary because each master's WebUI redirects browser traffic to the current leader when that master is not elected. Other Mesos components (i.e., agents and scheduler drivers) use the detector to find the current leader and connect to it.</p>
</li>
</ul>
<p>The notion of the group of leader candidates is implemented in <code>Group</code>. This abstraction handles reliable (through queues and retries of retryable errors under the covers) ZooKeeper group membership registration, cancellation, and monitoring. It watches for several ZooKeeper session events:</p>
<ul>
<li>Connection</li>
<li>Reconnection</li>
<li>Session Expiration</li>
<li>ZNode creation, deletion, updates</li>
</ul>
<p>We also explicitly timeout our sessions when disconnected from ZooKeeper for a specified amount of time. See <code>--zk_session_timeout</code> configuration option. This is because the ZooKeeper client libraries only notify of session expiration upon reconnection. These timeouts are of particular interest for network partitions.</p>
<h1 id="the-mesos-replicated-log"><a class="header" href="#the-mesos-replicated-log">The Mesos Replicated Log</a></h1>
<p>Mesos provides a library that lets you create replicated fault-tolerant append-only logs; this library is known as the <em>replicated log</em>. The Mesos master uses this library to store cluster state in a replicated, durable way; the library is also available for use by frameworks to store replicated framework state or to implement the common &quot;<a href="https://en.wikipedia.org/wiki/State_machine_replication">replicated state machine</a>&quot; pattern.</p>
<h2 id="what-is-the-replicated-log"><a class="header" href="#what-is-the-replicated-log">What is the replicated log?</a></h2>
<p><img src="images/log-cluster.png" alt="Aurora and the Replicated Log" /></p>
<p>The replicated log provides <em>append-only</em> storage of <em>log entries</em>; each log entry can contain arbitrary data. The log is <em>replicated</em>, which means that each log entry has multiple copies in the system. Replication provides both fault tolerance and high availability. In the following example, we use <a href="https://aurora.apache.org/">Apache Aurora</a>, a fault tolerant scheduler (i.e., framework) running on top of Mesos, to show a typical replicated log setup.</p>
<p>As shown above, there are multiple Aurora instances running simultaneously (for high availability), with one elected as the leader. There is a log replica on each host running Aurora. Aurora can access the replicated log through a thin library containing the log API.</p>
<p>Typically, the leader is the only one that appends data to the log. Each log entry is replicated and sent to all replicas in the system. Replicas are strongly consistent. In other words, all replicas agree on the value of each log entry. Because the log is replicated, when Aurora decides to failover, it does not need to copy the log from a remote host.</p>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<p>The replicated log can be used to build a wide variety of distributed applications. For example, Aurora uses the replicated log to store all task states and job configurations. The Mesos master's <em>registry</em> also leverages the replicated log to store information about all agents in the cluster.</p>
<p>The replicated log is often used to allow applications to manage replicated state in a strongly consistent way. One way to do this is to store a state-mutating operation in each log entry and have all instances of the distributed application agree on the same initial state (e.g., empty state). The replicated log ensures that each application instance will observe the same sequence of log entries in the same order; as long as applying a state-mutating operation is deterministic, this ensures that all application instances will remain consistent with one another. If any instance of the application crashes, it can reconstruct the current version of the replicated state by starting at the initial state and re-applying all the logged mutations in order.</p>
<p>If the log grows too large, an application can write out a snapshot and then delete all the log entries that occurred before the snapshot. Using this approach, we will be exposing a <a href="https://github.com/apache/mesos/blob/master/src/state/state.hpp">distributed state</a> abstraction in Mesos with replicated log as a backend.</p>
<p>Similarly, the replicated log can be used to build <a href="https://en.wikipedia.org/wiki/State_machine_replication">replicated state machines</a>. In this scenario, each log entry contains a state machine command. Since replicas are strongly consistent, all servers will execute the same commands in the same order.</p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p><img src="images/log-architecture.png" alt="Replicated Log Architecture" /></p>
<p>The replicated log uses the <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29">Paxos consensus algorithm</a> to ensure that all replicas agree on every log entry's value. It is similar to what's described in <a href="https://ramcloud.stanford.edu/%7Eongaro/userstudy/paxos.pdf">these slides</a>. Readers who are familiar with Paxos can skip this section.</p>
<p>The above figure is an implementation overview. When a user wants to append data to the log, the system creates a log writer. The log writer internally creates a coordinator. The coordinator contacts all replicas and executes the Paxos algorithm to make sure all replicas agree about the appended data. The coordinator is sometimes referred to as the <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29"><em>proposer</em></a>.</p>
<p>Each replica keeps an array of log entries. The array index is the log position. Each log entry is composed of three components: the value written by the user, the associated Paxos state and a <em>learned</em> bit where true means this log entry's value has been agreed. Therefore, a replica in our implementation is both an <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29"><em>acceptor</em></a> and a <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29"><em>learner</em></a>.</p>
<h3 id="reaching-consensus-for-a-single-log-entry"><a class="header" href="#reaching-consensus-for-a-single-log-entry">Reaching consensus for a single log entry</a></h3>
<p>A Paxos round can help all replicas reach consensus on a single log entry's value. It has two phases: a promise phase and a write phase. Note that we are using slightly different terminology from the <a href="https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">original Paxos paper</a>. In our implementation, the <em>prepare</em> and <em>accept</em> phases in the original paper are referred to as the <em>promise</em> and <em>write</em> phases, respectively. Consequently, a prepare request (response) is referred to as a promise request (response), and an accept request (response) is referred to as a write request (response).</p>
<p>To append value <em>X</em> to the log at position <em>p</em>, the coordinator first broadcasts a promise request to all replicas with proposal number <em>n</em>, asking replicas to promise that they will not respond to any request (promise/write request) with a proposal number lower than <em>n</em>. We assume that <em>n</em> is higher than any other previously used proposal number, and will explain how we do this later.</p>
<p>When receiving the promise request, each replica checks its Paxos state to decide if it can safely respond to the request, depending on the promises it has previously given out. If the replica is able to give the promise (i.e., passes the proposal number check), it will first persist its promise (the proposal number <em>n</em>) on disk and reply with a promise response. If the replica has been previously written (i.e., accepted a write request), it needs to include the previously written value along with the proposal number used in that write request into the promise response it's about to send out.</p>
<p>Upon receiving promise responses from a <a href="https://en.wikipedia.org/wiki/Quorum_%28distributed_computing%29">quorum</a> of replicas, the coordinator first checks if there exist any previously written value from those responses. The append operation cannot continue if a previously written value is found because it's likely that a value has already been agreed on for that log entry. This is one of the key ideas in Paxos: restrict the value that can be written to ensure consistency.</p>
<p>If no previous written value is found, the coordinator broadcasts a write request to all replicas with value <em>X</em> and proposal number <em>n</em>. On receiving the write request, each replica checks the promise it has given again, and replies with a write response if the write request's proposal number is equal to or larger than the proposal number it has promised. Once the coordinator receives write responses from a quorum of replicas, the append operation succeeds.</p>
<h3 id="optimizing-append-latency-using-multi-paxos"><a class="header" href="#optimizing-append-latency-using-multi-paxos">Optimizing append latency using Multi-Paxos</a></h3>
<p>One naive solution to implement a replicated log is to run a full Paxos round (promise phase and write phase) for each log entry. As discussed in the <a href="https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">original Paxos paper</a>, if the leader is relatively stable, <em>Multi-Paxos</em> can be used to eliminate the need for the promise phase for most of the append operations, resulting in improved performance.</p>
<p>To do that, we introduce a new type of promise request called an <em>implicit</em> promise request. An implicit promise request can be viewed as a <em>batched</em> promise request for a (potentially infinite) set of log entries. Broadcasting an implicit promise request is conceptually equivalent to broadcasting a promise request for every log entry whose value has not yet been agreed. If the implicit promise request broadcasted by a coordinator gets accepted by a quorum of replicas, this coordinator is no longer required to run the promise phase if it wants to append to a log entry whose value has not yet been agreed because the promise phase has already been done in <em>batch</em>. The coordinator in this case is therefore called <em>elected</em> (a.k.a., the leader), and has <em>exclusive</em> access to the replicated log. An elected coordinator may be <em>demoted</em> (or lose exclusive access) if another coordinator broadcasts an implicit promise request with a higher proposal number.</p>
<p>One question remaining is how can we find out those log entries whose values have not yet been agreed. We have a very simple solution: if a replica accepts an implicit promise request, it will include its largest known log position in the response. An elected coordinator will only append log entries at positions larger than <em>p</em>, where <em>p</em> is greater than any log position seen in these responses.</p>
<p>Multi-Paxos has better performance if the leader is stable. The replicated log itself does not perform leader election. Instead, we rely on the user of the replicated log to choose a stable leader. For example, Aurora uses <a href="https://zookeeper.apache.org/">ZooKeeper</a> to elect the leader.</p>
<h3 id="enabling-local-reads"><a class="header" href="#enabling-local-reads">Enabling local reads</a></h3>
<p>As discussed above, in our implementation, each replica is both an acceptor and a learner. Treating each replica as a learner allows us to do local reads without involving other replicas. When a log entry's value has been agreed, the coordinator will broadcast a <em>learned</em> message to all replicas. Once a replica receives the learned message, it will set the learned bit in the corresponding log entry, indicating the value of that log entry has been agreed. We say a log entry is &quot;learned&quot; if its learned bit is set. The coordinator does not have to wait for replicas' acknowledgments.</p>
<p>To perform a read, the log reader will directly look up the underlying local replica. If the corresponding log entry is learned, the reader can just return the value to the user. Otherwise, a full Paxos round is needed to discover the agreed value. We always make sure that the replica co-located with the elected coordinator always has all log entries learned. We achieve that by running full Paxos rounds for those unlearned log entries after the coordinator is elected.</p>
<h3 id="reducing-log-size-using-garbage-collection"><a class="header" href="#reducing-log-size-using-garbage-collection">Reducing log size using garbage collection</a></h3>
<p>In case the log grows large, the application has the choice to truncate the log. To perform a truncation, we append a special log entry whose value is the log position to which the user wants to truncate the log. A replica can actually truncate the log once this special log entry has been learned.</p>
<h3 id="unique-proposal-number"><a class="header" href="#unique-proposal-number">Unique proposal number</a></h3>
<p>Many of the <a href="https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">Paxos research papers</a> assume that each proposal number is globally unique, and a coordinator can always come up with a proposal number that is larger than any other proposal numbers in the system. However, implementing this is not trivial, especially in a distributed environment. <a href="https://ramcloud.stanford.edu/%7Eongaro/userstudy/paxos.pdf">Some researchers suggest</a> concatenating a globally unique server id to each proposal number. But it is still not clear how to generate a globally unique id for each server.</p>
<p>Our solution does not make the above assumptions. A coordinator can use an arbitrary proposal number initially. During the promise phase, if a replica knows a proposal number higher than the proposal number used by the coordinator, it will send the largest known proposal number back to the coordinator. The coordinator will retry the promise phase with a higher proposal number.</p>
<p>To avoid livelock (e.g., when two coordinators completing), we inject a randomly delay between T and 2T before each retry. T has to be chosen carefully. On one hand, we want T &gt;&gt; broadcast time such that one coordinator usually times out and wins before others wake up. On the other hand, we want T to be as small as possible such that we can reduce the wait time. Currently, we use T = 100ms. This idea is actually borrowed from <a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf">Raft</a>.</p>
<h2 id="automatic-replica-recovery"><a class="header" href="#automatic-replica-recovery">Automatic replica recovery</a></h2>
<p>The algorithm described above has a critical vulnerability: if a replica loses its durable state (i.e., log files) due to either disk failure or operational error, that replica may cause inconsistency in the log if it is simply restarted and re-added to the group. The operator needs to stop the application on all hosts, copy the log files from the leader's host, and then restart the application. Note that the operator cannot copy the log files from an arbitrary replica because copying an unlearned log entry may falsely assemble a quorum for an incorrect value, leading to inconsistency.</p>
<p>To avoid the need for operator intervention in this situation, the Mesos replicated log includes support for <em>auto recovery</em>. As long as a quorum of replicas is working properly, the users of the application won't notice any difference.</p>
<h3 id="non-voting-replicas"><a class="header" href="#non-voting-replicas">Non-voting replicas</a></h3>
<p>To enable auto recovery, a key insight is that a replica that loses its durable state should not be allowed to respond to requests from coordinators after restart. Otherwise, it may introduce inconsistency in the log as it could have accepted a promise/write request which it would not have accepted if its previous Paxos state had not been lost.</p>
<p>To solve that, we introduce a new status variable for each replica. A normal replica is said in VOTING status, meaning that it is allowed to respond to requests from coordinators. A replica with no persisted state is put in EMPTY status by default. A replica in EMPTY status is not allowed to respond to any request from coordinators.</p>
<p>A replica in EMPTY status will be promoted to VOTING status if the following two conditions are met:</p>
<ol>
<li>a sufficient amount of missing log entries are recovered such that if other replicas fail, the remaining replicas can recover all the learned log entries, and</li>
<li>its future responses to a coordinator will not break any of the promises (potentially lost) it has given out.</li>
</ol>
<p>In the following, we discuss how we achieve these two conditions.</p>
<h3 id="catch-up"><a class="header" href="#catch-up">Catch-up</a></h3>
<p>To satisfy the above two conditions, a replica needs to perform <em>catch-up</em> to recover lost states. In other words, it will run Paxos rounds to find out those log entries whose values that have already been agreed. The question is how many log entries the local replica should catch-up before the above two conditions can be satisfied.</p>
<p>We found that it is sufficient to catch-up those log entries from position <em>begin</em> to position <em>end</em> where <em>begin</em> is the smallest position seen in a quorum of VOTING replicas and <em>end</em> is the largest position seen in a quorum of VOTING replicas.</p>
<p>Here is our correctness argument. For a log entry at position <em>e</em> where <em>e</em> is larger than <em>end</em>, obviously no value has been agreed on. Otherwise, we should find at least one VOTING replica in a quorum of replicas such that its end position is larger than <em>end</em>. For the same reason, a coordinator should not have collected enough promises for the log entry at position <em>e</em>. Therefore, it's safe for the recovering replica to respond requests for that log entry. For a log entry at position <em>b</em> where <em>b</em> is smaller than <em>begin</em>, it should have already been truncated and the truncation should have already been agreed. Therefore, allowing the recovering replica to respond requests for that position is also safe.</p>
<h3 id="auto-initialization"><a class="header" href="#auto-initialization">Auto initialization</a></h3>
<p>Since we don't allow an empty replica (a replica in EMPTY status) to respond to requests from coordinators, that raises a question for bootstrapping because initially, each replica is empty. The replicated log provides two choices here. One choice is to use a tool (<code>mesos-log</code>) to explicitly initialize the log on each replica by setting the replica's status to VOTING, but that requires an extra step when setting up an application.</p>
<p>The other choice is to do automatic initialization. Our idea is: we allow a replica in EMPTY status to become VOTING immediately if it finds all replicas are in EMPTY status. This is based on the assumption that the only time <em>all</em> replicas are in EMPTY status is during start-up. This may not be true if a catastrophic failure causes all replicas to lose their durable state, and that's exactly the reason we allow conservative users to disable auto-initialization.</p>
<p>To do auto-initialization, if we use a single-phase protocol and allow a replica to directly transit from EMPTY status to VOTING status, we may run into a state where we cannot make progress even if all replicas are in EMPTY status initially. For example, say the quorum size is 2. All replicas are in EMPTY status initially. One replica will first set its status to VOTING because if finds all replicas are in EMPTY status. After that, neither the VOTING replica nor the EMPTY replicas can make progress. To solve this problem, we use a two-phase protocol and introduce an intermediate transient status (STARTING) between EMPTY and VOTING status. A replica in EMPTY status can transit to STARTING status if it finds all replicas are in either EMPTY or STARTING status. A replica in STARTING status can transit to VOTING status if it finds all replicas are in either STARTING or VOTING status. In that way, in our previous example, all replicas will be in STARTING status before any of them can transit to VOTING status.</p>
<h2 id="non-leading-voting-replica-catch-up"><a class="header" href="#non-leading-voting-replica-catch-up">Non-leading VOTING replica catch-up</a></h2>
<p>Starting with Mesos 1.5.0 it is possible to perform eventually consistent reads from a non-leading VOTING log replica. This makes possible to do additional work on non-leading framework replicas, e.g. offload some reading from a leader to standbys reduce failover time by keeping in-memory storage represented by the replicated log &quot;hot&quot;.</p>
<p>To serve eventually consistent reads a replica needs to perform <em>catch-up</em> to recover the latest log state in a manner similar to how it is done during <a href="replicated-log-internals.html#catch-up">EMPTY replica recovery</a>. After that the recovered positions can be replayed without fear of seeing &quot;holes&quot;.</p>
<p>A truncation can take place during the non-leading replica catch-up. The replica may try to fill the truncated position if truncation happens after the replica has recovered <em>begin</em> and <em>end</em> positions, which may lead to producing inconsistent data during log replay. In order to protect against it we use a special tombstone flag that signals to the replica that the position was truncated and <em>begin</em> needs to be adjusted. The replica is not blocked from truncations during or after catching-up, which means that the user may need to retry the catch-up procedure if positions that were recovered became truncated during log replay.</p>
<h2 id="future-work"><a class="header" href="#future-work">Future work</a></h2>
<p>Currently, replicated log does not support dynamic quorum size change, also known as <em>reconfiguration</em>. Supporting reconfiguration would allow us more easily to add, move or swap hosts for replicas. We plan to support reconfiguration in the future.</p>
<h1 id="agent-recovery"><a class="header" href="#agent-recovery">Agent Recovery</a></h1>
<p>If the <code>mesos-agent</code> process on a host exits (perhaps due to a Mesos bug or
because the operator kills the process while <a href="upgrades.html">upgrading Mesos</a>),
any executors/tasks that were being managed by the <code>mesos-agent</code> process will
continue to run.</p>
<p>By default, all the executors/tasks that were being managed by the old
<code>mesos-agent</code> process are expected to gracefully exit on their own, and
will be shut down after the agent restarted if they did not.</p>
<p>However, if a framework enabled  <em>checkpointing</em> when it registered with the
master, any executors belonging to that framework can reconnect to the new
<code>mesos-agent</code> process and continue running uninterrupted. Hence, enabling
framework checkpointing allows tasks to tolerate Mesos agent upgrades and
unexpected <code>mesos-agent</code> crashes without experiencing any downtime.</p>
<p>Agent recovery works by having the agent checkpoint information about its own
state and about the tasks and executors it is managing to local disk, for
example the <code>SlaveInfo</code>, <code>FrameworkInfo</code> and <code>ExecutorInfo</code> messages or the
unacknowledged status updates of running tasks.</p>
<p>When the agent restarts, it will verify that its current configuration, set
from the environment variables and command-line flags, is compatible with the
checkpointed information and will refuse to restart if not.</p>
<p>A special case occurs when the agent detects that its host system was rebooted
since the last run of the agent: The agent will try to recover its previous ID
as usual, but if that fails it will actually erase the information of the
previous run and will register with the master as a new agent.</p>
<p>Note that executors and tasks that exited between agent shutdown and restart
are not automatically restarted during agent recovery.</p>
<h2 id="framework-configuration"><a class="header" href="#framework-configuration">Framework Configuration</a></h2>
<p>A framework can control whether its executors will be recovered by setting
the <code>checkpoint</code> flag in its <code>FrameworkInfo</code> when registering with the master.
Enabling this feature results in increased I/O overhead at each agent that runs
tasks launched by the framework. By default, frameworks do <strong>not</strong> checkpoint
their state.</p>
<h2 id="agent-configuration"><a class="header" href="#agent-configuration">Agent Configuration</a></h2>
<p>Four <a href="configuration/agent.html">configuration flags</a> control the recovery
behavior of a Mesos agent:</p>
<ul>
<li>
<p><code>strict</code>: Whether to do agent recovery in strict mode [Default: true].</p>
<ul>
<li>If strict=true, all recovery errors are considered fatal.</li>
<li>If strict=false, any errors (e.g., corruption in checkpointed data) during
recovery are ignored and as much state as possible is recovered.</li>
</ul>
</li>
<li>
<p><code>reconfiguration_policy</code>: Which kind of configuration changes are accepted
when trying to recover [Default: equal].</p>
<ul>
<li>If reconfiguration_policy=equal, no configuration changes are accepted.</li>
<li>If reconfiguration_policy=additive, the agent will allow the new
configuration to contain additional attributes, increased resourced or an
additional fault domain. For a more detailed description, see
<a href="https://gitbox.apache.org/repos/asf?p=mesos.git;a=blob;f=src/slave/compatibility.hpp;h=78b421a01abe5d2178c93832577577a7ba282b38;hb=HEAD#l37">this</a>.</li>
</ul>
</li>
<li>
<p><code>recover</code>: Whether to recover status updates and reconnect with old
executors [Default: reconnect]</p>
<ul>
<li>If recover=reconnect, reconnect with any old live executors, provided
the executor's framework enabled checkpointing.</li>
<li>If recover=cleanup, kill any old live executors and exit. Use this
option when doing an incompatible agent or executor upgrade!
<strong>NOTE:</strong> If no checkpointing information exists, no recovery is performed
and the agent registers with the master as a new agent.</li>
</ul>
</li>
<li>
<p><code>recovery_timeout</code>: Amount of time allotted for the agent to
recover [Default: 15 mins].</p>
<ul>
<li>If the agent takes longer than <code>recovery_timeout</code> to recover, any
executors that are waiting to reconnect to the agent will self-terminate.
<strong>NOTE:</strong> If none of the frameworks have enabled checkpointing, the
executors and tasks running at an agent die when the agent dies and are
not recovered.</li>
</ul>
</li>
</ul>
<p>A restarted agent should reregister with master within a timeout (75 seconds
by default: see the <code>--max_agent_ping_timeouts</code> and <code>--agent_ping_timeout</code>
<a href="configuration.html">configuration flags</a>). If the agent takes longer than this
timeout to reregister, the master shuts down the agent, which in turn will
shutdown any live executors/tasks.</p>
<p>Therefore, it is highly recommended to automate the process of restarting an
agent, e.g. using a process supervisor such as <a href="http://mmonit.com/monit/">monit</a>
or <code>systemd</code>.</p>
<h2 id="known-issues-with-systemd-and-process-lifetime"><a class="header" href="#known-issues-with-systemd-and-process-lifetime">Known issues with <code>systemd</code> and process lifetime</a></h2>
<p>There is a known issue when using <code>systemd</code> to launch the <code>mesos-agent</code>. A
description of the problem can be found in <a href="https://issues.apache.org/jira/browse/MESOS-3425">MESOS-3425</a>
and all relevant work can be tracked in the epic <a href="https://issues.apache.org/jira/browse/MESOS-3007">MESOS-3007</a>.</p>
<p>This problem was fixed in Mesos <code>0.25.0</code> for the mesos containerizer when
cgroups isolation is enabled. Further fixes for the posix isolators and docker
containerizer are available in <code>0.25.1</code>, <code>0.26.1</code>, <code>0.27.1</code>, and <code>0.28.0</code>.</p>
<p>It is recommended that you use the default <a href="http://www.freedesktop.org/software/systemd/man/systemd.kill.html">KillMode</a>
for systemd processes, which is <code>control-group</code>, which kills all child processes
when the agent stops. This ensures that &quot;side-car&quot; processes such as the
<code>fetcher</code> and <code>perf</code> are terminated alongside the agent.
The systemd patches for Mesos explicitly move executors and their children into
a separate systemd slice, dissociating their lifetime from the agent. This
ensures the executors survive agent restarts.</p>
<p>The following excerpt of a <code>systemd</code> unit configuration file shows how to set
the flag explicitly:</p>
<pre><code>[Service]
ExecStart=/usr/bin/mesos-agent
KillMode=control-cgroup
</code></pre>
<h1 id="framework-rate-limiting"><a class="header" href="#framework-rate-limiting">Framework Rate Limiting</a></h1>
<p>Framework rate limiting is a feature introduced in Mesos 0.20.0.</p>
<h2 id="what-is-framework-rate-limiting"><a class="header" href="#what-is-framework-rate-limiting">What is Framework Rate Limiting</a></h2>
<p>In a multi-framework environment, this feature aims to protect the throughput of high-SLA (e.g., production, service) frameworks by having the master throttle messages from other (e.g., development, batch) frameworks.</p>
<p>To throttle messages from a framework, the Mesos cluster operator sets a <code>qps</code> (queries per seconds) value for each framework identified by its principal (You can also throttle a group of frameworks together but we'll assume individual frameworks in this doc unless otherwise stated; see the <code>RateLimits</code> <a href="https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto">Protobuf definition</a> and the configuration notes below). The master then promises not to process messages from that framework at a rate above <code>qps</code>. The outstanding messages are stored in memory on the master.</p>
<h2 id="rate-limits-configuration"><a class="header" href="#rate-limits-configuration">Rate Limits Configuration</a></h2>
<p>The following is a sample config file (in JSON format) which could be specified with the <code>--rate_limits</code> master flag.</p>
<pre><code>{
  &quot;limits&quot;: [
    {
      &quot;principal&quot;: &quot;foo&quot;,
      &quot;qps&quot;: 55.5
      &quot;capacity&quot;: 100000
    },
    {
      &quot;principal&quot;: &quot;bar&quot;,
      &quot;qps&quot;: 300
    },
    {
      &quot;principal&quot;: &quot;baz&quot;,
    }
  ],
  &quot;aggregate_default_qps&quot;: 333,
  &quot;aggregate_default_capacity&quot;: 1000000
}
</code></pre>
<p>In this example, framework <code>foo</code> is throttled at the configured <code>qps</code> and <code>capacity</code>, framework <code>bar</code> is given unlimited capacity and framework <code>baz</code> is not throttled at all. If there is a fourth framework <code>qux</code> or a framework without a principal connected to the master, it is throttled by the rules <code>aggregate_default_qps</code> and <code>aggregate_default_capacity</code>.</p>
<h3 id="configuration-notes"><a class="header" href="#configuration-notes">Configuration Notes</a></h3>
<p>Below are the fields in the JSON configuration.</p>
<ul>
<li><strong>principal</strong>: (Required) uniquely identifies the entity being throttled or given unlimited rate explicitly.
<ul>
<li>It should match the framework's <code>FrameworkInfo.principal</code> (See <a href="https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto">definition</a>).</li>
<li>You can have multiple frameworks use the same principal (e.g., some Mesos frameworks launch a new framework instance for each job), in which case the combined traffic from all frameworks using the same principal are throttled at the specified QPS.</li>
</ul>
</li>
<li><strong>qps</strong>: (Optional) queries per second, i.e., the rate.
<ul>
<li>Once set, the master guarantees that it does not process messages from this principal higher than this rate. However the master could be slower than this rate, especially if the specified rate is too high.</li>
<li>To explicitly give a framework unlimited rate (i.e., not throttling it), add an entry to <code>limits</code> without the qps.</li>
</ul>
</li>
<li><strong>capacity</strong>: (Optional) The number of <em>outstanding</em> messages frameworks of this principal can put on the master. If not specified, this principal is given unlimited capacity. Note that it is possible the queued messages use too much memory and cause the master to OOM if the capacity is set too high or not set.
<ul>
<li>NOTE: If <code>qps</code> is not specified, <code>capacity</code> is ignored.</li>
</ul>
</li>
<li>Use <strong>aggregate_default_qps</strong> and <strong>aggregate_default_capacity</strong> to safeguard the master from unspecified frameworks. All the frameworks not specified in <code>limits</code> get this default rate and capacity.
<ul>
<li>The rate and capacity are aggregate values for all of them, i.e., their combined traffic is throttled together.</li>
<li>Same as above, if <code>aggregate_default_qps</code> is not specified, <code>aggregate_default_capacity</code> is ignored.</li>
<li>If these fields are not present, the unspecified frameworks are not throttled.
This is an implicit way of giving frameworks unlimited rate compared to the explicit way above (using an entry in <code>limits</code> with only the principal).
We recommend using the explicit option especially when the master does not require authentication to prevent unexpected frameworks from overwhelming the master.</li>
</ul>
</li>
</ul>
<h2 id="using-framework-rate-limiting"><a class="header" href="#using-framework-rate-limiting">Using Framework Rate Limiting</a></h2>
<h3 id="monitoring-framework-traffic"><a class="header" href="#monitoring-framework-traffic">Monitoring Framework Traffic</a></h3>
<p>While a framework is registered with the master, the master exposes counters for all messages received and processed from that framework at its metrics endpoint: <code>http://&lt;master&gt;/metrics/snapshot</code>. For instance, framework <code>foo</code> has two message counters <code>frameworks/foo/messages_received</code> and <code>frameworks/foo/messages_processed</code>. Without framework rate limiting the two numbers should differ by little or none (because messages are processed ASAP) but when a framework is being throttled the difference indicates the outstanding messages as a result of the throttling.</p>
<p>By continuously monitoring the counters, you can derive the rate messages arrive and how fast the message queue length for the framework is growing (if it is throttled). This should depict the characteristics of the framework in terms of network traffic.</p>
<h2 id="configuring-rate-limits"><a class="header" href="#configuring-rate-limits">Configuring Rate Limits</a></h2>
<p>Since the goal for framework rate limiting is to prevent low-SLA frameworks from using <strong>too much</strong> resources and not to model their traffic and behavior as precisely as possible, you can start by using large <code>qps</code> values to throttle them. The fact that they are throttled (regardless of the configured <code>qps</code>) is already effective in giving messages from high-SLA frameworks higher priority because they are processed ASAP.</p>
<p>To calculate how much <code>capacity</code> the master can handle, you need to know the memory limit for the master process, the amount of memory it typically uses to serve similar workload without rate limiting (e.g., use <code>ps -o rss $MASTER_PID</code>) and average sizes of the framework messages (queued messages are stored as <a href="https://github.com/apache/mesos/blob/master/3rdparty/libprocess/include/process/message.hpp">serialized Protocol Buffers with a few additional fields</a>) and you should sum up all capacity values in the config.
However since this kind of calculation is imprecise, you should start with small values that tolerate reasonable temporary framework burstiness but far from the memory limit to leave enough headroom for the master and frameworks that don't have limited capacity.</p>
<h2 id="handling-capacity-exceeded-error"><a class="header" href="#handling-capacity-exceeded-error">Handling &quot;Capacity Exceeded&quot; Error</a></h2>
<p>When a framework <strong>exceeds the capacity</strong>, a FrameworkErrorMessage is sent back to the framework which will <a href="https://github.com/apache/mesos/blob/master/src/sched/sched.cpp">abort the scheduler driver and invoke the error() callback</a>. It doesn't kill any tasks or the scheduler itself. The framework developer can choose to restart or failover the scheduler instance to remedy the consequences of dropped messages (unless your framework doesn't assume all messages sent to the master are processed).</p>
<p>After version 0.20.0 we are going to iterate on this feature by having the master send an early alert when the message queue for this framework <strong>starts to build up</strong> (<a href="https://issues.apache.org/jira/browse/MESOS-1664">MESOS-1664</a>, consider it a &quot;soft limit&quot;). The scheduler can react by throttling itself (to avoid the error message) or ignoring this alert if it's a temporary burst by design.</p>
<p>Before the early alerting is implemented we <strong>don't recommend using the rate limiting feature to throttle production frameworks</strong> for now unless you are sure about the consequences of the error message. Of course it's OK to use it to protect production frameworks by throttling other frameworks and it doesn't have any effect on the master if it's not explicitly enabled.</p>
<h1 id="performing-node-maintenance-in-a-mesos-cluster"><a class="header" href="#performing-node-maintenance-in-a-mesos-cluster">Performing Node Maintenance in a Mesos Cluster</a></h1>
<p>Operators regularly need to perform maintenance tasks on machines that comprise
a Mesos cluster.  Most Mesos upgrades can be done without affecting running
tasks, but there are situations where maintenance may affect running tasks.
For example:</p>
<ul>
<li>Hardware repair</li>
<li>Kernel upgrades</li>
<li>Agent upgrades (e.g., adjusting agent attributes or resources)</li>
</ul>
<p>Before performing maintenance on an agent node in a Mesos cluster, it is
typically desirable to gracefully migrate tasks away from the node beforehand in
order to minimize service disruption when the machine is taken down. Mesos
provides several ways to accomplish this migration:</p>
<ul>
<li>Automatic agent draining, which does not explicitly require cooperation from
schedulers</li>
<li>Manual node draining, which allows operators to exercise precise control over
the task draining process</li>
<li>Maintenance primitives, which permit complex coordination but do require that
schedulers react to the maintenance-related messages that they receive</li>
</ul>
<h1 id="automatic-node-draining"><a class="header" href="#automatic-node-draining">Automatic Node Draining</a></h1>
<p>Node draining was added to provide a simple method for operators to drain tasks
from nodes on which they plan to perform maintenance, without requiring that
schedulers implement support for any maintenance-specific messages.</p>
<p>Initiating draining will cause all tasks on the target agent node to receive a
kill event immediately, assuming the agent is currently reachable. If the agent
is unreachable, initiation of the kill event will be delayed until the agent is
reachable by the master again. When the tasks receive a kill event, a SIGTERM
signal will be sent to the task to begin the killing process. Depending on the
particular task's behavior, this signal may be sufficient to terminate it. Some
tasks may use this signal to begin the process of graceful termination, which
may take some time. After some delay, a SIGKILL signal will be sent to the task,
which forcefully terminates the task if it is still running. The delay between
the SIGTERM and SIGKILL signals is determined by the length of the task's kill
grace period. If no grace period is set for the task, a default value of several
seconds will be used.</p>
<h2 id="initiating-draining-on-a-node"><a class="header" href="#initiating-draining-on-a-node">Initiating Draining on a Node</a></h2>
<p>To begin draining an agent, issue the operator API <a href="operator-http-api.html#drain_agent"><code>DRAIN_AGENT</code>
call</a> to the master:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DRAIN_AGENT&quot;, &quot;drain_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}}}' masterhost:5050/api/v1
</code></pre>
<p>This will immediately begin the process of killing all tasks on the agent. Once
draining has begun, it cannot be cancelled. To monitor the progress of the
draining process, you can inspect the state of the agent via the master operator
API <a href="operator-http-api.html#get_state"><code>GET_STATE</code></a> or
<a href="operator-http-api.html#get_agents"><code>GET_AGENTS</code></a> calls:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;GET_AGENTS&quot;}' masterhost:5050/api/v1
</code></pre>
<p>Locate the relevant agent and inspect its <code>drain_info.state</code> field. While
draining, the state will be <code>DRAINING</code>. When all tasks on the agent have
terminated, all their terminal status updates have been acknowledged by the
schedulers, and all offer operations on the agent have finished, draining is
complete and the agent's drain state will transition to <code>DRAINED</code>. At this
point, the node may be taken down for maintenance.</p>
<h2 id="options-for-automatic-node-draining"><a class="header" href="#options-for-automatic-node-draining">Options for Automatic Node Draining</a></h2>
<p>You may set an upper bound on the kill grace period of draining tasks by
specifying the <code>max_grace_period</code> option when draining:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DRAIN_AGENT&quot;, &quot;drain_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}, &quot;max_grace_period&quot;: &quot;10mins&quot;}}' masterhost:5050/api/v1
</code></pre>
<p>In cases where you know that the node being drained will not return after
draining is complete, and you would like it to be automatically permanently
removed from the cluster, you may specify the <code>mark_gone</code> option:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DRAIN_AGENT&quot;, &quot;drain_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}, &quot;mark_gone&quot;: true}}' masterhost:5050/api/v1
</code></pre>
<p>This can be useful, for example, in the case of autoscaled cloud instances,
where an instance is being scaled down and will never return. This is equivalent
to issuing the <a href="operator-http-api.html#mark_agent_gone"><code>MARK_AGENT_GONE</code></a> call on
the agent immediately after it finishes draining. WARNING: draining with the
<code>mark_gone</code> option is irreversible, and results in the loss of all local
persistent data on the agent node. Use this option with caution!</p>
<h2 id="reactivating-a-node-after-maintenance"><a class="header" href="#reactivating-a-node-after-maintenance">Reactivating a Node After Maintenance</a></h2>
<p>Once maintenance on an agent is complete, it must be reactivated so that it can
reregister with the master and rejoin the cluster. You may use the master
operator API <a href="operator-http-api.html#reactivate_agent"><code>REACTIVATE_AGENT</code></a> call to
accomplish this:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;REACTIVATE_AGENT&quot;, &quot;reactivate_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}}}' masterhost:5050/api/v1
</code></pre>
<h1 id="manual-node-draining"><a class="header" href="#manual-node-draining">Manual Node Draining</a></h1>
<p>If you require greater control over the draining process, you may be able to
drain the agent manually using both the Mesos operator API as well as APIs
exposed by the schedulers running tasks on the agent.</p>
<h2 id="deactivating-an-agent"><a class="header" href="#deactivating-an-agent">Deactivating an Agent</a></h2>
<p>The first step in the manual draining process is agent deactivation, which
prevents new tasks from launching on the target agent:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DEACTIVATE_AGENT&quot;, &quot;deactivate_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}}}' masterhost:5050/api/v1
</code></pre>
<p>If you receive a <code>200 OK</code> response, then the agent has been deactivated. You can
confirm the deactivation state of any agent by inspecting its <code>deactivated</code>
field in the response of the master operator API
<a href="operator-http-api.html#get_state"><code>GET_STATE</code></a> or
<a href="operator-http-api.html#get_agents"><code>GET_AGENTS</code></a> calls. Once the agent is
deactivated, you can use the APIs exposed by the schedulers responsible for the
tasks running on the agent to kill those tasks manually. To verify that all
tasks on the agent have terminated and their terminal status updates have been
acknowledged by the schedulers, ensure that the <code>pending_tasks</code>, <code>queued_tasks</code>,
and <code>launched_tasks</code> fields in the response to the
<a href="operator-http-api.html#get_tasks-1"><code>GET_TASKS</code></a> agent operator API call are
empty:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;GET_TASKS&quot;}' agenthost:5051/api/v1
</code></pre>
<p>If you are making use of volumes backed by network storage on the target agent,
it's possible that there may be a long-running offer operation on the agent
which has not yet finished. To check if this is the case, issue the agent
operator API <a href="operator-http-api.html#get_operations-1"><code>GET_OPERATIONS</code></a> call to
the agent:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;GET_OPERATIONS&quot;}' agenthost:5051/api/v1
</code></pre>
<p>If any operations have a <code>latest_status</code> with a state of <code>OPERATION_PENDING</code>,
you should wait for them to finish before taking down the node. Unfortunately,
it is not possible to cancel or forcefully terminate such storage operations. If
such an operation becomes stuck in the pending state, you should inspect the
relevant storage backend for any issues.</p>
<p>Once all tasks on the agent have terminated and all offer operations are
finished, the node may be taken down for maintenance. Once maintenance is
complete, the procedure for reactivating the node is the same as that detailed
in the section on automatic node draining.</p>
<h1 id="maintenance-primitives"><a class="header" href="#maintenance-primitives">Maintenance Primitives</a></h1>
<p>Frameworks require visibility into any actions that disrupt cluster operation
in order to meet Service Level Agreements or to ensure uninterrupted services
for their end users.  Therefore, to reconcile the requirements of frameworks
and operators, frameworks must be aware of planned maintenance events and
operators must be aware of frameworks' ability to adapt to maintenance.
Maintenance primitives add a layer to facilitate communication between the
frameworks and operator.</p>
<h2 id="terminology"><a class="header" href="#terminology">Terminology</a></h2>
<p>For the purpose of this section, an &quot;Operator&quot; is a person, tool, or script
that manages a Mesos cluster.</p>
<p>Maintenance primitives add several new concepts to Mesos. Those concepts are:</p>
<ul>
<li><strong>Maintenance</strong>: An operation that makes resources on a machine unavailable,
either temporarily or permanently.</li>
<li><strong>Maintenance window</strong>: A set of machines and an associated time interval during
which some maintenance is planned on those machines.</li>
<li><strong>Maintenance schedule</strong>: A list of maintenance windows.
A single machine may only appear in a schedule once.</li>
<li><strong>Unavailability</strong>: An operator-specified interval, defined by a start time
and duration, during which an associated machine may become unavailable.
In general, no assumptions should be made about the availability of the
machine (or resources) after the unavailability.</li>
<li><strong>Drain</strong>: An interval between the scheduling of maintenance and when the
machine(s) become unavailable.  Offers sent with resources from draining
machines will contain unavailability information.  Frameworks running on
draining machines will receive inverse offers (see next).  Frameworks
utilizing resources on affected machines are expected either to take
preemptive steps to prepare for the unavailability; or to communicate the
framework's inability to conform to the maintenance schedule.</li>
<li><strong>Inverse offer</strong>: A communication mechanism for the master to ask for
resources back from a framework.  This notifies frameworks about any
unavailability and gives frameworks a mechanism to respond about their
ability to comply.  Inverse offers are similar to offers in that they
can be accepted, declined, re-offered, and rescinded.</li>
</ul>
<p><strong>Note</strong>: Unavailability and inverse offers are not specific to maintenance.
The same concepts can be used for non-maintenance goals, such as reallocating
resources or resource preemption.</p>
<h2 id="how-does-it-work"><a class="header" href="#how-does-it-work">How does it work?</a></h2>
<p>Maintenance primitives were introduced in Mesos 0.25.0.  Several machine
maintenance modes were also introduced.  Those modes are illustrated below.</p>
<p><img src="images/maintenance-primitives-modes.png" alt="Maintenance mode transitions" /></p>
<p>All mode transitions must be initiated by the operator.  Mesos will not
change the mode of any machine, regardless of the estimate provided in
the maintenance schedule.</p>
<h3 id="scheduling-maintenance"><a class="header" href="#scheduling-maintenance">Scheduling maintenance</a></h3>
<p>A machine is transitioned from Up mode to Draining mode as soon as it is
scheduled for maintenance.  To transition a machine into Draining mode, an
operator constructs a maintenance schedule as a JSON document and posts it to
the <a href="endpoints/master/maintenance/schedule.html">/maintenance/schedule</a> HTTP
endpoint on the Mesos master. Each Mesos cluster has a single maintenance
schedule; posting a new schedule replaces the previous schedule, if any.</p>
<p>See the definition of a <a href="https://github.com/apache/mesos/blob/016b02d7ed5a65bcad9261a133c8237c2df66e6e/include/mesos/maintenance/maintenance.proto#L48-L67">maintenance::Schedule</a>
and of <a href="https://github.com/apache/mesos/blob/016b02d7ed5a65bcad9261a133c8237c2df66e6e/include/mesos/v1/mesos.proto#L140-L154">Unavailability</a>.</p>
<p>In a production environment, the schedule should be constructed to ensure that
enough agents are operational at any given point in time to ensure
uninterrupted service by the frameworks.</p>
<p>For example, in a cluster of three machines, the operator might schedule two
machines for one hour of maintenance, followed by another hour for the last
machine.  The timestamps for unavailability are expressed in nanoseconds since
the Unix epoch (note that making reliable use of maintenance primitives requires
that the system clocks of all machines in the cluster are roughly synchronized).</p>
<p>The schedule might look like:</p>
<pre><code>{
  &quot;windows&quot; : [
    {
      &quot;machine_ids&quot; : [
        { &quot;hostname&quot; : &quot;machine1&quot;, &quot;ip&quot; : &quot;10.0.0.1&quot; },
        { &quot;hostname&quot; : &quot;machine2&quot;, &quot;ip&quot; : &quot;10.0.0.2&quot; }
      ],
      &quot;unavailability&quot; : {
        &quot;start&quot; : { &quot;nanoseconds&quot; : 1443830400000000000 },
        &quot;duration&quot; : { &quot;nanoseconds&quot; : 3600000000000 }
      }
    }, {
      &quot;machine_ids&quot; : [
        { &quot;hostname&quot; : &quot;machine3&quot;, &quot;ip&quot; : &quot;10.0.0.3&quot; }
      ],
      &quot;unavailability&quot; : {
        &quot;start&quot; : { &quot;nanoseconds&quot; : 1443834000000000000 },
        &quot;duration&quot; : { &quot;nanoseconds&quot; : 3600000000000 }
      }
    }
  ]
}
</code></pre>
<p>The operator can then post the schedule to the master's
<a href="endpoints/master/maintenance/schedule.html">/maintenance/schedule</a> endpoint:</p>
<pre><code>curl http://localhost:5050/maintenance/schedule \
  -H &quot;Content-type: application/json&quot; \
  -X POST \
  -d @schedule.json
</code></pre>
<p>The machines in a maintenance schedule do not need to be registered with the
Mesos master at the time when the schedule is set.  The operator may add a
machine to the maintenance schedule prior to launching an agent on the machine.
For example, this can be useful to prevent a faulty machine from launching an
agent on boot.</p>
<p><strong>Note</strong>: Each machine in the maintenance schedule should have as
complete information as possible.  In order for Mesos to recognize an agent
as coming from a particular machine, both the <code>hostname</code> and <code>ip</code> fields must
match.  Any omitted data defaults to the empty string <code>&quot;&quot;</code>.  If there are
multiple hostnames or IPs for a machine, the machine's fields need to match
what the agent announces to the master.  If there is any ambiguity in a
machine's configuration, the operator should use the <code>--hostname</code> and <code>--ip</code>
options when starting agents.</p>
<p>The master checks that a maintenance schedule has the following properties:</p>
<ul>
<li>Each maintenance window in the schedule must have at least one machine
and a specified unavailability interval.</li>
<li>Each machine must only appear in the schedule once.</li>
<li>Each machine must have at least a hostname or IP included.
The hostname is not case-sensitive.</li>
<li>All machines that are in Down mode must be present in the schedule.
This is required because this endpoint does not handle the transition
from Down mode to Up mode.</li>
</ul>
<p>If any of these properties are not met, the maintenance schedule is rejected
with a corresponding error message and the master's state is not changed.</p>
<p>To update the maintenance schedule, the operator should first read the current
schedule, make any necessary changes, and then post the modified schedule. The
current maintenance schedule can be obtained by sending a GET request to the
master's <code>/maintenance/schedule</code> endpoint.</p>
<p>To cancel the maintenance schedule, the operator should post an empty schedule.</p>
<h3 id="draining-mode"><a class="header" href="#draining-mode">Draining mode</a></h3>
<p>As soon as a schedule is posted to the Mesos master, the following things occur:</p>
<ul>
<li>The schedule is stored in the <a href="replicated-log-internals.html">replicated log</a>.
This means the schedule is persisted in case of master failover.</li>
<li>All machines in the schedule are immediately transitioned into Draining
mode.  The mode of each machine is also persisted in the replicated log.</li>
<li>All frameworks using resources on affected agents are immediately
notified.  Existing offers from the affected agents are rescinded
and re-sent with additional unavailability data.  All frameworks using
resources from the affected agents are given inverse offers.</li>
<li>New offers from the affected agents will also include
the additional unavailability data.</li>
</ul>
<p>Frameworks should use this additional information to schedule tasks in a
maintenance-aware fashion. Exactly how to do this depends on the design
requirements of each scheduler, but tasks should typically be scheduled in a way
that maximizes utilization but that also attempts to vacate machines before that
machine's advertised unavailability period occurs. A scheduler might choose to
place long-running tasks on machines with no unavailability, or failing that, on
machines whose unavailability is the furthest away.</p>
<p>How a framework responds to an inverse offer indicates its ability to conform to
the maintenance schedule. Accepting an inverse offer communicates that the
framework is okay with the current maintenance schedule, given the current state
of the framework's resources.  The master and operator should interpret
acceptance as a best-effort promise by the framework to free all the resources
contained in the inverse offer before the start of the unavailability
interval. Declining an inverse offer is an advisory notice to the operator that
the framework is unable or unlikely to meet to the maintenance schedule.</p>
<p>For example:</p>
<ul>
<li>A data store may choose to start a new replica if one of its agents is
scheduled for maintenance. The data store should accept an inverse offer if it
can reasonably copy the data on the machine to a new host before the
unavailability interval described in the inverse offer begins. Otherwise, the
data store should decline the offer.</li>
<li>A stateful task on an agent with an impending unavailability may be migrated
to another available agent.  If the framework has sufficient resources to do
so, it would accept any inverse offers.  Otherwise, it would decline them.</li>
</ul>
<p>A framework can use a filter to control when it wants to be contacted again
with an inverse offer.  This is useful since future circumstances may change
the viability of the maintenance schedule.  The filter for inverse offers is
identical to the existing mechanism for re-offering offers to frameworks.</p>
<p><strong>Note</strong>: Accepting or declining an inverse offer does not result in
immediate changes in the maintenance schedule or in the way Mesos acts.
Inverse offers only represent extra information that frameworks may
find useful. In the same manner, rejecting or accepting an inverse offer is a
hint for an operator. The operator may or may not choose to take that hint
into account.</p>
<h3 id="starting-maintenance"><a class="header" href="#starting-maintenance">Starting maintenance</a></h3>
<p>The operator starts maintenance by posting a list of machines to the
<a href="endpoints/master/machine/down.html">/machine/down</a> HTTP endpoint. The list of
machines is specified in JSON format; each element of the list is a
<a href="https://github.com/apache/mesos/blob/016b02d7ed5a65bcad9261a133c8237c2df66e6e/include/mesos/v1/mesos.proto#L157-L167">MachineID</a>.</p>
<p>For example, to start maintenance on two machines:</p>
<pre><code>[
  { &quot;hostname&quot; : &quot;machine1&quot;, &quot;ip&quot; : &quot;10.0.0.1&quot; },
  { &quot;hostname&quot; : &quot;machine2&quot;, &quot;ip&quot; : &quot;10.0.0.2&quot; }
]
</code></pre>
<pre><code>curl http://localhost:5050/machine/down \
  -H &quot;Content-type: application/json&quot; \
  -X POST \
  -d @machines.json
</code></pre>
<p>The master checks that a list of machines has the following properties:</p>
<ul>
<li>The list of machines must not be empty.</li>
<li>Each machine must only appear once.</li>
<li>Each machine must have at least a hostname or IP included.
The hostname is not case-sensitive.</li>
<li>If a machine's IP is included, it must be correctly formed.</li>
<li>All listed machines must be present in the schedule.</li>
</ul>
<p>If any of these properties are not met, the operation is rejected with a
corresponding error message and the master's state is not changed.</p>
<p>The operator can start maintenance on any machine that is scheduled for
maintenance. Machines that are not scheduled for maintenance cannot be directly
transitioned from Up mode into Down mode.  However, the operator may schedule a
machine for maintenance with a timestamp equal to the current time or in the
past, and then immediately start maintenance on that machine.</p>
<p>This endpoint can be used to start maintenance on machines that are not
currently registered with the Mesos master. This can be useful if a machine has
failed and the operator intends to remove it from the cluster; starting
maintenance on the machine prevents the machine from being accidentally rebooted
and rejoining the Mesos cluster.</p>
<p>The operator must explicitly transition a machine from Draining to Down
mode. That is, Mesos will keep a machine in Draining mode even if the
unavailability window arrives or passes.  This means that the operation of the
machine is not disrupted in any way and offers (with unavailability information)
are still sent for this machine.</p>
<p>When maintenance is triggered by the operator, all agents on the machine are
told to shutdown.  These agents are removed from the master, which means that a
<code>TASK_LOST</code> status update will be sent for every task running on each of those
agents. The scheduler driver's <code>slaveLost</code> callback will also be invoked for
each of the removed agents. Any agents on machines in maintenance are also
prevented from reregistering with the master in the future (until maintenance
is completed and the machine is brought back up).</p>
<h3 id="completing-maintenance"><a class="header" href="#completing-maintenance">Completing maintenance</a></h3>
<p>When maintenance is complete or if maintenance needs to be cancelled,
the operator can stop maintenance.  The process is very similar
to starting maintenance (same validation criteria as the previous section).
The operator posts a list of machines to the master's <a href="endpoints/master/machine/up.html">/machine/up</a> endpoint:</p>
<pre><code>[
  { &quot;hostname&quot; : &quot;machine1&quot;, &quot;ip&quot; : &quot;10.0.0.1&quot; },
  { &quot;hostname&quot; : &quot;machine2&quot;, &quot;ip&quot; : &quot;10.0.0.2&quot; }
]
</code></pre>
<pre><code>curl http://localhost:5050/machine/up \
  -H &quot;Content-type: application/json&quot; \
  -X POST \
  -d @machines.json
</code></pre>
<p><strong>Note</strong>: The duration of the maintenance window, as indicated by the
&quot;unavailability&quot; field in the maintenance schedule, is a best-effort guess made
by the operator.  Stopping maintenance before the end of the unavailability
interval is allowed, as is stopping maintenance after the end of the
unavailability interval.  Machines are never automatically transitioned out of
maintenance.</p>
<p>Frameworks are informed about the completion or cancellation of maintenance when
offers from that machine start being sent.  There is no explicit mechanism for
notifying frameworks when maintenance has finished.  After maintenance has
finished, new offers are no longer tagged with unavailability and inverse offers
are no longer sent.  Also, agents running on the machine will be allowed to
register with the Mesos master.</p>
<h3 id="viewing-maintenance-status"><a class="header" href="#viewing-maintenance-status">Viewing maintenance status</a></h3>
<p>The current maintenance status (Up, Draining, or Down) of each machine in the
cluster can be viewed by accessing the master's
<a href="endpoints/master/maintenance/status.html">/maintenance/status</a> HTTP endpoint. For
each machine that is Draining, this endpoint also includes the frameworks' responses to
inverse offers for resources on that machine. For more information, see the
format of the <a href="https://github.com/apache/mesos/blob/fa36917dd142f66924c5f7ed689b87d5ceabbf79/include/mesos/maintenance/maintenance.proto#L73-L84">ClusterStatus message</a>.</p>
<blockquote>
<p>NOTE: The format of the data returned by this endpoint may change in a
future release of Mesos.</p>
</blockquote>
<hr />
<h2>title: Apache Mesos - Upgrading Mesos
layout: documentation</h2>
<h1 id="upgrading-mesos"><a class="header" href="#upgrading-mesos">Upgrading Mesos</a></h1>
<p>This document serves as a guide for users who wish to upgrade an existing Mesos cluster. Some versions require particular upgrade techniques when upgrading a running cluster. Some upgrades will have incompatible changes.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>This section provides an overview of the changes for each version (in particular when upgrading from the next lower version). For more details please check the respective sections below.</p>
<p>We categorize the changes as follows:</p>
<pre><code>A New feature/behavior
C Changed feature/behavior
D Deprecated feature/behavior
R Removed feature/behavior
</code></pre>
<table class="table table-bordered" style="table-layout: fixed;">
  <thead>
    <tr>
      <th width="10%">
        Version
      </th>
      <th width="18%">
        Mesos Core
      </th>
      <th width="18%">
        Flags
      </th>
      <th width="18%">
        Framework API
      </th>
      <th width="18%">
        Module API
      </th>
      <th width="18%">
        Endpoints
      </th>
    </tr>
  </thead>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.10.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-10-x-ssl-env-var-rename">Renamed LIBPROCESS_SSL_VERIFY_CERT and LIBPROCESS_SSL_REQUIRE_CERT environment variables.</a></li>
      <li>D <a href="upgrades.html#1-10-x-limits-cfs-quota">CPU limits affect the function of the agent's `cgroups_enable_cfs` flag.</a></li>
    </ul>
 </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-10-x-agent-features">agent_features</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-10-x-synchronous-authorization">Authorizers must support synchronous authorization.</a></li>
      <li>AC <a href="upgrades.html#1-10-x-allocator-module-changes">Resource consumption is exposed to allocators.</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-10-x-tasks-pending-authoirization-deprecated">v1 GetTasks pending_tasks</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.9.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-9-x-quota-guarantees">Quota Limits</a></li>
      <li>A <a href="upgrades.html#1-9-x-linux-nnp-isolator">Linux NNP isolator</a></li>
      <li>A <a href="upgrades.html#1-9-x-hostname-validation-scheme">hostname_validation_scheme</a></li>
      <li>C <a href="upgrades.html#1-9-x-client-certificate-verification">TLS certificate verification behaviour</a></li>
      <li>C <a href="upgrades.html#1-9-x-configurable-ipc">Configurable IPC namespace and /dev/shm</a></li>
      <li>A <a href="upgrades.html#1-9-x-automatic-agent-draining">Automatic Agent Draining</a></li>
    </ul>
 </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-9-x-docker-ignore-runtime">docker_ignore_runtime</a></li>
      <li>A <a href="upgrades.html#1-9-x-configurable-ipc">disallow_sharing_agent_ipc_namespace</a></li>
      <li>A <a href="upgrades.html#1-9-x-configurable-ipc">default_container_shm_size</a></li>
      <li>C <a href="upgrades.html#1-9-x-agent-features">agent_features</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-9-x-configurable-ipc">LinuxInfo.ipc_mode and LinuxInfo.shm_size</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-9-x-update-quota">SET_QUOTA and REMOVE QUOTA deprecated
            in favor of UPDATE_QUOTA</a></li>
      <li>D <a href="upgrades.html#1-9-x-quota-guarantees">Quota guarantees deprecated in favor
            of using quota limits</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.8.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-8-x-linux-seccomp-isolator">Linux Seccomp isolator</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-8-x-linux-seccomp-isolator">seccomp_config_dir</a></li>
      <li>A <a href="upgrades.html#1-8-x-linux-seccomp-isolator">seccomp_profile_name</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.7.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-7-x-linux-devices-isolator">Linux devices isolator</a></li>
      <li>A <a href="upgrades.html#1-7-x-auto-load-subsystems">Automatically load local enabled cgroups subsystems</a></li>
      <li>A <a href="upgrades.html#1-7-x-container-specific-cgroups-mounts">Container-specific cgroups mounts</a></li>
      <li>A <a href="upgrades.html#1-7-x-volume-mode-support">Volume mode support</a></li>
      <li>A <a href="upgrades.html#1-7-x-resource-provider-acls">Resource Provider ACLs</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-7-x-enforce-container-ports">enforce_container_ports</a></li>
      <li>A <a href="upgrades.html#1-7-x-gc-non-executor-container-sandboxes">gc_non_executor_container_sandboxes</a></li>
      <li>A <a href="upgrades.html#1-7-x-network-cni-root-dir-persist">network_cni_root_dir_persist</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-7-x-create-disk">`CREATE_DISK` and `DESTROY_DISK` operations and ACLs</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-7-x-container-logger">ContainerLogger module interface changes</a></li>
      <li>C <a href="upgrades.html#1-7-x-isolator-recover">Isolator::recover module interface changes</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-7-x-json-serialization">JSON serialization changes</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.6.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-6-x-grpc-requirement">Requirement for gRPC library</a></li>
      <li>C <a href="upgrades.html#1-6-x-csi-support">CSI v0.2 Support</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-6-x-fetcher-stall-timeout">fetcher_stall_timeout</a></li>
    </ul>
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-6-x-xfs-kill-containers">xfs_kill_containers</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-6-x-disk-profile-adaptor">Disk profile adaptor module changes</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.5.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-5-x-task-starting">Built-in executors send a TASK_STARTING update</a></li>
      <li>A <a href="upgrades.html#1-5-x-network-ports-isolator">Network ports isolator</a></li>
      <li>C <a href="upgrades.html#1-5-x-relative-disk-source-root-path">Relative source root paths for disk resources</a></li>
      <li>A <a href="upgrades.html#1-5-x-reconfiguration-policy">Agent state recovery after resource changes</a></li>
      <li>C <a href="upgrades.html#1-5-x-protobuf-requirement">Requirement for Protobuf library</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-5-x-network-ports-isolator">container_ports_watch_interval</a></li>
      <li>A <a href="upgrades.html#1-5-x-network-ports-isolator">check_agent_port_range_only</a></li>
      <li>D <a href="upgrades.html#1-5-x-executor-secret-key">executor_secret_key</a></li>
      <li>A <a href="upgrades.html#1-5-x-reconfiguration-policy">reconfiguration_policy</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-5-x-task-resource-limitation">Added the TaskStatus.limitation message</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-5-x-get-containers">Allowed to view nested/standalone containers</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.4.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-4-x-ambient-capabilities">Container capabilities are made ambient if supported</a></li>
      <li>A <a href="upgrades.html#1-4-x-bounding-capabilities">Support for explicit bounding capabilities</a></li>
      <li>C <a href="upgrades.html#1-4-x-agent-recovery">Agent recovery post reboot</a></li>
      <li>C <a href="upgrades.html#1-4-x-xfs-no-enforce">XFS disk isolator support for not enforcing disk limits</a></li>
      <li>C <a href="upgrades.html#1-4-x-update-minimal-docker-version">Update the minimal supported Docker version</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-4-x-agent-capabilities-flags">effective_capabilities</a></li>
      <li>A <a href="upgrades.html#1-4-x-agent-capabilities-flags">bounding-capabilities</a></li>
      <li>D <a href="upgrades.html#1-4-x-agent-capabilities-flags">allowed-capabilities</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-4-x-bounding-capabilities">Support for explicit setting bounding capabilities</a></li>
      <li>D <a href="upgrades.html#1-4-x-linuxinfo-capabilities">LinuxInfo.effective_capabilities deprecates LinuxInfo.capabilities</a></li>
      <li>C <a href="upgrades.html#1-4-x-mesos-library">`Resources` class in the internal Mesos C++ library only supports post-`RESERVATION_REFINEMENT` format</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-4-x-allocator-update-slave">Changed semantics of Allocator::updateSlave</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.3.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-3-x-disallow-old-agents">Prevent registration by old Mesos agents</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-3-x-setquota-removequota-acl">--acls (set_quotas and remove_quotas)</a></li>
      <li>R <a href="upgrades.html#1-3-x-shutdown-framework-acl">--acls (shutdown_frameworks)</a></li>
      <li>A <a href="upgrades.html#1-3-x-executor-authentication">authenticate_http_executors</a></li>
      <li>A <a href="upgrades.html#1-3-x-executor-authentication">executor_secret_key</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-3-x-multi-role-support">MULTI_ROLE support</a></li>
      <li>D <a href="upgrades.html#1-3-x-framework-info-role">FrameworkInfo.roles deprecates FrameworkInfo.role</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-3-x-allocator-interface-change">Allocator MULTI_ROLE interface changes</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-3-x-endpoints-roles">MULTI_ROLE deprecates 'role' field in endpoints</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.2.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-2-1-disallow-old-agents">Prevent registration by old Mesos agents</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-2-x-heartbeat-flag">http_heartbeat_interval</a></li>
      <li>A <a href="upgrades.html#1-2-x-backend-flag">image_provisioner_backend</a></li>
      <li>A <a href="upgrades.html#1-2-x-unreachable-flag">max_unreachable_tasks_per_framework</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-2-x-revive-suppress">Revive and Suppress v1 scheduler Calls</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-2-x-container-logger-interface">Container Logger prepare method</a></li>
      <li>C <a href="upgrades.html#1-2-x-allocator-module-changes">Allocator module changes</a></li>
      <li>A <a href="upgrades.html#1-2-x-new-authz-actions">New Authorizer module actions</a></li>
      <li>D <a href="upgrades.html#1-2-x-renamed-authz-actions">Renamed Authorizer module actions (deprecated old aliases)</a></li>
      <li>R <a href="upgrades.html#1-2-x-removed-hooks">Removed slavePreLaunchDockerEnvironmentDecorator and slavePreLaunchDockerHook</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-2-x-debug-endpoints">LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT, ATTACH_CONTAINER_OUTPUT</a></li>
      <li>D <a href="upgrades.html#1-2-x-recovered-frameworks">v1 GetFrameworks recovered_frameworks</a></li>
      <li>D <a href="upgrades.html#1-2-x-orphan-executors">v1 GetExecutors orphan_executors</a></li>
      <li>D <a href="upgrades.html#1-2-x-orphan-tasks">v1 GetTasks orphan_tasks</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.1.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-1-x-container-logger-interface">Container Logger recovery method</a></li>
      <li>C <a href="upgrades.html#1-1-x-allocator-updateallocation">Allocator updateAllocation method</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.0.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>CD <a href="upgrades.html#1-0-x-allocator-metrics">Allocator Metrics</a></li>
      <li>C <a href="upgrades.html#1-0-x-persistent-volume">Destruction of persistent volumes</a></li>
      <li>C <a href="upgrades.html#1-0-x-slave">Slave to Agent rename</a></li>
      <li>C <a href="upgrades.html#1-0-x-quota-acls">Quota ACLs</a></li>
      <li>R <a href="upgrades.html#1-0-x-executor-environment-variables">Executor environment variables inheritance</a></li>
      <li>R <a href="upgrades.html#1-0-x-deprecated-fields-in-container-config">Deprecated fields in ContainerConfig</a></li>
      <li>C <a href="upgrades.html#1-0-x-persistent-volume-ownership">Persistent volume ownership</a></li>
      <li>C <a href="upgrades.html#1-0-x-fetcher-user">Fetcher assumes same user as task</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-0-x-docker-timeout-flag">docker_stop_timeout</a></li>
      <li>D <a href="upgrades.html#1-0-x-credentials-file">credential(s) (plain text format)</a></li>
      <li>C <a href="upgrades.html#1-0-x-slave">Slave to Agent rename</a></li>
      <li>R <a href="upgrades.html#1-0-x-workdir">work_dir default value</a></li>
      <li>D <a href="upgrades.html#1-0-x-deprecated-ssl-env-variables">SSL environment variables</a></li>
      <li>ACD <a href="upgrades.html#1-0-x-http-authentication-flags">HTTP authentication</a></li>
      <li>R <a href="upgrades.html#1-0-x-registry-strict">registry_strict</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>DC <a href="upgrades.html#1-0-x-executorinfo">ExecutorInfo.source</a></li>
      <li>A <a href="upgrades.html#1-0-x-v1-commandinfo">CommandInfo.URI output_file</a></li>
      <li>C <a href="upgrades.html#1-0-x-scheduler-proto">scheduler.proto optional fields</a></li>
      <li>C <a href="upgrades.html#1-0-x-executor-proto">executor.proto optional fields</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-0-x-authorizer">Authorizer</a></li>
      <li>C <a href="upgrades.html#1-0-x-allocator">Allocator</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-0-x-status-code">HTTP return codes</a></li>
      <li>R <a href="upgrades.html#1-0-x-status-code">/observe</a></li>
      <li>C <a href="upgrades.html#1-0-x-endpoint-authorization">Added authorization</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.28.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-28-x-resource-precision">Resource Precision</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-28-x-autherization-acls">Authentication ACLs</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.27.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#0-27-x-implicit-roles">--roles</a></li>
      <li>D <a href="upgrades.html#0-27-x-acl-shutdown-flag">--acls (shutdown_frameworks)</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-27-x-executor-lost-callback">executorLost callback</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-27-x-allocator-api">Allocator API</a></li>
      <li>C <a href="upgrades.html#0-27-x-isolator-api">Isolator API</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.26.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-26-x-taskstatus-reason">TaskStatus::Reason Enum</a></li>
      <li>C <a href="upgrades.html#0-26-x-credential-protobuf">Credential Protobuf</a></li>
      <li>C <a href="upgrades.html#0-26-x-network-info-protobuf">NetworkInfo Protobuf</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-26-x-state-endpoint">State Endpoint</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.25.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-25-x-scheduler-bindings">C++/Java/Python Scheduler Bindings</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#0-25-x-json-endpoints">*.json Endpoints</a></li>
    </ul>
  </td>
</tr>
</table>
<h2 id="upgrading-from-19x-to-110x"><a class="header" href="#upgrading-from-19x-to-110x">Upgrading from 1.9.x to 1.10.x</a></h2>
<p><a name="1-10-x-ssl-env-var-rename"></a></p>
<ul>
<li>The canonical name for the environment variable <code>LIBPROCESS_SSL_VERIFY_CERT</code> was changed to <code>LIBPROCESS_SSL_VERIFY_SERVER_CERT</code>.
The canonical name for the environment variable <code>LIBPROCESS_SSL_REQUIRE_CERT</code> was changed to <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code>.
The old names will continue to work as before, but operators are encouraged to update their configuration to reduce confusion.</li>
</ul>
<p><a name="1-10-x-limits-cfs-quota"></a></p>
<ul>
<li>The Mesos agent's <code>cgroups_enable_cfs</code> flag previously controlled whether or not CFS quota would be used for all tasks on the agent. Resource limits have been added to tasks, and when a CPU limit is specified on a task, the agent will now apply a CFS quota regardless of the value of <code>cgroups_enable_cfs</code>.</li>
</ul>
<p><a name="1-10-x-agent-features"></a></p>
<ul>
<li>The Mesos agent now requires the new <code>TASK_RESOURCE_LIMITS</code> feature. This capability is set by default, but if the <code>--agent_features</code> flag is specified explicitly, <code>TASK_RESOURCE_LIMITS</code> must be included.</li>
</ul>
<p><a name="1-10-x-synchronous-authorization"></a></p>
<ul>
<li>Authorizers now must implement a method <code>getApprover(...)</code> (see the
<a href="authorization.html#implementing-an-authorizer">authorization documentation</a>
and <a href="https://issues.apache.org/jira/browse/MESOS-10056">MESOS-10056</a>)
that returns <code>ObjectApprover</code>s that are valid throughout their whole lifetime.
Keeping the state of an <code>ObjectApprover</code> up-to-date becomes a responsibility
of the authorizer. This is a <strong>breaking change</strong> for authorizer modules.</li>
</ul>
<p><a name="1-10-x-tasks-pending-authoirization-deprecated"></a></p>
<ul>
<li>The field <code>pending_tasks</code> in <code>GetTasks</code> master API call has been deprecated.
From now on, this field will be empty. Moreover, the notion of
<em>tasks pending authorization</em> no longer exists
(see <a href="https://issues.apache.org/jira/browse/MESOS-10056">MESOS-10056</a>).</li>
</ul>
<p><a name="1-10-x-allocator-module-changes"></a></p>
<ul>
<li>Allocator interface has been changed to supply allocator with information on
resources actually consumed by frameworks. A method
<code>transitionOfferedToAllocated(...)</code> has been added and the signature of
<code>recoverResources(...)</code> has been extended. Note that allocators must implement
these new/extended method signatures, but are free to ignore resource
consumption data provided by master.</li>
</ul>
<h2 id="upgrading-from-18x-to-19x"><a class="header" href="#upgrading-from-18x-to-19x">Upgrading from 1.8.x to 1.9.x</a></h2>
<p><a name="1-9-x-automatic-agent-draining"></a></p>
<ul>
<li>A new <code>DRAINING</code> state has been added to Mesos agents. Once an agent is draining, all tasks running on that agent are gracefully
killed and no offers for that agent are sent to schedulers, preventing the launching of new tasks.
Operators can put an agent into <code>DRAINING</code> state by using the <code>DRAIN_AGENT</code> operator API call.
See <a href="maintenance.html"><code>docs/maintenance</code></a> for details.</li>
</ul>
<p><a name="1-9-x-agent-features"></a></p>
<ul>
<li>The Mesos agent now requires the new <code>AGENT_DRAINING</code> feature. This capability is set by default, but if the <code>--agent_features</code> flag is specified explicitly, <code>AGENT_DRAINING</code> must be included.</li>
</ul>
<p><a name="1-9-x-linux-nnp-isolator"></a></p>
<ul>
<li>A new <a href="isolators/linux-nnp.html"><code>linux/nnp</code></a> isolator has been added. The isolator supports setting of the <code>no_new_privs</code> bit in the container, preventing tasks from acquiring additional privileges.</li>
</ul>
<p><a name="1-9-x-docker-ignore-runtime"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#docker_ignore_runtime"><code>--docker_ignore_runtime</code></a> flag has been added. This causes the agent to ignore any runtime configuration present in Docker images.</li>
</ul>
<p><a name="1-9-x-hostname-validation-scheme"></a></p>
<ul>
<li>A new libprocess TLS flag <code>--hostname_validation_scheme</code> along with the corresponding environment variable <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME</code>
has been added. Using this flag, users can configure the way libprocess performs hostname validation for TLS connections.
See <a href="ssl.html"><code>docs/ssl</code></a> for details.</li>
</ul>
<p><a name="1-9-x-client-certificate-verification"></a></p>
<ul>
<li>The semantics of the libprocess environment variables <code>LIBPROCESS_SSL_VERIFY_CERT</code> and <code>LIBPROCESS_SSL_REQUIRE_CERT</code> have been slightly updated such that
the former now only applies to client-mode and the latter only to server-mode connections. As part of this re-adjustment, the following two changes have
been introduced that might require changes for operators running Mesos in unusual TLS configurations.
<ul>
<li>Anonymous ciphers can not be used anymore when <code>LIBPROCESS_SSL_VERIFY_CERT</code> is set to true. This is because the use of anonymous ciphers enables
a malicious attacker to bypass certificate verification by choosing a certificate-less cipher.
Users that rely on anonymous ciphers being available should make sure that <code>LIBPROCESS_SSL_VERIFY_CERT</code> is set to false.</li>
<li>For incoming connections, certificates are not verified unless <code>LIBPROCESS_SSL_REQUIRE_CERT</code> is set to true.
This is because verifying the certificate can lead to false negatives, where a connection is aborted even though presenting no certificate at all
would have been successfull. Users that rely on incoming connection requests presenting valid TLS certificates should make sure that
the <code>LIBPROCESS_SSL_REQUIRE_CERT</code> option is set to true.</li>
</ul>
</li>
</ul>
<p><a name="1-9-x-configurable-ipc"></a></p>
<ul>
<li>The Mesos containerizer now supports configurable IPC namespace and /dev/shm. Container can be configured to have a private IPC namespace and /dev/shm or share them from its parent via the field <code>LinuxInfo.ipc_mode</code>, and the size of its private /dev/shm is also configurable via the field <code>LinuxInfo.shm_size</code>. Operators can control whether it is allowed to share host's IPC namespace and /dev/shm with top level containers via the agent flag <code>--disallow_sharing_agent_ipc_namespace</code>, and specify the default size of the /dev/shm for the container which has a private /dev/shm via the agent flag <code>--default_container_shm_size</code>.</li>
</ul>
<p><a name="1-9-x-update-quota"></a></p>
<ul>
<li>The <code>SET_QUOTA</code> and <code>REMOVE QUOTA</code> master calls are deprecated in favor of a new <code>UPDATE_QUOTA</code> master call.</li>
</ul>
<p><a name="1-9-x-quota-guarantees"></a></p>
<ul>
<li>Prior to Mesos 1.9, the quota related APIs only exposed quota &quot;guarantees&quot; which ensured a minimum amount of resources would be available to a role. Setting guarantees also set implicit quota limits. In Mesos 1.9+, quota limits are now exposed directly.
<ul>
<li>Quota guarantees are now deprecated in favor of using only quota limits. Enforcement of quota guarantees required that Mesos holds back enough resources to meet all of the unsatisfied quota guarantees. Since Mesos is moving towards an optimistic offer model (to improve multi-role / multi- scheduler scalability, see MESOS-1607), it will become no longer possible to enforce quota guarantees by holding back resources. In such a model, quota limits are simple to enforce, but quota guarantees would require a complex &quot;effective limit&quot; propagation model to leave space for unsatisfied guarantees.</li>
<li>For these reasons, quota guarantees, while still functional in Mesos 1.9, are now deprecated. A combination of limits and priority based preemption will be simpler in an optimistic offer model.</li>
</ul>
</li>
</ul>
<h2 id="upgrading-from-17x-to-18x"><a class="header" href="#upgrading-from-17x-to-18x">Upgrading from 1.7.x to 1.8.x</a></h2>
<p><a name="1-8-x-linux-seccomp-isolator"></a></p>
<ul>
<li>A new <a href="isolators/linux-seccomp.html"><code>linux/seccomp</code></a> isolator has been added. The isolator supports the following new agent flags:
<ul>
<li><code>--seccomp_config_dir</code> specifies the directory path of the Seccomp profiles.</li>
<li><code>--seccomp_profile_name</code> specifies the path of the default Seccomp profile relative to the <code>seccomp_config_dir</code>.</li>
</ul>
</li>
</ul>
<h2 id="upgrading-from-16x-to-17x"><a class="header" href="#upgrading-from-16x-to-17x">Upgrading from 1.6.x to 1.7.x</a></h2>
<p><a name="1-7-x-linux-devices-isolator"></a></p>
<ul>
<li>A new <a href="isolators/linux-devices.html"><code>linux/devices</code></a> isolator has been
added. This isolator automatically populates containers with devices
that have been whitelisted with the <code>--allowed_devices</code> agent flag.</li>
</ul>
<p><a name="1-7-x-auto-load-subsystems"></a></p>
<ul>
<li>A new option <code>cgroups/all</code> has been added to the agent flag <code>--isolation</code>. This allows cgroups isolator to automatically load all the local enabled cgroups subsystems. If this option is specified in the agent flag <code>--isolation</code> along with other cgroups related options (e.g., <code>cgroups/cpu</code>), those options will be just ignored.</li>
</ul>
<p><a name="1-7-x-container-specific-cgroups-mounts"></a></p>
<ul>
<li>Added container-specific cgroups mounts under <code>/sys/fs/cgroup</code> to containers with image launched by Mesos containerizer.</li>
</ul>
<p><a name="1-7-x-volume-mode-support"></a></p>
<ul>
<li>Previously the <code>HOST_PATH</code>, <code>SANDBOX_PATH</code>, <code>IMAGE</code>, <code>SECRET</code>, and <code>DOCKER_VOLUME</code> volumes were always mounted for container in read-write mode, i.e., the <code>Volume.mode</code> field was not honored. Now we will mount these volumes based on the <code>Volume.mode</code> field so framework can choose to mount the volume for the container in either read-write mode or read-only mode.</li>
</ul>
<p><a name="1-7-x-create-disk"></a></p>
<ul>
<li>To simplify the API for CSI-backed disk resources, the following operations and corresponding ACLs have been introduced to replace the experimental <code>CREATE_VOLUME</code>, <code>CREATE_BLOCK</code>, <code>DESTROY_VOLUME</code> and <code>DESTROY_BLOCK</code> operations:
<ul>
<li><code>CREATE_DISK</code> to create a <code>MOUNT</code> or <code>BLOCK</code> disk resource from a <code>RAW</code> disk resource. The <code>CreateMountDisk</code> and <code>CreateBlockDisk</code> ACLs control which principals are allowed to create <code>MOUNT</code> or <code>BLOCK</code> disks for which roles.</li>
<li><code>DESTROY_DISK</code> to reclaim a <code>MOUNT</code> or <code>BLOCK</code> disk resource back to a <code>RAW</code> disk resource. The <code>DestroyMountDisk</code> and <code>DestroyBlockDisk</code> ACLs control which principals are allowed to reclaim <code>MOUNT</code> or <code>BLOCK</code> disks for which roles.</li>
</ul>
</li>
</ul>
<p><a name="1-7-x-resource-provider-acls"></a></p>
<ul>
<li>A new <code>ViewResourceProvider</code> ACL has been introduced to control which principals are allowed to call the <code>GET_RESOURCE_PROVIDERS</code> agent API.</li>
</ul>
<p><a name="1-7-x-enforce-container-ports"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#enforce_container_ports"><code>--enforce_container_ports</code></a> flag has been added to toggle whether the <a href="isolators/network-ports.html"><code>network/ports</code></a> isolator should enforce TCP ports usage limits.</li>
</ul>
<p><a name="1-7-x-gc-non-executor-container-sandboxes"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#gc_non_executor_container_sandboxes"><code>--gc_non_executor_container_sandboxes</code></a>
agent flag has been added to garbage collect the sandboxes of nested
containers, which includes the tasks groups launched by the default executor.
We recommend enabling the flag if you have frameworks that launch multiple
task groups on the same default executor instance.</li>
</ul>
<p><a name="1-7-x-network-cni-root-dir-persist"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#network_cni_root_dir_persist"><code>--network_cni_root_dir_persist</code></a> flag has been added to toggle whether the <a href="cni.html"><code>network/cni</code></a> isolator should persist the network information across reboots.</li>
</ul>
<p><a name="1-7-x-container-logger"></a></p>
<ul>
<li><code>ContainerLogger</code> module interface has been changed. The <code>prepare()</code> method now takes <code>ContainerID</code> and <code>ContainerConfig</code> instead.</li>
</ul>
<p><a name="1-7-x-isolator-recover"></a></p>
<ul>
<li><code>Isolator::recover()</code> has been updated to take an <code>std::vector</code> instead of <code>std::list</code> of container states.</li>
</ul>
<p><a name="1-7-x-json-serialization"></a></p>
<ul>
<li>As a result of adapting rapidjson for performance improvement, all JSON endpoints serialize differently while still conforming to the ECMA-404 spec for JSON. This means that if a client has a JSON de-serializer that conforms to ECMA-404 they will see no change. Otherwise, they may break. As an example, Mesos would previously serialize '/' as '/', but the spec does not require the escaping and rapidjson does not escape '/'.</li>
</ul>
<h2 id="upgrading-from-15x-to-16x"><a class="header" href="#upgrading-from-15x-to-16x">Upgrading from 1.5.x to 1.6.x</a></h2>
<p><a name="1-6-x-grpc-requirement"></a></p>
<ul>
<li>gRPC version 1.10+ is required to build Mesos when enabling gRPC-related features. Please upgrade your gRPC library if you are using an unbundled one.</li>
</ul>
<p><a name="1-6-x-csi-support"></a></p>
<ul>
<li>CSI v0.2 is now supported as experimental. Due to the incompatibility between CSI v0.1 and v0.2, the experimental support for CSI v0.1 is removed, and the operator must remove all storage local resource providers within an agent before upgrading the agent. NOTE: This is a <strong>breaking change</strong> for storage local resource providers.</li>
</ul>
<p><a name="1-6-x-fetcher-stall-timeout"></a></p>
<ul>
<li>A new agent flag <code>--fetcher_stall_timeout</code> has been added. This flag specifies the amount of time for the container image and artifact fetchers to wait before aborting a stalled download (i.e., the speed keeps below one byte per second). NOTE: This flag only applies when downloading data from the net and does not apply to HDFS.</li>
</ul>
<p><a name="1-6-x-disk-profile-adaptor"></a></p>
<ul>
<li>The disk profile adaptor module has been changed to support CSI v0.2, and its header file has been renamed to be consistent with other modules. See <code>disk_profile_adaptor.hpp</code> for interface changes.</li>
</ul>
<p><a name="1-6-x-xfs-kill-containers"></a></p>
<ul>
<li>A new agent flag <code>--xfs_kill_containers</code> has been added. By setting this flag, the <a href="isolators/disk-xfs.html"><code>disk/xfs</code></a> isolator
will now kill containers that exceed the disk limit.</li>
</ul>
<h2 id="upgrading-from-14x-to-15x"><a class="header" href="#upgrading-from-14x-to-15x">Upgrading from 1.4.x to 1.5.x</a></h2>
<p><a name="1-5-x-task-starting"></a></p>
<ul>
<li>The built-in executors will now send a <code>TASK_STARTING</code> status update for
every task they've successfully received and are about to start.
The possibility of any executor sending this update has been documented since
the beginning of Mesos, but prior to this version the built-in executors did
not actually send it. This means that all schedulers using one of the built-in
executors must be upgraded to expect <code>TASK_STARTING</code> updates before upgrading
Mesos itself.</li>
</ul>
<p><a name="1-5-x-task-resource-limitation"></a></p>
<ul>
<li>A new field, <code>limitation</code>, was added to the <code>TaskStatus</code> message. This
field is a <code>TaskResourceLimitation</code> message that describes the resources
that caused a task to fail with a resource limitation reason.</li>
</ul>
<p><a name="1-5-x-network-ports-isolator"></a></p>
<ul>
<li>A new <a href="isolators/network-ports.html"><code>network/ports</code></a> isolator has been added. The isolator supports the following new agent flags:
<ul>
<li><code>--container_ports_watch_interval</code> specifies the interval at which the isolator reconciles port assignments.</li>
<li><code>--check_agent_port_range_only</code> excludes ports outside the agent's range from port reconciliation.</li>
</ul>
</li>
</ul>
<p><a name="1-5-x-executor-secret-key"></a></p>
<ul>
<li>Agent flag <code>--executor_secret_key</code> has been deprecated. Operators should use <code>--jwt_secret_key</code> instead.</li>
</ul>
<p><a name="1-5-x-relative-disk-source-root-path"></a></p>
<ul>
<li>The fields <code>Resource.disk.source.path.root</code> and <code>Resource.disk.source.mount.root</code> can now be set to relative paths to an agent's work directory. The containerizers will interpret the paths based on the <code>--work_dir</code> flag on an agent.</li>
</ul>
<p><a name="1-5-x-get-containers"></a></p>
<ul>
<li>The agent operator API call <code>GET_CONTAINERS</code> has been updated to support listing nested or standalone containers. One can specify the following fields in the request:
<ul>
<li><code>show_nested</code>: Whether to show nested containers.</li>
<li><code>show_standalone</code>: Whether to show standalone containers.</li>
</ul>
</li>
</ul>
<p><a name="1-5-x-reconfiguration-policy"></a></p>
<ul>
<li>A new agent flag <code>--reconfiguration_policy</code> has been added. By setting the value of this flag to <code>additive</code>,
operators can allow the agent to be restarted with increased resources without requiring the agent ID to be
changed. Note that if this feature is used, the master version is required to be &gt;= 1.5 as well.</li>
</ul>
<p><a name="1-5-x-protobuf-requirement"></a></p>
<ul>
<li>Protobuf version 3+ is required to build Mesos. Please upgrade your Protobuf library if you are using an unbundled one.</li>
</ul>
<p><a name="1-5-x-log-reader-catchup"></a></p>
<ul>
<li>A new <code>catchup()</code> method has been added to the replicated log reader API. The method allows to catch-up positions missing in the local non-leading replica to allow safe eventually consistent reads from it. Note about backwards compatibility: In order for the feature to work correctly in presence of log truncations all log replicas need to be updated.</li>
</ul>
<h2 id="upgrading-from-13x-to-14x"><a class="header" href="#upgrading-from-13x-to-14x">Upgrading from 1.3.x to 1.4.x</a></h2>
<p><a name="1-4-x-ambient-capabilities"></a></p>
<ul>
<li>If the <code>mesos-agent</code> kernel supports ambient capabilities (Linux 4.3 or later), the capabilities specified in the <code>LinuxInfo.effective_capabilities</code> message will be made ambient in the container task.</li>
</ul>
<p><a name="1-4-x-bounding-capabilities"></a></p>
<ul>
<li>Explicitly setting the bounding capabilities of a task independently of the effective capabilities is now supported. Frameworks can specify the task bounding capabilities by using the <code>LinuxInfo.bounding_capabilities</code> message. Operators can specify the default bounding capabilities using the agent <code>--bounding_capabilities</code> flag. This flag also specifies the maximum bounding set that a framework is allowed to specify.</li>
</ul>
<p><a name="1-4-x-agent-recovery"></a></p>
<ul>
<li>Agent is now allowed to recover its agent ID post a host reboot. This prevents the unnecessary discarding of agent ID by prior Mesos versions. Notes about backwards compatibility:
<ul>
<li>In case the agent's recovery runs into agent info mismatch which may happen due to resource change associated with reboot, it'll fall back to recovering as a new agent (existing behavior).</li>
<li>In other cases such as checkpointed resources (e.g. persistent volumes) being incompatible with the agent's resources the recovery will still fail (existing behavior).</li>
</ul>
</li>
</ul>
<p><a name="1-4-x-linuxinfo-capabilities"></a></p>
<ul>
<li>The <code>LinuxInfo.capabilities</code> field has been deprecated in favor of <code>LinuxInfo.effective_capabilities</code>.</li>
</ul>
<p><a name="1-4-x-agent-capabilities-flags"></a></p>
<ul>
<li>Changes to capability-related agent flags:
<ul>
<li>The agent <code>--effective_capabilities</code> flag has been added to specify the default effective capability set for tasks.</li>
<li>The agent <code>--bounding_capabilities</code> flag has been added to specify the default bounding capability set for tasks.</li>
<li>The agent <code>--allowed-capabilities</code> flag has been deprecated in favor of <code>--effective_capabilities</code>.</li>
</ul>
</li>
</ul>
<p><a name="1-4-x-allocator-update-slave"></a></p>
<ul>
<li>The semantics of the optional resource argument passed in <code>Allocator::updateSlave</code> was change. While previously the passed value denoted a new amount of oversubscribed (revocable) resources on the agent, it now denotes the new amount of total resources on the agent. This requires custom allocator implementations to update their interpretation of the passed value.</li>
</ul>
<p><a name="1-4-x-xfs-no-enforce"></a></p>
<ul>
<li>The XFS Disk Isolator now supports the <code>--no-enforce_container_disk_quota</code> option to efficiently measure disk resource usage without enforcing any usage limits.</li>
</ul>
<p><a name="1-4-x-mesos-library"></a></p>
<ul>
<li>The <code>Resources</code> class in the internal Mesos C++ library changed its behavior to only support post-<code>RESERVATION_REFINEMENT</code> format. If a framework is using this internal utility, it is likely to break if the <code>RESERVATION_REFINEMENT</code> capability is not enabled.</li>
</ul>
<p><a name="1-4-x-update-minimal-docker-version"></a></p>
<ul>
<li>To specify the <code>--type=container</code> option for the <code>docker inspect &lt;container_name&gt;</code> command, the minimal supported Docker version has been updated from 1.0.0 to 1.8.0 since Docker supported <code>--type=container</code> for the <code>docker inspect</code> command starting from 1.8.0.</li>
</ul>
<h2 id="upgrading-from-12x-to-13x"><a class="header" href="#upgrading-from-12x-to-13x">Upgrading from 1.2.x to 1.3.x</a></h2>
<p><a name="1-3-x-disallow-old-agents"></a></p>
<ul>
<li>The master will no longer allow 0.x agents to register. Interoperability between 1.1+ masters and 0.x agents has never been supported; however, it was not explicitly disallowed, either. Starting with this release of Mesos, registration attempts by 0.x agents will be ignored.</li>
</ul>
<p><a name="1-3-x-setquota-removequota-acl"></a></p>
<ul>
<li>Support for deprecated ACLs <code>set_quotas</code> and <code>remove_quotas</code> has been removed from the local authorizer. Before upgrading the Mesos binaries, consolidate the ACLs used under <code>set_quotas</code> and <code>remove_quotes</code> under their replacement ACL <code>update_quotas</code>. After consolidation of the ACLs, the binaries could be safely replaced.</li>
</ul>
<p><a name="1-3-x-shutdown-framework-acl"></a></p>
<ul>
<li>Support for deprecated ACL <code>shutdown_frameworks</code> has been removed from the local authorizer. Before upgrading the Mesos binaries, replace all instances of the ACL <code>shutdown_frameworks</code> with the newer ACL <code>teardown_frameworks</code>. After updating the ACLs, the binaries can be safely replaced.</li>
</ul>
<p><a name="1-3-x-multi-role-support"></a>
<a name="1-3-x-framework-info-role"></a></p>
<ul>
<li>Support for multi-role frameworks deprecates the <code>FrameworkInfo.role</code> field in favor of <code>FrameworkInfo.roles</code> and the <code>MULTI_ROLE</code> capability. Frameworks using the new field can continue to use a single role.</li>
</ul>
<p><a name="1-3-x-endpoints-roles"></a></p>
<ul>
<li>Support for multi-role frameworks means that the framework <code>role</code> field in the master and agent endpoints is deprecated in favor of <code>roles</code>. Any tooling parsing endpoint information and relying on the role field needs to be updated before multi-role frameworks can be safely run in the cluster.</li>
</ul>
<p><a name="1-3-x-allocator-interface-change"></a></p>
<ul>
<li>Implementors of allocator modules have to provide new implementation functionality to satisfy the <code>MULTI_ROLE</code> framework capability. Also, the interface has changed.</li>
</ul>
<p><a name="1-3-x-executor-authentication"></a></p>
<ul>
<li>New Agent flags <code>authenticate_http_executors</code> and <code>executor_secret_key</code>: Used to enable required HTTP executor authentication and set the key file used for generation and authentication of HTTP executor tokens. Note that enabling these flags after upgrade is disruptive to HTTP executors that were launched before the upgrade. For more information on the recommended upgrade procedure when enabling these flags, see the <a href="authentication.html">authentication documentation</a>.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents/schedulers can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-11x-to-12x"><a class="header" href="#upgrading-from-11x-to-12x">Upgrading from 1.1.x to 1.2.x</a></h2>
<p><a name="1-2-1-disallow-old-agents"></a></p>
<ul>
<li>In Mesos 1.2.1, the master will no longer allow 0.x agents to register. Interoperability between 1.1+ masters and 0.x agents has never been supported; however, it was not explicitly disallowed, either. Starting with Mesos 1.2.1, registration attempts by 0.x agents will be ignored. <strong>NOTE:</strong> This applies only when upgrading to Mesos 1.2.1. Mesos 1.2.0 does not implement this behavior.</li>
</ul>
<p><a name="1-2-x-heartbeat-flag"></a></p>
<ul>
<li>New Agent flag http_heartbeat_interval: This flag sets a heartbeat interval for messages to be sent over persistent connections made against the agent HTTP API. Currently, this only applies to the LAUNCH_NESTED_CONTAINER_SESSION and ATTACH_CONTAINER_OUTPUT calls. (default: 30secs)</li>
</ul>
<p><a name="1-2-x-backend-flag"></a></p>
<ul>
<li>New Agent flag image_provisioner_backend: Strategy for provisioning container rootfs from images, e.g., aufs, bind, copy, overlay.</li>
</ul>
<p><a name="1-2-x-unreachable-flag"></a></p>
<ul>
<li>New Master flag max_unreachable_tasks_per_framework: Maximum number of unreachable tasks per framework to store in memory. (default: 1000)</li>
</ul>
<p><a name="1-2-x-revive-suppress"></a></p>
<ul>
<li>New Revive and Suppress v1 scheduler Calls: Revive or Suppress offers for a specified role. If role is unset, the call will revive/suppress offers for all of the roles the framework is subscribed to. (Especially for multi-role frameworks.)</li>
</ul>
<p><a name="1-2-x-container-logger-interface"></a></p>
<ul>
<li>Mesos 1.2 modifies the <code>ContainerLogger</code>'s <code>prepare()</code> method.  The method now takes an additional argument for the <code>user</code> the logger should run a subprocess as.  Please see <a href="https://issues.apache.org/jira/browse/MESOS-5856">MESOS-5856</a> for more information.</li>
</ul>
<p><a name="1-2-x-allocator-module-changes"></a></p>
<ul>
<li>Allocator module changes to support inactive frameworks, multi-role frameworks, and suppress/revive. See <code>allocator.hpp</code> for interface changes.</li>
</ul>
<p><a name="1-2-x-new-authz-actions"></a></p>
<ul>
<li>New Authorizer module actions: LAUNCH_NESTED_CONTAINER, KILL_NESTED_CONTAINER, WAIT_NESTED_CONTAINER, LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT, ATTACH_CONTAINER_OUTPUT, VIEW_CONTAINER, and SET_LOG_LEVEL. See <code>authorizer.proto</code> for module interface changes, and <code>acls.proto</code> for corresponding LocalAuthorizer ACL changes.</li>
</ul>
<p><a name="1-2-x-renamed-authz-actions"></a></p>
<ul>
<li>Renamed Authorizer module actions (and deprecated old aliases): REGISTER_FRAMEWORK, TEARDOWN_FRAMEWORK, RESERVE_RESOURCES, UNRESERVE_RESOURCES, CREATE_VOLUME, DESTROY_VOLUME, UPDATE_WEIGHT, GET_QUOTA. See <code>authorizer.proto</code> for interface changes.</li>
</ul>
<p><a name="1-2-x-removed-hooks"></a></p>
<ul>
<li>Removed slavePreLaunchDockerEnvironmentDecorator and slavePreLaunchDockerHook in favor of slavePreLaunchDockerTaskExecutorDecorator.</li>
</ul>
<p><a name="1-2-x-debug-endpoints"></a></p>
<ul>
<li>New Agent v1 operator API calls: LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT, ATTACH_CONTAINER_OUTPUT for debugging into running containers (Mesos containerizer only).</li>
</ul>
<p><a name="1-2-x-recovered-frameworks"></a></p>
<ul>
<li>Deprecated <code>recovered_frameworks</code> in v1 GetFrameworks call. Now it will be empty.</li>
</ul>
<p><a name="1-2-x-orphan-executors"></a></p>
<ul>
<li>Deprecated <code>orphan_executors</code> in v1 GetExecutors call. Now it will be empty.</li>
</ul>
<p><a name="1-2-x-orphan-tasks"></a></p>
<ul>
<li>Deprecated <code>orphan_tasks</code> in v1 GetTasks call. Now it will be empty.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents/schedulers can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-10x-to-11x"><a class="header" href="#upgrading-from-10x-to-11x">Upgrading from 1.0.x to 1.1.x</a></h2>
<p><a name="1-1-x-container-logger-interface"></a></p>
<ul>
<li>Mesos 1.1 removes the <code>ContainerLogger</code>'s <code>recover()</code> method.  The <code>ContainerLogger</code> had an incomplete interface for a stateful implementation.  This removes the incomplete parts to avoid adding tech debt in the containerizer.  Please see <a href="https://issues.apache.org/jira/browse/MESOS-6371">MESOS-6371</a> for more information.</li>
</ul>
<p><a name="1-1-x-allocator-updateallocation"></a></p>
<ul>
<li>Mesos 1.1 adds an <code>offeredResources</code> argument to the <code>Allocator::updateAllocation()</code> method. It is used to indicate the resources that the operations passed to <code>updateAllocation()</code> are applied to. <a href="https://issues.apache.org/jira/browse/MESOS-4431">MESOS-4431</a> (particularly <a href="https://reviews.apache.org/r/45961/">/r/45961/</a>) has more details on the motivation.</li>
</ul>
<h2 id="upgrading-from-028x-to-10x"><a class="header" href="#upgrading-from-028x-to-10x">Upgrading from 0.28.x to 1.0.x</a></h2>
<p><a name="1-0-x-deprecated-ssl-env-variables"></a></p>
<ul>
<li>Prior to Mesos 1.0, environment variables prefixed by <code>SSL_</code> are used to control libprocess SSL support. However, it was found that those environment variables may collide with some libraries or programs (e.g., openssl, curl). From Mesos 1.0, <code>SSL_*</code> environment variables are deprecated in favor of the corresponding <code>LIBPROCESS_SSL_*</code> variables.</li>
</ul>
<p><a name="1-0-x-persistent-volume-ownership"></a></p>
<ul>
<li>Prior to Mesos 1.0, Mesos agent recursively changes the ownership of the persistent volumes every time they are mounted to a container. From Mesos 1.0, this behavior has been changed. Mesos agent will do a <em>non-recursive</em> change of ownership of the persistent volumes.</li>
</ul>
<p><a name="1-0-x-deprecated-fields-in-container-config"></a></p>
<ul>
<li>Mesos 1.0 removed the camel cased protobuf fields in <code>ContainerConfig</code> (see <code>include/mesos/slave/isolator.proto</code>):
<ul>
<li><code>required ExecutorInfo executorInfo = 1;</code></li>
<li><code>optional TaskInfo taskInfo = 2;</code></li>
</ul>
</li>
</ul>
<p><a name="1-0-x-executor-environment-variables"></a></p>
<ul>
<li>By default, executors will no longer inherit environment variables from the agent. The operator can still use the <code>--executor_environment_variables</code> flag on the agent to explicitly specify what environment variables the executors will get. Mesos generated environment variables (i.e., <code>$MESOS_</code>, <code>$LIBPROCESS_</code>) will not be affected. If <code>$PATH</code> is not specified for an executor, a default value <code>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code> will be used.</li>
</ul>
<p><a name="1-0-x-allocator-metrics"></a></p>
<ul>
<li>The allocator metric named <code>allocator/event_queue_dispatches</code> is now deprecated. The new name is <code>allocator/mesos/event_queue_dispatches</code> to better support metrics for alternative allocator implementations.</li>
</ul>
<p><a name="1-0-x-docker-timeout-flag"></a></p>
<ul>
<li>The <code>--docker_stop_timeout</code> agent flag is deprecated.</li>
</ul>
<p><a name="1-0-x-executorinfo"></a></p>
<ul>
<li>The ExecutorInfo.source field is deprecated in favor of ExecutorInfo.labels.</li>
</ul>
<p><a name="1-0-x-slave"></a></p>
<ul>
<li>Mesos 1.0 deprecates the 'slave' keyword in favor of 'agent' in a number of places
<ul>
<li>Deprecated flags with keyword 'slave' in favor of 'agent'.</li>
<li>Deprecated sandbox links with 'slave' keyword in the WebUI.</li>
<li>Deprecated <code>slave</code> subcommand for mesos-cli.</li>
</ul>
</li>
</ul>
<p><a name="1-0-x-workdir"></a></p>
<ul>
<li>Mesos 1.0 removes the default value for the agent's <code>work_dir</code> command-line flag. This flag is now required; the agent will exit immediately if it is not provided.</li>
</ul>
<p><a name="1-0-x-registry-strict"></a></p>
<ul>
<li>Mesos 1.0 disables support for the master's <code>registry_strict</code> command-line flag. If this flag is specified, the master will exit immediately. Note that this flag was previously marked as experimental and not recommended for production use.</li>
</ul>
<p><a name="1-0-x-credentials-file"></a></p>
<ul>
<li>Mesos 1.0 deprecates the use of plain text credential files in favor of JSON-formatted credential files.</li>
</ul>
<p><a name="1-0-x-persistent-volume"></a></p>
<ul>
<li>When a persistent volume is destroyed, Mesos will now remove any data that was stored on the volume from the filesystem of the appropriate agent. In prior versions of Mesos, destroying a volume would not delete data (this was a known missing feature that has now been implemented).</li>
</ul>
<p><a name="1-0-x-status-code"></a></p>
<ul>
<li>Mesos 1.0 changes the HTTP status code of the following endpoints from <code>200 OK</code> to <code>202 Accepted</code>:
<ul>
<li><code>/reserve</code></li>
<li><code>/unreserve</code></li>
<li><code>/create-volumes</code></li>
<li><code>/destroy-volumes</code></li>
</ul>
</li>
</ul>
<p><a name="1-0-x-v1-commandinfo"></a></p>
<ul>
<li>Added <code>output_file</code> field to CommandInfo.URI in Scheduler API and v1 Scheduler HTTP API.</li>
</ul>
<p><a name="1-0-x-scheduler-proto"></a></p>
<ul>
<li>Changed Call and Event Type enums in scheduler.proto from required to optional for the purpose of backwards compatibility.</li>
</ul>
<p><a name="1-0-x-executor-proto"></a></p>
<ul>
<li>Changed Call and Event Type enums in executor.proto from required to optional for the purpose of backwards compatibility.</li>
</ul>
<p><a name="1-0-x-nonterminal"></a></p>
<ul>
<li>Added non-terminal task metadata to the container resource usage information.</li>
</ul>
<p><a name="1-0-x-observe-endpoint"></a></p>
<ul>
<li>Deleted the /observe HTTP endpoint.</li>
</ul>
<p><a name="1-0-x-quota-acls"></a></p>
<ul>
<li>The <code>SetQuota</code> and <code>RemoveQuota</code> ACLs have been deprecated. To replace these, a new ACL <code>UpdateQuota</code> have been introduced. In addition, a new ACL <code>GetQuota</code> have been added; these control which principals are allowed to query quota information for which roles. These changes affect the <code>--acls</code> flag for the local authorizer in the following ways:
<ul>
<li>The <code>update_quotas</code> ACL cannot be used in combination with either the <code>set_quotas</code> or <code>remove_quotas</code> ACL. The local authorizer will produce an error in such a case;</li>
<li>When upgrading a Mesos cluster that uses the <code>set_quotas</code> or <code>remove_quotas</code> ACLs, the operator should first upgrade the Mesos binaries. At this point, the deprecated ACLs will still be enforced. After the upgrade has been verified, the operator should replace deprecated values for <code>set_quotas</code> and <code>remove_quotas</code> with equivalent values for <code>update_quotas</code>;</li>
<li>If desired, the operator can use the <code>get_quotas</code> ACL after the upgrade to control which principals are allowed to query quota information.</li>
</ul>
</li>
</ul>
<p><a name="1-0-x-authorizer"></a></p>
<ul>
<li>Mesos 1.0 contains a number of authorizer changes that particularly effect custom authorizer modules:
<ul>
<li>The authorizer interface has been refactored in order to decouple the ACL definition language from the interface. It additionally includes the option of retrieving <code>ObjectApprover</code>. An <code>ObjectApprover</code> can be used to synchronously check authorizations for a given object and is hence useful when authorizing a large number of objects and/or large objects (which need to be copied using request-based authorization). NOTE: This is a <strong>breaking change</strong> for authorizer modules.</li>
<li>Authorization-based HTTP endpoint filtering enables operators to restrict which parts of the cluster state a user is authorized to see. Consider for example the <code>/state</code> master endpoint: an operator can now authorize users to only see a subset of the running frameworks, tasks, or executors.</li>
<li>The <code>subject</code> and <code>object</code> fields in the authorization::Request protobuf message have been changed to be optional. If these fields are not set, the request should only be allowed for ACLs with <code>ANY</code> semantics. NOTE: This is a semantic change for authorizer modules.</li>
</ul>
</li>
</ul>
<p><a name="1-0-x-allocator"></a></p>
<ul>
<li>Namespace and header file of <code>Allocator</code> has been moved to be consistent with other packages.</li>
</ul>
<p><a name="1-0-x-fetcher-user"></a></p>
<ul>
<li>When a task is run as a particular user, the fetcher now fetches files as that user also. Note, this means that filesystem permissions for that user will be enforced when fetching local files.</li>
</ul>
<p><a name="1-0-x-http-authentication-flags"></a></p>
<ul>
<li>The <code>--authenticate_http</code> flag has been deprecated in favor of <code>--authenticate_http_readwrite</code>. Setting <code>--authenticate_http_readwrite</code> will now enable authentication for all endpoints which previously had authentication support. These happen to be the endpoints which allow modification of the cluster state, or &quot;read-write&quot; endpoints. Note that <code>/logging/toggle</code>, <code>/profiler/start</code>, <code>/profiler/stop</code>, <code>/maintenance/schedule</code>, <code>/machine/up</code>, and <code>/machine/down</code> previously did not have authentication support, but in 1.0 if either <code>--authenticate_http</code> or <code>--authenticate_http_readwrite</code> is set, those endpoints will now require authentication. A new flag has also been introduced, <code>--authenticate_http_readonly</code>, which enables authentication for endpoints which support authentication and do not allow modification of the state of the cluster, like <code>/state</code> or <code>/flags</code>.</li>
</ul>
<p><a name="1-0-x-endpoint-authorization"></a></p>
<ul>
<li>
<p>Mesos 1.0 introduces authorization support for several HTTP endpoints. Note that some of these endpoints are used by the web UI, and thus using the web UI in a cluster with authorization enabled will require that ACLs be set appropriately. Please refer to the <a href="authorization.html">authorization documentation</a> for details.</p>
</li>
<li>
<p>The endpoints with coarse-grained authorization enabled are:</p>
<ul>
<li><code>/files/debug</code></li>
<li><code>/logging/toggle</code></li>
<li><code>/metrics/snapshot</code></li>
<li><code>/slave(id)/containers</code></li>
<li><code>/slave(id)/monitor/statistics</code></li>
</ul>
</li>
<li>
<p>If the defined ACLs used <code>permissive: false</code>, the listed HTTP endpoints will stop working unless ACLs for the <code>get_endpoints</code> actions are defined.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-027x-to-028x"><a class="header" href="#upgrading-from-027x-to-028x">Upgrading from 0.27.x to 0.28.x</a></h2>
<p><a name="0-28-x-resource-precision"></a></p>
<ul>
<li>Mesos 0.28 only supports three decimal digits of precision for scalar resource values. For example, frameworks can reserve &quot;0.001&quot; CPUs but more fine-grained reservations (e.g., &quot;0.0001&quot; CPUs) are no longer supported (although they did not work reliably in prior versions of Mesos anyway). Internally, resource math is now done using a fixed-point format that supports three decimal digits of precision, and then converted to/from floating point for input and output, respectively. Frameworks that do their own resource math and manipulate fractional resources may observe differences in roundoff error and numerical precision.</li>
</ul>
<p><a name="0-28-x-autherization-acls"></a></p>
<ul>
<li>Mesos 0.28 changes the definitions of two ACLs used for authorization. The objects of the <code>ReserveResources</code> and <code>CreateVolume</code> ACLs have been changed to <code>roles</code>. In both cases, principals can now be authorized to perform these operations for particular roles. This means that by default, a framework or operator can reserve resources/create volumes for any role. To restrict this behavior, <a href="authorization.html">ACLs can be added</a> to the master which authorize principals to reserve resources/create volumes for specified roles only. Previously, frameworks could only reserve resources for their own role; this behavior can be preserved by configuring the <code>ReserveResources</code> ACLs such that the framework's principal is only authorized to reserve for the framework's role. <strong>NOTE</strong> This renders existing <code>ReserveResources</code> and <code>CreateVolume</code> ACL definitions obsolete; if you are authorizing these operations, your ACL definitions should be updated.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-026x-to-027x"><a class="header" href="#upgrading-from-026x-to-027x">Upgrading from 0.26.x to 0.27.x</a></h2>
<p><a name="0-27-x-implicit-roles"></a></p>
<ul>
<li>Mesos 0.27 introduces the concept of <em>implicit roles</em>. In previous releases, configuring roles required specifying a static whitelist of valid role names on master startup (via the <code>--roles</code> flag). In Mesos 0.27, if <code>--roles</code> is omitted, <em>any</em> role name can be used; controlling which principals are allowed to register as which roles should be done using <a href="authorization.html">ACLs</a>. The role whitelist functionality is still supported but is deprecated.</li>
</ul>
<p><a name="0-27-x-allocator-api"></a></p>
<ul>
<li>The Allocator API has changed due to the introduction of implicit roles. Custom allocator implementations will need to be updated. See <a href="https://issues.apache.org/jira/browse/MESOS-4000">MESOS-4000</a> for more information.</li>
</ul>
<p><a name="0-27-x-executor-lost-callback"></a></p>
<ul>
<li>The <code>executorLost</code> callback in the Scheduler interface will now be called whenever the agent detects termination of a custom executor. This callback was never called in previous versions, so please make sure any framework schedulers can now safely handle this callback. Note that this callback may not be reliably delivered.</li>
</ul>
<p><a name="0-27-x-isolator-api"></a></p>
<ul>
<li>The isolator <code>prepare</code> interface has been changed slightly. Instead of keeping adding parameters to the <code>prepare</code> interface, we decide to use a protobuf (<code>ContainerConfig</code>). Also, we renamed <code>ContainerPrepareInfo</code> to <code>ContainerLaunchInfo</code> to better capture the purpose of this struct. See <a href="https://issues.apache.org/jira/browse/MESOS-4240">MESOS-4240</a> and <a href="https://issues.apache.org/jira/browse/MESOS-4282">MESOS-4282</a> for more information. If you are an isolator module writer, you will have to adjust your isolator module according to the new interface and re-compile with 0.27.</li>
</ul>
<p><a name="0-27-x-acl-shutdown-flag"></a></p>
<ul>
<li>
<p>ACLs.shutdown_frameworks has been deprecated in favor of the new ACLs.teardown_frameworks. This affects the <code>--acls</code> master flag for the local authorizer.</p>
</li>
<li>
<p>Reserved resources are now accounted for in the DRF role sorter. Previously unaccounted reservations will influence the weighted DRF sorter. If role weights were explicitly set, they may need to be adjusted in order to account for the reserved resources in the cluster.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-025x-to-026x"><a class="header" href="#upgrading-from-025x-to-026x">Upgrading from 0.25.x to 0.26.x</a></h2>
<p><a name="0-26-x-taskstatus-reason"></a></p>
<ul>
<li>
<p>The names of some TaskStatus::Reason enums have been changed. But the tag numbers remain unchanged, so it is backwards compatible. Frameworks using the new version might need to do some compile time adjustments:</p>
<ul>
<li>REASON_MEM_LIMIT -&gt; REASON_CONTAINER_LIMITATION_MEMORY</li>
<li>REASON_EXECUTOR_PREEMPTED -&gt; REASON_CONTAINER_PREEMPTED</li>
</ul>
</li>
</ul>
<p><a name="0-26-x-credential-protobuf"></a></p>
<ul>
<li>The <code>Credential</code> protobuf has been changed. <code>Credential</code> field <code>secret</code> is now a string, it used to be bytes. This will affect framework developers and language bindings ought to update their generated protobuf with the new version. This fixes JSON based credentials file support.</li>
</ul>
<p><a name="0-26-x-state-endpoint"></a></p>
<ul>
<li>The <code>/state</code> endpoints on master and agent will no longer include <code>data</code> fields as part of the JSON models for <code>ExecutorInfo</code> and <code>TaskInfo</code> out of consideration for memory scalability (see <a href="https://issues.apache.org/jira/browse/MESOS-3794">MESOS-3794</a> and <a href="http://www.mail-archive.com/dev@mesos.apache.org/msg33536.html">this email thread</a>).
<ul>
<li>On master, the affected <code>data</code> field was originally found via <code>frameworks[*].executors[*].data</code>.</li>
<li>On agents, the affected <code>data</code> field was originally found via <code>executors[*].tasks[*].data</code>.</li>
</ul>
</li>
</ul>
<p><a name="0-26-x-network-info-protobuf"></a></p>
<ul>
<li>The <code>NetworkInfo</code> protobuf has been changed. The fields <code>protocol</code> and <code>ip_address</code> are now deprecated. The new field <code>ip_addresses</code> subsumes the information provided by them.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-024x-to-025x"><a class="header" href="#upgrading-from-024x-to-025x">Upgrading from 0.24.x to 0.25.x</a></h2>
<p><a name="0-25-x-json-endpoints"></a></p>
<ul>
<li>
<p>The following endpoints will be deprecated in favor of new endpoints. Both versions will be available in 0.25 but the deprecated endpoints will be removed in a subsequent release.</p>
<p>For master endpoints:</p>
<ul>
<li>/state.json becomes /state</li>
<li>/tasks.json becomes /tasks</li>
</ul>
<p>For agent endpoints:</p>
<ul>
<li>/state.json becomes /state</li>
<li>/monitor/statistics.json becomes /monitor/statistics</li>
</ul>
<p>For both master and agent:</p>
<ul>
<li>/files/browse.json becomes /files/browse</li>
<li>/files/debug.json becomes /files/debug</li>
<li>/files/download.json becomes /files/download</li>
<li>/files/read.json becomes /files/read</li>
</ul>
</li>
</ul>
<p><a name="0-25-x-scheduler-bindings"></a></p>
<ul>
<li>The C++/Java/Python scheduler bindings have been updated. In particular, the driver can make a suppressOffers() call to stop receiving offers (until reviveOffers() is called).</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-023x-to-024x"><a class="header" href="#upgrading-from-023x-to-024x">Upgrading from 0.23.x to 0.24.x</a></h2>
<ul>
<li>
<p>Support for live upgrading a driver based scheduler to HTTP based (experimental) scheduler has been added.</p>
</li>
<li>
<p>Master now publishes its information in ZooKeeper in JSON (instead of protobuf). Make sure schedulers are linked against &gt;= 0.23.0 libmesos before upgrading the master.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-022x-to-023x"><a class="header" href="#upgrading-from-022x-to-023x">Upgrading from 0.22.x to 0.23.x</a></h2>
<ul>
<li>
<p>The 'stats.json' endpoints for masters and agents have been removed. Please use the 'metrics/snapshot' endpoints instead.</p>
</li>
<li>
<p>The '/master/shutdown' endpoint is deprecated in favor of the new '/master/teardown' endpoint.</p>
</li>
<li>
<p>In order to enable decorator modules to remove metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks in Mesos 0.23.0. Please refer to the modules documentation for more details.</p>
</li>
<li>
<p>Agent ping timeouts are now configurable on the master via <code>--slave_ping_timeout</code> and <code>--max_slave_ping_timeouts</code>. Agents should be upgraded to 0.23.x before changing these flags.</p>
</li>
<li>
<p>A new scheduler driver API, <code>acceptOffers</code>, has been introduced. This is a more general version of the <code>launchTasks</code> API, which allows the scheduler to accept an offer and specify a list of operations (Offer.Operation) to perform using the resources in the offer. Currently, the supported operations include LAUNCH (launching tasks), RESERVE (making dynamic reservations), UNRESERVE (releasing dynamic reservations), CREATE (creating persistent volumes) and DESTROY (releasing persistent volumes). Similar to the <code>launchTasks</code> API, any unused resources will be considered declined, and the specified filters will be applied on all unused resources.</p>
</li>
<li>
<p>The Resource protobuf has been extended to include more metadata for supporting persistence (DiskInfo), dynamic reservations (ReservationInfo) and oversubscription (RevocableInfo). You must not combine two Resource objects if they have different metadata.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-021x-to-022x"><a class="header" href="#upgrading-from-021x-to-022x">Upgrading from 0.21.x to 0.22.x</a></h2>
<ul>
<li>
<p>Agent checkpoint flag has been removed as it will be enabled for all
agents. Frameworks must still enable checkpointing during registration to take advantage
of checkpointing their tasks.</p>
</li>
<li>
<p>The stats.json endpoints for masters and agents have been deprecated.
Please refer to the metrics/snapshot endpoint.</p>
</li>
<li>
<p>The C++/Java/Python scheduler bindings have been updated. In particular, the driver can be constructed with an additional argument that specifies whether to use implicit driver acknowledgements. In <code>statusUpdate</code>, the <code>TaskStatus</code> now includes a UUID to make explicit acknowledgements possible.</p>
</li>
<li>
<p>The Authentication API has changed slightly in this release to support additional authentication mechanisms. The change from 'string' to 'bytes' for AuthenticationStartMessage.data has no impact on C++ or the over-the-wire representation, so it only impacts pure language bindings for languages like Java and Python that use different types for UTF-8 strings vs. byte arrays.</p>
<p>message AuthenticationStartMessage {
required string mechanism = 1;
optional bytes data = 2;
}</p>
</li>
<li>
<p>All Mesos arguments can now be passed using file:// to read them out of a file (either an absolute or relative path). The --credentials, --whitelist, and any flags that expect JSON backed arguments (such as --modules) behave as before, although support for just passing an absolute path for any JSON flags rather than file:// has been deprecated and will produce a warning (and the absolute path behavior will be removed in a future release).</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers:</li>
</ol>
<ul>
<li>For Java schedulers, link the new native library against the new JAR. The JAR contains API above changes. A 0.21.0 JAR will work with a 0.22.0 libmesos. A 0.22.0 JAR will work with a 0.21.0 libmesos if explicit acks are not being used. 0.22.0 and 0.21.0 are inter-operable at the protocol level between the master and the scheduler.</li>
<li>For Python schedulers, upgrade to use a 0.22.0 egg. If constructing <code>MesosSchedulerDriverImpl</code> with <code>Credentials</code>, your code must be updated to pass the <code>implicitAcknowledgements</code> argument before <code>Credentials</code>. You may run a 0.21.0 Python scheduler against a 0.22.0 master, and vice versa.</li>
</ul>
<ol start="4">
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg.</li>
</ol>
<h2 id="upgrading-from-020x-to-021x"><a class="header" href="#upgrading-from-020x-to-021x">Upgrading from 0.20.x to 0.21.x</a></h2>
<ul>
<li>Disabling agent checkpointing has been deprecated; the agent --checkpoint flag has been deprecated and will be removed in a future release.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library (mesos jar upgrade not necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-019x-to-020x"><a class="header" href="#upgrading-from-019x-to-020x">Upgrading from 0.19.x to 0.20.x.</a></h2>
<ul>
<li>
<p>The Mesos API has been changed slightly in this release. The CommandInfo has been changed (see below), which makes launching a command more flexible. The 'value' field has been changed from <em>required</em> to <em>optional</em>. However, it will not cause any issue during the upgrade (since the existing schedulers always set this field).</p>
<pre><code>  message CommandInfo {
    ...
    // There are two ways to specify the command:
    // 1) If 'shell == true', the command will be launched via shell
    //    (i.e., /bin/sh -c 'value'). The 'value' specified will be
    //    treated as the shell command. The 'arguments' will be ignored.
    // 2) If 'shell == false', the command will be launched by passing
    //    arguments to an executable. The 'value' specified will be
    //    treated as the filename of the executable. The 'arguments'
    //    will be treated as the arguments to the executable. This is
    //    similar to how POSIX exec families launch processes (i.e.,
    //    execlp(value, arguments(0), arguments(1), ...)).
    optional bool shell = 6 [default = true];
    optional string value = 3;
    repeated string arguments = 7;
    ...
  }
</code></pre>
</li>
<li>
<p>The Python bindings are also changing in this release. There are now sub-modules which allow you to use either the interfaces and/or the native driver.</p>
<ul>
<li><code>import mesos.native</code> for the native drivers</li>
<li><code>import mesos.interface</code> for the stub implementations and protobufs</li>
</ul>
<p>To ensure a smooth upgrade, we recommend to upgrade your python framework and executor first. You will be able to either import using the new configuration or the old. Replace the existing imports with something like the following:</p>
<p>try:
from mesos.native import MesosExecutorDriver, MesosSchedulerDriver
from mesos.interface import Executor, Scheduler
from mesos.interface import mesos_pb2
except ImportError:
from mesos import Executor, MesosExecutorDriver, MesosSchedulerDriver, Scheduler
import mesos_pb2</p>
</li>
<li>
<p>If you're using a pure language binding, please ensure that it sends status update acknowledgements through the master before upgrading.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library (install the latest mesos jar and python egg if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library (install the latest mesos jar and python egg if necessary).</li>
</ol>
<h2 id="upgrading-from-018x-to-019x"><a class="header" href="#upgrading-from-018x-to-019x">Upgrading from 0.18.x to 0.19.x.</a></h2>
<ul>
<li>
<p>There are new required flags on the master (<code>--work_dir</code> and <code>--quorum</code>) to support the <em>Registrar</em> feature, which adds replicated state on the masters.</p>
</li>
<li>
<p>No required upgrade ordering across components.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library (mesos jar upgrade not necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0170-to-018x"><a class="header" href="#upgrading-from-0170-to-018x">Upgrading from 0.17.0 to 0.18.x.</a></h2>
<ul>
<li>This upgrade requires a system reboot for agents that use Linux cgroups for isolation.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries then perform one of the following two steps, depending on if cgroups isolation is used:</li>
</ol>
<ul>
<li>[no cgroups]
<ul>
<li>Restart the agents. The &quot;--isolation&quot; flag has changed and &quot;process&quot; has been deprecated in favor of &quot;posix/cpu,posix/mem&quot;.</li>
</ul>
</li>
<li>[cgroups]
<ul>
<li>Change from a single mountpoint for all controllers to separate mountpoints for each controller, e.g., /sys/fs/cgroup/memory/ and /sys/fs/cgroup/cpu/.</li>
<li>The suggested configuration is to mount a tmpfs filesystem to /sys/fs/cgroup and to let the agent mount the required controllers. However, the agent will also use previously mounted controllers if they are appropriately mounted under &quot;--cgroups_hierarchy&quot;.</li>
<li>It has been observed that unmounting and remounting of cgroups from the single to separate configuration is unreliable and a reboot into the new configuration is strongly advised. Restart the agents after reboot.</li>
<li>The &quot;--cgroups_hierarchy&quot; now defaults to &quot;/sys/fs/cgroup&quot;. The &quot;--cgroups_root&quot; flag default remains &quot;mesos&quot;.</li>
<li>The &quot;--isolation&quot; flag has changed and &quot;cgroups&quot; has been deprecated in favor of &quot;cgroups/cpu,cgroups/mem&quot;.</li>
<li>The &quot;--cgroup_subsystems&quot; flag is no longer required and will be ignored.</li>
</ul>
</li>
</ul>
<ol start="5">
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0160-to-0170"><a class="header" href="#upgrading-from-0160-to-0170">Upgrading from 0.16.0 to 0.17.0.</a></h2>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0150-to-0160"><a class="header" href="#upgrading-from-0150-to-0160">Upgrading from 0.15.0 to 0.16.0.</a></h2>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0140-to-0150"><a class="header" href="#upgrading-from-0140-to-0150">Upgrading from 0.14.0 to 0.15.0.</a></h2>
<ul>
<li>Schedulers should implement the new <code>reconcileTasks</code> driver method.</li>
<li>Schedulers should call the new <code>MesosSchedulerDriver</code> constructor that takes <code>Credential</code> to authenticate.</li>
<li>--authentication=false (default) allows both authenticated and unauthenticated frameworks to register.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries.</li>
<li>Restart the masters with --credentials pointing to credentials of the framework(s).</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.
Restart the masters with --authentication=true.</li>
</ol>
<p>NOTE: After the restart unauthenticated frameworks <em>will not</em> be allowed to register.</p>
<h2 id="upgrading-from-0130-to-0140"><a class="header" href="#upgrading-from-0130-to-0140">Upgrading from 0.13.0 to 0.14.0.</a></h2>
<ul>
<li>/vars endpoint has been removed.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
<li>Install the new agent binaries.</li>
<li>Restart the agents after adding --checkpoint flag to enable checkpointing.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Set FrameworkInfo.checkpoint in the scheduler if checkpointing is desired (recommended).</li>
<li>Restart the schedulers.</li>
<li>Restart the masters (to get rid of the cached FrameworkInfo).</li>
<li>Restart the agents (to get rid of the cached FrameworkInfo).</li>
</ol>
<h2 id="upgrading-from-0120-to-0130"><a class="header" href="#upgrading-from-0120-to-0130">Upgrading from 0.12.0 to 0.13.0.</a></h2>
<ul>
<li>cgroups_hierarchy_root agent flag is renamed as cgroups_hierarchy</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries.</li>
<li>Restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0110-to-0120"><a class="header" href="#upgrading-from-0110-to-0120">Upgrading from 0.11.0 to 0.12.0.</a></h2>
<ul>
<li>If you are a framework developer, you will want to examine the new 'source' field in the ExecutorInfo protobuf. This will allow you to take further advantage of the resource monitoring.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new agent binaries and restart the agents.</li>
<li>Install the new master binaries and restart the masters.</li>
</ol>
<h1 id="downgrade-mesos"><a class="header" href="#downgrade-mesos">Downgrade Mesos</a></h1>
<p>This document serves as a guide for users who wish to downgrade from an
existing Mesos cluster to a previous version. This usually happens when
rolling back from problematic upgrades. Mesos provides compatibility
between any 1.x and 1.y versions of masters/agents as long as new features
are not used. Since Mesos 1.8, we introduced a check for minimum capabilities
on the master. If a backwards incompatible feature is used, a corresponding
minimum capability entry will be persisted to the registry. If an old master
(that does not possess the capability) tries to recover from the registry
(e.g. when rolling back), an error message will be printed containing the
missing capabilities. This document lists the detailed information regarding
these minimum capabilities and remediation for downgrade errors.</p>
<h2 id="list-of-master-minimum-capabilities"><a class="header" href="#list-of-master-minimum-capabilities">List of Master Minimum Capabilities</a></h2>
<table class="table table-striped">
<thead>
<tr><th>Capability</th><th>Description</th>
</thead>
<tr>
  <td>
    <code>AGENT_DRAINING</code>
  </td>
  <td>
    This capability is required when any agent is marked for draining
    or deactivated.  These states were added in Mesos 1.9 and are
    triggered by using the <code>DRAIN_AGENT</code> or
    <code>DEACTIVATE_AGENT</code> operator APIs.
    <br/>
    To remove this minimum capability requirement:
    <ol>
      <li>
        Stop the master downgrade and return to the more recent version.
      </li>
      <li>
        Find all agents that are marked for draining or deactivated.
        This can be done by using the <code>GET_AGENTS</code> operator
        API and checking the <code>deactivated</code> boolean field of
        each agent.  All draining agents will also be deactivated.
      </li>
      <li>
        Use the <code>REACTIVATE_AGENT</code> operator API for each
        deactivated agent.
      </li>
    </ol>
  </td>
</tr>
<tr>
  <td>
    <code>QUOTA_V2</code>
  </td>
  <td>
    This capability is required when quota is configured in Mesos 1.9 or
    higher. When that happens, the newly configured quota will be persisted
    in the <code>quota_configs</code> field in the registry which requires this
    capability to decode.
    <br/>
    To remove this minimum capability requirement:
    <ol>
      <li>
        Stop the master downgrade and return to the more recent version.
      </li>
      <li>
        Use the <code>/registrar(id)/registry</code> endpoint to read the
        registry content and identify roles listed under the
        <code>quota_configs</code> field.
      </li>
      <li>
        Reset those roles' quota back to default (no guarantees and no limits).
        This will remove the roles from the <code>quota_configs</code> field.
        Once <code>quota_configs</code> becomes empty, the capability
        requirement will be removed.
      </li>
    </ol>
  </td>
</tr>
</table>
<h1 id="logging"><a class="header" href="#logging">Logging</a></h1>
<p>Mesos handles the logs of each Mesos component differently depending on the
degree of control Mesos has over the source code of the component.</p>
<p>Roughly, these categories are:</p>
<ul>
<li><a href="logging.html#Internal">Internal</a> - Master and Agent.</li>
<li><a href="logging.html#Containers">Containers</a> - Executors and Tasks.</li>
<li>External - Components launched outside of Mesos, like
Frameworks and <a href="high-availability.html">ZooKeeper</a>.  These are expected to
implement their own logging solution.</li>
</ul>
<h2 id="a-nameinternalainternal"><a class="header" href="#a-nameinternalainternal"><a name="Internal"></a>Internal</a></h2>
<p>The Mesos Master and Agent use the
<a href="https://github.com/google/glog">Google's logging library</a>.
For information regarding the command-line options used to configure this
library, see the
<a href="configuration/master-and-agent.html#logging-options">configuration documentation</a>.
Google logging options that are not explicitly mentioned there can be
configured via environment variables.</p>
<p>Both Master and Agent also expose a <a href="endpoints/logging/toggle.html">/logging/toggle</a>
HTTP endpoint which temporarily toggles verbose logging:</p>
<pre><code>POST &lt;ip:port&gt;/logging/toggle?level=[1|2|3]&amp;duration=VALUE
</code></pre>
<p>The effect is analogous to setting the <code>GLOG_v</code> environment variable prior
to starting the Master/Agent, except the logging level will revert to the
original level after the given duration.</p>
<h2 id="a-namecontainersacontainers"><a class="header" href="#a-namecontainersacontainers"><a name="Containers"></a>Containers</a></h2>
<p>For background, see <a href="containerizers.html">the containerizer documentation</a>.</p>
<p>Mesos does not assume any structured logging for entities running inside
containers.  Instead, Mesos will store the stdout and stderr of containers
into plain files (&quot;stdout&quot; and &quot;stderr&quot;) located inside
<a href="sandbox.html#where-is-it">the sandbox</a>.</p>
<p>In some cases, the default Container logger behavior of Mesos is not ideal:</p>
<ul>
<li>Logging may not be standardized across containers.</li>
<li>Logs are not easily aggregated.</li>
<li>Log file sizes are not managed.  Given enough time, the &quot;stdout&quot; and &quot;stderr&quot;
files can fill up the Agent's disk.</li>
</ul>
<h2 id="containerlogger-module"><a class="header" href="#containerlogger-module"><code>ContainerLogger</code> Module</a></h2>
<p>The <code>ContainerLogger</code> module was introduced in Mesos 0.27.0 and aims to address
the shortcomings of the default logging behavior for containers.  The module
can be used to change how Mesos redirects the stdout and stderr of containers.</p>
<p>The <a href="https://github.com/apache/mesos/blob/master/include/mesos/slave/container_logger.hpp">interface for a <code>ContainerLogger</code> can be found here</a>.</p>
<p>Mesos comes with two <code>ContainerLogger</code> modules:</p>
<ul>
<li>The <code>SandboxContainerLogger</code> implements the existing logging behavior as
a <code>ContainerLogger</code>.  This is the default behavior.</li>
<li>The <code>LogrotateContainerLogger</code> addresses the problem of unbounded log file
sizes.</li>
</ul>
<h3 id="logrotatecontainerlogger"><a class="header" href="#logrotatecontainerlogger"><code>LogrotateContainerLogger</code></a></h3>
<p>The <code>LogrotateContainerLogger</code> constrains the total size of a container's
stdout and stderr files.  The module does this by rotating log files based
on the parameters to the module.  When a log file reaches its specified
maximum size, it is renamed by appending a <code>.N</code> to the end of the filename,
where <code>N</code> increments each rotation.  Older log files are deleted when the
specified maximum number of files is reached.</p>
<h4 id="invoking-the-module"><a class="header" href="#invoking-the-module">Invoking the module</a></h4>
<p>The <code>LogrotateContainerLogger</code> can be loaded by specifying the library
<code>liblogrotate_container_logger.so</code> in the
<a href="modules.html#Invoking"><code>--modules</code> flag</a> when starting the Agent and by
setting the <code>--container_logger</code> Agent flag to
<code>org_apache_mesos_LogrotateContainerLogger</code>.</p>
<h4 id="module-parameters"><a class="header" href="#module-parameters">Module parameters</a></h4>
<table class="table table-striped">
  <thead>
    <tr>
      <th width="30%">
        Key
      </th>
      <th>
        Explanation
      </th>
    </tr>
  </thead>
<tr>
    <td>
      <code>max_stdout_size</code>/<code>max_stderr_size</code>
    </td>
    <td>
      Maximum size, in bytes, of a single stdout/stderr log file.
      When the size is reached, the file will be rotated.
<pre><code>  Defaults to 10 MB.  Minimum size of 1 (memory) page, usually around 4 KB.
&lt;/td&gt;
</code></pre>
</tr>
<tr>
    <td>
      <code>logrotate_stdout_options</code>/
      <code>logrotate_stderr_options</code>
    </td>
    <td>
      Additional config options to pass into <code>logrotate</code> for stdout.
      This string will be inserted into a <code>logrotate</code> configuration
      file. i.e. For "stdout":
      <pre>
/path/to/stdout {
  [logrotate_stdout_options]
  size [max_stdout_size]
}</pre>
      NOTE: The <code>size</code> option will be overridden by this module.
    </td>
  </tr>
<tr>
    <td>
      <code>environment_variable_prefix</code>
    </td>
    <td>
      Prefix for environment variables meant to modify the behavior of
      the logrotate logger for the specific container being launched.
      The logger will look for four prefixed environment variables in the
      container's <code>CommandInfo</code>'s <code>Environment</code>:
      <ul>
        <li><code>MAX_STDOUT_SIZE</code></li>
        <li><code>LOGROTATE_STDOUT_OPTIONS</code></li>
        <li><code>MAX_STDERR_SIZE</code></li>
        <li><code>LOGROTATE_STDERR_OPTIONS</code></li>
      </ul>
      If present, these variables will overwrite the global values set
      via module parameters.
<pre><code>  Defaults to &lt;code&gt;CONTAINER_LOGGER_&lt;/code&gt;.
&lt;/td&gt;
</code></pre>
</tr>
<tr>
    <td>
      <code>launcher_dir</code>
    </td>
    <td>
      Directory path of Mesos binaries.
      The <code>LogrotateContainerLogger</code> will find the
      <code>mesos-logrotate-logger</code> binary under this directory.
<pre><code>  Defaults to &lt;code&gt;/usr/local/libexec/mesos&lt;/code&gt;.
&lt;/td&gt;
</code></pre>
</tr>
<tr>
    <td>
      <code>logrotate_path</code>
    </td>
    <td>
      If specified, the <code>LogrotateContainerLogger</code> will use the
      specified <code>logrotate</code> instead of the system's
      <code>logrotate</code>.  If <code>logrotate</code> is not found, then
      the module will exit with an error.
    </td>
  </tr>
</table>
<h4 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h4>
<ol>
<li>Every time a container starts up, the <code>LogrotateContainerLogger</code>
starts up companion subprocesses of the <code>mesos-logrotate-logger</code> binary.</li>
<li>The module instructs Mesos to redirect the container's stdout/stderr
to the <code>mesos-logrotate-logger</code>.</li>
<li>As the container outputs to stdout/stderr, <code>mesos-logrotate-logger</code> will
pipe the output into the &quot;stdout&quot;/&quot;stderr&quot; files.  As the files grow,
<code>mesos-logrotate-logger</code> will call <code>logrotate</code> to keep the files strictly
under the configured maximum size.</li>
<li>When the container exits, <code>mesos-logrotate-logger</code> will finish logging before
exiting as well.</li>
</ol>
<p>The <code>LogrotateContainerLogger</code> is designed to be resilient across Agent
failover.  If the Agent process dies, any instances of <code>mesos-logrotate-logger</code>
will continue to run.</p>
<h3 id="writing-a-custom-containerlogger"><a class="header" href="#writing-a-custom-containerlogger">Writing a Custom <code>ContainerLogger</code></a></h3>
<p>For basics on module writing, see <a href="modules.html">the modules documentation</a>.</p>
<p>There are several caveats to consider when designing a new <code>ContainerLogger</code>:</p>
<ul>
<li>Logging by the <code>ContainerLogger</code> should be resilient to Agent failover.
If the Agent process dies (which includes the <code>ContainerLogger</code> module),
logging should continue.  This is usually achieved by using subprocesses.</li>
<li>When containers shut down, the <code>ContainerLogger</code> is not explicitly notified.
Instead, encountering <code>EOF</code> in the container's stdout/stderr signifies
that the container has exited.  This provides a stronger guarantee that the
<code>ContainerLogger</code> has seen all the logs before exiting itself.</li>
<li>The <code>ContainerLogger</code> should not assume that containers have been launched
with any specific <code>ContainerLogger</code>.  The Agent may be restarted with a
different <code>ContainerLogger</code>.</li>
<li>Each <a href="containerizers.html">containerizer</a> running on an Agent uses its own
instance of the <code>ContainerLogger</code>.  This means more than one <code>ContainerLogger</code>
may be running in a single Agent.  However, each Agent will only run a single
type of <code>ContainerLogger</code>.</li>
</ul>
<h1 id="mesos-observability-metrics"><a class="header" href="#mesos-observability-metrics">Mesos Observability Metrics</a></h1>
<p>This document describes the observability metrics provided by Mesos master and
agent nodes. This document also provides some initial guidance on which metrics
you should monitor to detect abnormal situations in your cluster.</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Mesos master and agent nodes report a set of statistics and metrics that enable
cluster operators to monitor resource usage and detect abnormal situations early. The
information reported by Mesos includes details about available resources, used
resources, registered frameworks, active agents, and task state. You can use
this information to create automated alerts and to plot different metrics over
time inside a monitoring dashboard.</p>
<p>Metric information is not persisted to disk at either master or agent
nodes, which means that metrics will be reset when masters and agents
are restarted. Similarly, if the current leading master fails and a new
leading master is elected, metrics at the new master will be reset.</p>
<h2 id="metric-types"><a class="header" href="#metric-types">Metric Types</a></h2>
<p>Mesos provides two different kinds of metrics: counters and gauges.</p>
<p><strong>Counters</strong> keep track of discrete events and are monotonically increasing. The
value of a metric of this type is always a natural number. Examples include the
number of failed tasks and the number of agent registrations. For some metrics
of this type, the rate of change is often more useful than the value itself.</p>
<p><strong>Gauges</strong> represent an instantaneous sample of some magnitude. Examples include
the amount of used memory in the cluster and the number of connected agents. For
some metrics of this type, it is often useful to determine whether the value is
above or below a threshold for a sustained period of time.</p>
<p>The tables in this document indicate the type of each available metric.</p>
<h2 id="master-nodes"><a class="header" href="#master-nodes">Master Nodes</a></h2>
<p>Metrics from each master node are available via the
<a href="endpoints/metrics/snapshot.html">/metrics/snapshot</a> master endpoint.  The response
is a JSON object that contains metrics names and values as key-value pairs.</p>
<h3 id="observability-metrics"><a class="header" href="#observability-metrics">Observability metrics</a></h3>
<p>This section lists all available metrics from Mesos master nodes grouped by
category.</p>
<h4 id="resources"><a class="header" href="#resources">Resources</a></h4>
<p>The following metrics provide information about the total resources available in
the cluster and their current usage. High resource usage for sustained periods
of time may indicate that you need to add capacity to your cluster or that a
framework is misbehaving.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/cpus_percent</code>
  </td>
  <td>Percentage of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_used</code>
  </td>
  <td>Number of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_total</code>
  </td>
  <td>Number of CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_revocable_total</code>
  </td>
  <td>Number of revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_percent</code>
  </td>
  <td>Percentage of allocated disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_used</code>
  </td>
  <td>Allocated disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_total</code>
  </td>
  <td>Disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_revocable_total</code>
  </td>
  <td>Revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_revocable_used</code>
  </td>
  <td>Allocated revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_percent</code>
  </td>
  <td>Percentage of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_used</code>
  </td>
  <td>Number of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_total</code>
  </td>
  <td>Number of GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_revocable_total</code>
  </td>
  <td>Number of revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_percent</code>
  </td>
  <td>Percentage of allocated memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_used</code>
  </td>
  <td>Allocated memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_total</code>
  </td>
  <td>Memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_revocable_total</code>
  </td>
  <td>Revocable memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_revocable_used</code>
  </td>
  <td>Allocated revocable memory in MB</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="master"><a class="header" href="#master">Master</a></h4>
<p>The following metrics provide information about whether a master is currently
elected and how long it has been running. A cluster with no elected master
for sustained periods of time indicates a malfunctioning cluster. This
points to either leadership election issues (so check the connection to
ZooKeeper) or a flapping Master process. A low uptime value indicates that the
master has restarted recently.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/elected</code>
  </td>
  <td>Whether this is the elected master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/uptime_secs</code>
  </td>
  <td>Uptime in seconds</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="system"><a class="header" href="#system">System</a></h4>
<p>The following metrics provide information about the resources available on this
master node and their current usage. High resource usage in a master node for
sustained periods of time may degrade the performance of the cluster.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>system/cpus_total</code>
  </td>
  <td>Number of CPUs available in this master node</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_15min</code>
  </td>
  <td>Load average for the past 15 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_5min</code>
  </td>
  <td>Load average for the past 5 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_1min</code>
  </td>
  <td>Load average for the past minute</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_free_bytes</code>
  </td>
  <td>Free memory in bytes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_total_bytes</code>
  </td>
  <td>Total memory in bytes</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="agents"><a class="header" href="#agents">Agents</a></h4>
<p>The following metrics provide information about agent events, agent counts, and
agent states. A low number of active agents may indicate that agents are
unhealthy or that they are not able to connect to the elected master.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/slave_registrations</code>
  </td>
  <td>Number of agents that were able to cleanly re-join the cluster and
      connect back to the master after the master is disconnected.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals</code>
  </td>
  <td>Number of agent removed for various reasons, including maintenance</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_reregistrations</code>
  </td>
  <td>Number of agent re-registrations</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_unreachable_scheduled</code>
  </td>
  <td>Number of agents which have failed their health check and are scheduled
      to be marked unreachable. They will not be marked unreachable immediately due to the Agent
      Removal Rate-Limit, but <code>master/slave_unreachable_completed</code>
      will start increasing as they do get removed.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_unreachable_canceled</code>
  </td>
  <td>Number of times that an agent was due to be marked unreachable but this
      transition was cancelled. This happens when the agent removal rate limit
      is enabled and the agent sends a <code>PONG</code> response message to the
      master before the rate limit allows the agent to be marked unreachable.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_unreachable_completed</code>
  </td>
  <td>Number of agents that were marked as unreachable because they failed
      health checks. These are agents which were not heard from despite the
      agent-removal rate limit, and have been marked as unreachable in the
      master's agent registry.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slaves_active</code>
  </td>
  <td>Number of active agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_connected</code>
  </td>
  <td>Number of connected agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_disconnected</code>
  </td>
  <td>Number of disconnected agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_inactive</code>
  </td>
  <td>Number of inactive agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_unreachable</code>
  </td>
  <td>Number of unreachable agents. Unreachable agents are periodically
      garbage collected from the registry, which will cause this value to
      decrease.</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="frameworks"><a class="header" href="#frameworks">Frameworks</a></h4>
<p>The following metrics provide information about the registered frameworks in the
cluster. No active or connected frameworks may indicate that a scheduler is not
registered or that it is misbehaving.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/frameworks_active</code>
  </td>
  <td>Number of active frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks_connected</code>
  </td>
  <td>Number of connected frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks_disconnected</code>
  </td>
  <td>Number of disconnected frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks_inactive</code>
  </td>
  <td>Number of inactive frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/outstanding_offers</code>
  </td>
  <td>Number of outstanding resource offers</td>
  <td>Gauge</td>
</tr>
</table>
<p>The following metrics are added for each framework which registers with the
master, in order to provide detailed information about the behavior of the
framework. The framework name is percent-encoded before creating these metrics;
the actual name can be recovered by percent-decoding.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/subscribed</code>
  </td>
  <td>Whether or not this framework is currently subscribed</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/calls</code>
  </td>
  <td>Total number of calls sent by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/calls/&lt;CALL_TYPE&gt;</code>
  </td>
  <td>Number of each type of call sent by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/events</code>
  </td>
  <td>Total number of events sent to this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/events/&lt;EVENT_TYPE&gt;</code>
  </td>
  <td>Number of each type of event sent to this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/operations</code>
  </td>
  <td>Total number of offer operations performed by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/operations/&lt;OPERATION_TYPE&gt;</code>
  </td>
  <td>Number of each type of offer operation performed by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/tasks/active/&lt;TASK_STATE&gt;</code>
  </td>
  <td>Number of this framework's tasks currently in each active task state</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/tasks/terminal/&lt;TASK_STATE&gt;</code>
  </td>
  <td>Number of this framework's tasks which have transitioned into each terminal task state</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/sent</code>
  </td>
  <td>Number of offers sent to this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/accepted</code>
  </td>
  <td>Number of offers accepted by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/declined</code>
  </td>
  <td>Number of offers explicitly declined by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/rescinded</code>
  </td>
  <td>Number of offers sent to this framework which were subsequently rescinded</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/roles/&lt;ROLE_NAME&gt;/suppressed</code>
  </td>
  <td>For each of the framework's subscribed roles, whether or not offers for that role are currently suppressed</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="tasks"><a class="header" href="#tasks">Tasks</a></h4>
<p>The following metrics provide information about active and terminated tasks. A
high rate of lost tasks may indicate that there is a problem with the cluster.
The task states listed here match those of the task state machine.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/tasks_error</code>
  </td>
  <td>Number of tasks that were invalid</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_failed</code>
  </td>
  <td>Number of failed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_finished</code>
  </td>
  <td>Number of finished tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_killed</code>
  </td>
  <td>Number of killed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_killing</code>
  </td>
  <td>Number of tasks currently being killed</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_lost</code>
  </td>
  <td>Number of lost tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_running</code>
  </td>
  <td>Number of running tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_staging</code>
  </td>
  <td>Number of staging tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_starting</code>
  </td>
  <td>Number of starting tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_unreachable</code>
  </td>
  <td>Number of unreachable tasks</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="operations"><a class="header" href="#operations">Operations</a></h4>
<p>The following metrics provide information about offer operations on the master.</p>
<p>Below, <code>OPERATION_TYPE</code> refers to any one of <code>reserve</code>, <code>unreserve</code>, <code>create</code>,
<code>destroy</code>, <code>grow_volume</code>, <code>shrink_volume</code>, <code>create_disk</code> or <code>destroy_disk</code>.</p>
<p>NOTE: The counter for terminal operation states can over-count over time. In
particular if an agent contained unacknowledged terminal status updates when
it was marked gone or marked unreachable, these operations will be double-counted
as both their original state and <code>OPERATION_GONE</code>/<code>OPERATION_UNREACHABLE</code>.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/operations/total</code>
  </td>
  <td>Total number of operations known to this master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations in the given non-terminal state (`pending`, `recovering` or `unreachable`)</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations in the given terminal state (`finished`, `error`, `dropped` or `gone_by_operator`)</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_TYPE&gt;/total</code>
  </td>
  <td>Total number of operations with the given type known to this master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_TYPE&gt;/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations with the given type in the given non-terminal state (`pending`, `recovering` or `unreachable`)</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_TYPE&gt;/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations with the given type in the given state (`finished`, `error`, `dropped` or `gone_by_operator`)</td>
  <td>Counter</td>
</tr>
</table>
<h4 id="messages"><a class="header" href="#messages">Messages</a></h4>
<p>The following metrics provide information about messages between the master and
the agents and between the framework and the executors. A high rate of dropped
messages may indicate that there is a problem with the network.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/invalid_executor_to_framework_messages</code>
  </td>
  <td>Number of invalid executor to framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_framework_to_executor_messages</code>
  </td>
  <td>Number of invalid framework to executor messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_operation_status_update_acknowledgements</code>
  </td>
  <td>Number of invalid operation status update acknowledgements</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_status_update_acknowledgements</code>
  </td>
  <td>Number of invalid status update acknowledgements</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_status_updates</code>
  </td>
  <td>Number of invalid status updates</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/dropped_messages</code>
  </td>
  <td>Number of dropped messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_authenticate</code>
  </td>
  <td>Number of authentication messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_deactivate_framework</code>
  </td>
  <td>Number of framework deactivation messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_decline_offers</code>
  </td>
  <td>Number of offers declined</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_executor_to_framework</code>
  </td>
  <td>Number of executor to framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_exited_executor</code>
  </td>
  <td>Number of terminated executor messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_framework_to_executor</code>
  </td>
  <td>Number of messages from a framework to an executor</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_kill_task</code>
  </td>
  <td>Number of kill task messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_launch_tasks</code>
  </td>
  <td>Number of launch task messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_operation_status_update_acknowledgement</code>
  </td>
  <td>Number of operation status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reconcile_operations</code>
  </td>
  <td>Number of reconcile operations messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reconcile_tasks</code>
  </td>
  <td>Number of reconcile task messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_register_framework</code>
  </td>
  <td>Number of framework registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_register_slave</code>
  </td>
  <td>Number of agent registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reregister_framework</code>
  </td>
  <td>Number of framework re-registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reregister_slave</code>
  </td>
  <td>Number of agent re-registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_resource_request</code>
  </td>
  <td>Number of resource request messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_revive_offers</code>
  </td>
  <td>Number of offer revival messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_status_update</code>
  </td>
  <td>Number of status update messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_status_update_acknowledgement</code>
  </td>
  <td>Number of status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_unregister_framework</code>
  </td>
  <td>Number of framework unregistration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_unregister_slave</code>
  </td>
  <td>Number of agent unregistration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_update_slave</code>
  </td>
  <td>Number of update agent messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/recovery_slave_removals</code>
  </td>
  <td>Number of agents not reregistered during master failover</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals/reason_registered</code>
  </td>
  <td>Number of agents removed when new agents registered at the same address</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals/reason_unhealthy</code>
  </td>
  <td>Number of agents failed due to failed health checks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals/reason_unregistered</code>
  </td>
  <td>Number of agents unregistered</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_framework_to_executor_messages</code>
  </td>
  <td>Number of valid framework to executor messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_operation_status_update_acknowledgements</code>
  </td>
  <td>Number of valid operation status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_status_update_acknowledgements</code>
  </td>
  <td>Number of valid status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_status_updates</code>
  </td>
  <td>Number of valid status update messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/task_lost/source_master/reason_invalid_offers</code>
  </td>
  <td>Number of tasks lost due to invalid offers</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/task_lost/source_master/reason_slave_removed</code>
  </td>
  <td>Number of tasks lost due to agent removal</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/task_lost/source_slave/reason_executor_terminated</code>
  </td>
  <td>Number of tasks lost due to executor termination</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_executor_to_framework_messages</code>
  </td>
  <td>Number of valid executor to framework messages</td>
  <td>Counter</td>
</tr>
</table>
<h4 id="event-queue"><a class="header" href="#event-queue">Event queue</a></h4>
<p>The following metrics provide information about different types of events in the
event queue.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/event_queue_dispatches</code>
  </td>
  <td>Number of dispatches in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/event_queue_http_requests</code>
  </td>
  <td>Number of HTTP requests in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/event_queue_messages</code>
  </td>
  <td>Number of messages in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operator_event_stream_subscribers</code>
  </td>
  <td>Number of subscribers to the operator event stream</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="registrar"><a class="header" href="#registrar">Registrar</a></h4>
<p>The following metrics provide information about read and write latency to the
agent registrar.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>registrar/state_fetch_ms</code>
  </td>
  <td>Registry read latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms</code>
  </td>
  <td>Registry write latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/max</code>
  </td>
  <td>Maximum registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/min</code>
  </td>
  <td>Minimum registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p50</code>
  </td>
  <td>Median registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p90</code>
  </td>
  <td>90th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p95</code>
  </td>
  <td>95th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p99</code>
  </td>
  <td>99th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p999</code>
  </td>
  <td>99.9th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p9999</code>
  </td>
  <td>99.99th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="replicated-log"><a class="header" href="#replicated-log">Replicated log</a></h4>
<p>The following metrics provide information about the replicated log underneath
the registrar, which is the persistent store for masters.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>registrar/log/recovered</code>
  </td>
  <td>
    Whether the replicated log for the registrar has caught up with the other
    masters in the cluster. A cluster is operational as long as a quorum of
    "recovered" masters is available in the cluster.
  </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/log/ensemble_size</code>
  </td>
  <td>
    The number of masters in the ensemble (cluster) that the current master
    communicates with (including itself) to form the replicated log quorum.
    It's imperative that this number is always less than `--quorum * 2` to
    prevent split-brain. It's also important that it should be greater than
    or equal to `--quorum` to maintain availability.
  </td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="allocator"><a class="header" href="#allocator">Allocator</a></h4>
<p>The following metrics provide information about performance
and resource allocations in the allocator.</p>
<table class="table table-stripped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms</code>
  </td>
  <td>Time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/count</code>
  </td>
  <td>Number of allocation algorithm time measurements in the window</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/max</code>
  </td>
  <td>Maximum time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/min</code>
  </td>
  <td>Minimum time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p50</code>
  </td>
  <td>Median time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p90</code>
  </td>
  <td>90th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p95</code>
  </td>
  <td>95th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p99</code>
  </td>
  <td>99th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p999</code>
  </td>
  <td>99.9th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p9999</code>
  </td>
  <td>99.99th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_runs</code>
  </td>
  <td>Number of times the allocation algorithm has run</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms</code>
  </td>
  <td>Allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/count</code>
  </td>
  <td>Number of allocation batch latency measurements in the window</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/max</code>
  </td>
  <td>Maximum allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/min</code>
  </td>
  <td>Minimum allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p50</code>
  </td>
  <td>Median allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p90</code>
  </td>
  <td>90th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p95</code>
  </td>
  <td>95th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p99</code>
  </td>
  <td>99th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p999</code>
  </td>
  <td>99.9th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p9999</code>
  </td>
  <td>99.99th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/roles/<i>&lt;role&gt;</i>/shares/dominant</code>
  </td>
  <td>Dominant <i>resource</i> share for the <i>role</i>, exposed as a percentage (0.0-1.0)</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/event_queue_dispatches</code>
  </td>
  <td>Number of dispatch events in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/offer_filters/roles/<i>&lt;role&gt;</i>/active</code>
  </td>
  <td>Number of active offer filters for all frameworks within the <i>role</i></td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/quota/roles/<i>&lt;role&gt;</i>/resources/<i>&lt;resource&gt;</i>/offered_or_allocated</code>
  </td>
  <td>Amount of <i>resource</i>s considered offered or allocated towards
      a <i>role</i>'s quota guarantee</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/quota/roles/<i>&lt;role&gt;</i>/resources/<i>&lt;resource&gt;</i>/guarantee</code>
  </td>
  <td>Amount of <i>resource</i>s guaranteed for a <i>role</i> via quota</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/cpus/offered_or_allocated</code>
  </td>
  <td>Number of CPUs offered or allocated</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/cpus/total</code>
  </td>
  <td>Number of CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/disk/offered_or_allocated</code>
  </td>
  <td>Allocated or offered disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/disk/total</code>
  </td>
  <td>Total disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/mem/offered_or_allocated</code>
  </td>
  <td>Allocated or offered memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/mem/total</code>
  </td>
  <td>Total memory in MB</td>
  <td>Gauge</td>
</tr>
</table>
<h3 id="basic-alerts"><a class="header" href="#basic-alerts">Basic Alerts</a></h3>
<p>This section lists some examples of basic alerts that you can use to detect
abnormal situations in a cluster.</p>
<h4 id="masteruptime_secs-is-low"><a class="header" href="#masteruptime_secs-is-low">master/uptime_secs is low</a></h4>
<p>The master has restarted.</p>
<h4 id="masteruptime_secs--60-for-sustained-periods-of-time"><a class="header" href="#masteruptime_secs--60-for-sustained-periods-of-time">master/uptime_secs &lt; 60 for sustained periods of time</a></h4>
<p>The cluster has a flapping master node.</p>
<h4 id="mastertasks_lost-is-increasing-rapidly"><a class="header" href="#mastertasks_lost-is-increasing-rapidly">master/tasks_lost is increasing rapidly</a></h4>
<p>Tasks in the cluster are disappearing. Possible causes include hardware
failures, bugs in one of the frameworks, or bugs in Mesos.</p>
<h4 id="masterslaves_active-is-low"><a class="header" href="#masterslaves_active-is-low">master/slaves_active is low</a></h4>
<p>Agents are having trouble connecting to the master.</p>
<h4 id="mastercpus_percent--09-for-sustained-periods-of-time"><a class="header" href="#mastercpus_percent--09-for-sustained-periods-of-time">master/cpus_percent &gt; 0.9 for sustained periods of time</a></h4>
<p>Cluster CPU utilization is close to capacity.</p>
<h4 id="mastermem_percent--09-for-sustained-periods-of-time"><a class="header" href="#mastermem_percent--09-for-sustained-periods-of-time">master/mem_percent &gt; 0.9 for sustained periods of time</a></h4>
<p>Cluster memory utilization is close to capacity.</p>
<h4 id="masterelected-is-0-for-sustained-periods-of-time"><a class="header" href="#masterelected-is-0-for-sustained-periods-of-time">master/elected is 0 for sustained periods of time</a></h4>
<p>No master is currently elected.</p>
<h2 id="agent-nodes"><a class="header" href="#agent-nodes">Agent Nodes</a></h2>
<p>Metrics from each agent node are available via the
<a href="endpoints/metrics/snapshot.html">/metrics/snapshot</a> agent endpoint.  The response
is a JSON object that contains metrics names and values as key-value pairs.</p>
<h3 id="observability-metrics-1"><a class="header" href="#observability-metrics-1">Observability Metrics</a></h3>
<p>This section lists all available metrics from Mesos agent nodes grouped by
category.</p>
<h4 id="resources-1"><a class="header" href="#resources-1">Resources</a></h4>
<p>The following metrics provide information about the total resources available in
the agent and their current usage.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>containerizer/fetcher/cache_size_total_bytes</code>
  </td>
  <td>The configured maximum size of the fetcher cache in bytes. This value is
  constant for the life of the Mesos agent.</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/fetcher/cache_size_used_bytes</code>
  </td>
  <td>The current amount of data stored in the fetcher cache in bytes.</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>gc/path_removals_failed</code>
  </td>
  <td>Number of times the agent garbage collection process has failed to remove a sandbox path.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>gc/path_removals_pending</code>
  </td>
  <td>Number of sandbox paths that are currently pending agent garbage collection.</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>gc/path_removals_succeeded</code>
  </td>
  <td>Number of sandbox paths the agent successfully removed.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_percent</code>
  </td>
  <td>Percentage of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_used</code>
  </td>
  <td>Number of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_total</code>
  </td>
  <td>Number of CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_revocable_total</code>
  </td>
  <td>Number of revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_percent</code>
  </td>
  <td>Percentage of allocated disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_used</code>
  </td>
  <td>Allocated disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_total</code>
  </td>
  <td>Disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_percent</code>
  </td>
  <td>Percentage of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_used</code>
  </td>
  <td>Number of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_total</code>
  </td>
  <td>Number of GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_revocable_total</code>
  </td>
  <td>Number of revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_percent</code>
  </td>
  <td>Percentage of allocated memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_revocable_total</code>
  </td>
  <td>Revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_revocable_used</code>
  </td>
  <td>Allocated revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_used</code>
  </td>
  <td>Allocated memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_total</code>
  </td>
  <td>Memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_revocable_total</code>
  </td>
  <td>Revocable memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_revocable_used</code>
  </td>
  <td>Allocated revocable memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>volume_gid_manager/volume_gids_total</code>
  </td>
  <td>Number of gids configured for volume gid manager</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>volume_gid_manager/volume_gids_free</code>
  </td>
  <td>Number of free gids available for volume gid manager</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="agent"><a class="header" href="#agent">Agent</a></h4>
<p>The following metrics provide information about whether an agent is currently
registered with a master and for how long it has been running.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>slave/registered</code>
  </td>
  <td>Whether this agent is registered with a master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/uptime_secs</code>
  </td>
  <td>Uptime in seconds</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="system-1"><a class="header" href="#system-1">System</a></h4>
<p>The following metrics provide information about the agent system.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>system/cpus_total</code>
  </td>
  <td>Number of CPUs available</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_15min</code>
  </td>
  <td>Load average for the past 15 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_5min</code>
  </td>
  <td>Load average for the past 5 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_1min</code>
  </td>
  <td>Load average for the past minute</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_free_bytes</code>
  </td>
  <td>Free memory in bytes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_total_bytes</code>
  </td>
  <td>Total memory in bytes</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="executors"><a class="header" href="#executors">Executors</a></h4>
<p>The following metrics provide information about the executor instances running
on the agent.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>containerizer/mesos/container_destroy_errors</code>
  </td>
  <td>Number of containers destroyed due to launch errors</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>containerizer/fetcher/task_fetches_succeeded</code>
  </td>
  <td>Total number of times the Mesos fetcher successfully fetched all the URIs for a task.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>containerizer/fetcher/task_fetches_failed</code>
  </td>
  <td>Number of times the Mesos fetcher failed to fetch all the URIs for a task.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/container_launch_errors</code>
  </td>
  <td>Number of container launch errors</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/executors_preempted</code>
  </td>
  <td>Number of executors destroyed due to preemption</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/frameworks_active</code>
  </td>
  <td>Number of active frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executor_directory_max_allowed_age_secs</code>
  </td>
  <td>Maximum allowed age in seconds to delete executor directory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executors_registering</code>
  </td>
  <td>Number of executors registering</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executors_running</code>
  </td>
  <td>Number of executors running</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executors_terminated</code>
  </td>
  <td>Number of terminated executors</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/executors_terminating</code>
  </td>
  <td>Number of terminating executors</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/recovery_errors</code>
  </td>
  <td>Number of errors encountered during agent recovery</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/recovery_time_secs</code>
  </td>
  <td>Agent recovery time in seconds. This value is only available after agent
  recovery succeeded and remains constant for the life of the Mesos agent.</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="tasks-1"><a class="header" href="#tasks-1">Tasks</a></h4>
<p>The following metrics provide information about active and terminated tasks.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>slave/tasks_failed</code>
  </td>
  <td>Number of failed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_finished</code>
  </td>
  <td>Number of finished tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_killed</code>
  </td>
  <td>Number of killed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_lost</code>
  </td>
  <td>Number of lost tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_running</code>
  </td>
  <td>Number of running tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_staging</code>
  </td>
  <td>Number of staging tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_starting</code>
  </td>
  <td>Number of starting tasks</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="messages-1"><a class="header" href="#messages-1">Messages</a></h4>
<p>The following metrics provide information about messages between the agents and
the master it is registered with.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>slave/invalid_framework_messages</code>
  </td>
  <td>Number of invalid framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/invalid_status_updates</code>
  </td>
  <td>Number of invalid status updates</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/valid_framework_messages</code>
  </td>
  <td>Number of valid framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/valid_status_updates</code>
  </td>
  <td>Number of valid status updates</td>
  <td>Counter</td>
</tr>
</table>
<h4 id="containerizers"><a class="header" href="#containerizers">Containerizers</a></h4>
<p>The following metrics provide information about both Mesos and Docker
containerizers.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms</code>
  </td>
  <td>Docker containerizer image pull latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/count</code>
  </td>
  <td>Number of Docker containerizer image pulls</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/max</code>
  </td>
  <td>Maximum Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/min</code>
  </td>
  <td>Minimum Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p50</code>
  </td>
  <td>Median Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p90</code>
  </td>
  <td>90th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p95</code>
  </td>
  <td>95th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p99</code>
  </td>
  <td>99th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p999</code>
  </td>
  <td>99.9th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p9999</code>
  </td>
  <td>99.99th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/disk/project_ids_free</code>
  </td>
  <td>Number of free project IDs available to the XFS Disk isolator</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/disk/project_ids_total</code>
  </td>
  <td>Number of project IDs configured for the XFS Disk isolator</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms</code>
  </td>
  <td>Mesos containerizer docker image pull latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/count</code>
  </td>
  <td>Number of Mesos containerizer docker image pulls</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/max</code>
  </td>
  <td>Maximum Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/min</code>
  </td>
  <td>Minimum Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p50</code>
  </td>
  <td>Median Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p90</code>
  </td>
  <td>90th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p95</code>
  </td>
  <td>95th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p99</code>
  </td>
  <td>99th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p999</code>
  </td>
  <td>99.9th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p9999</code>
  </td>
  <td>99.99th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="resource-providers"><a class="header" href="#resource-providers">Resource Providers</a></h4>
<p>The following metrics provide information about ongoing and completed
<a href="operations.html">operations</a> that apply to resources provided by a
<a href="resource-provider.html">resource provider</a> with the given <em>type</em> and <em>name</em>. In
the following metrics, the <em>operation</em> placeholder refers to the name of a
particular operation type, which is described in the list of
<a href="monitoring.html#supported-operation-types">supported operation types</a>.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/pending</code>
  </td>
  <td>Number of ongoing <i>operation</i>s</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/finished</code>
  </td>
  <td>Number of finished <i>operation</i>s</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/failed</code>
  </td>
  <td>Number of failed <i>operation</i>s</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/dropped</code>
  </td>
  <td>Number of dropped <i>operation</i>s</td>
  <td>Counter</td>
</tr>
</table>
<h5 id="supported-operation-types"><a class="header" href="#supported-operation-types">Supported Operation Types</a></h5>
<p>Since the supported operation types may vary among different resource providers,
the following is a comprehensive list of operation types and the corresponding
resource providers that support them. Note that the name column is for the
<em>operation</em> placeholder in the above metrics.</p>
<table class="table table-striped">
<thead>
<tr><th>Type</th><th>Name</th><th>Supported Resource Provider Types</th>
</thead>
<tr>
  <td><code><a href="reservation.html">RESERVE</a></code></td>
  <td><code>reserve</code></td>
  <td>All</td>
</tr>
<tr>
  <td><code><a href="reservation.html">UNRESERVE</a></code></td>
  <td><code>unreserve</code></td>
  <td>All</td>
</tr>
<tr>
  <td><code><a href="persistent-volume.html#-offer-operation-create-">CREATE</a></code></td>
  <td><code>create</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
<tr>
  <td><code><a href="persistent-volume.html#-offer-operation-destroy-">DESTROY</a></code></td>
  <td><code>destroy</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
<tr>
  <td><code><a href="csi.html#-create_disk-operation">CREATE_DISK</a></code></td>
  <td><code>create_disk</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
<tr>
  <td><code><a href="csi.html#-destroy_disk-operation">DESTROY_DISK</a></code></td>
  <td><code>destroy_disk</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
</table>
<p>For example, cluster operators can monitor the number of successful
<code>CREATE_VOLUME</code> operations that are applied to the resource provider with type
<code>org.apache.mesos.rp.local.storage</code> and name <code>lvm</code> through the
<code>resource_providers/org.apache.mesos.rp.local.storage.lvm/operations/create_disk/finished</code>
metric.</p>
<h4 id="csi-plugins"><a class="header" href="#csi-plugins">CSI Plugins</a></h4>
<p>Storage resource providers in Mesos are backed by
<a href="csi.html#standalone-containers-for-csi-plugins">CSI plugins</a> running in
<a href="standalone-container.html">standalone containers</a>. To monitor the health of these
CSI plugins for a storage resource provider with <em>type</em> and <em>name</em>, the
following metrics provide information about plugin terminations and ongoing and
completed CSI calls made to the plugin.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/container_terminations</code>
  </td>
  <td>Number of terminated CSI plugin containers</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_pending</code>
  </td>
  <td>Number of ongoing CSI calls</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_finished</code>
  </td>
  <td>Number of successful CSI calls</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_failed</code>
  </td>
  <td>Number of failed CSI calls</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_cancelled</code>
  </td>
  <td>Number of cancelled CSI calls</td>
  <td>Counter</td>
</tr>
</table>
<h1 id="the-new-cli"><a class="header" href="#the-new-cli">The new CLI</a></h1>
<p>The new Mesos Command Line Interface provides one executable Python 3
script to run all default commands and additional custom plugins.</p>
<p>Two of the subcommands available allow you to debug running containers:</p>
<ul>
<li><code>mesos task exec</code>, to run a command in a running task's container.</li>
<li><code>mesos task attach</code>, to attach your local terminal to a running task
and stream its input/output.</li>
</ul>
<h2 id="building-the-cli"><a class="header" href="#building-the-cli">Building the CLI</a></h2>
<p>For now, the Mesos CLI is still under development and not built as part
of a standard Mesos distribution.</p>
<p>However, the CLI can be built using <a href="configuration/autotools.html">Autotools</a> and
<a href="configuration/cmake.html">Cmake options</a>. If necessary, check the options
described in the linked pages to set Python 3 before starting a build.</p>
<p>The result of this build will be a <code>mesos</code> binary that can be executed.</p>
<h2 id="using-the-cli"><a class="header" href="#using-the-cli">Using the CLI</a></h2>
<p>Using the CLI without building Mesos is also possible. To do so, activate
the CLI virtual environment by following the steps described below:</p>
<pre><code>$ cd src/python/cli_new/
$ PYTHON=python3 ./bootstrap
$ source activate
$ mesos
</code></pre>
<p>Calling <code>mesos</code> will then run the CLI and calling <code>mesos-cli-tests</code> will
run the integration tests.</p>
<h2 id="configuring-the-cli"><a class="header" href="#configuring-the-cli">Configuring the CLI</a></h2>
<p>The CLI uses a configuration file to know where the masters of the cluster are
as well as list any plugins that should be used in addition to the default ones
provided.</p>
<p>The configuation file, located by default at <code>~/.mesos/config.toml</code>, looks
like this:</p>
<pre><code># The `plugins` array lists the absolute paths of the
# plugins you want to add to the CLI.
plugins = [
  &quot;&lt;/absolute/path/to/plugin-1/directory&gt;&quot;,
  &quot;&lt;/absolute/path/to/plugin-2/directory&gt;&quot;
]

# The `master` field is either composed of an `address` field
# or a `zookeeper` field, but not both. For example:
[master]
  address = &quot;10.10.0.30:5050&quot;
  # The `zookeeper` field has an `addresses` array and a `path` field.
  # [master.zookeeper]
  #   addresses = [
  #     &quot;10.10.0.31:5050&quot;,
  #     &quot;10.10.0.32:5050&quot;,
  #     &quot;10.10.0.33:5050&quot;
  #   ]
  #   path = &quot;/mesos&quot;
</code></pre>
<h1 id="operational-guide"><a class="header" href="#operational-guide">Operational Guide</a></h1>
<h2 id="using-a-process-supervisor"><a class="header" href="#using-a-process-supervisor">Using a process supervisor</a></h2>
<p>Mesos uses a &quot;<a href="https://en.wikipedia.org/wiki/Fail-fast">fail-fast</a>&quot; approach to error handling: if a serious error occurs, Mesos will typically exit rather than trying to continue running in a possibly erroneous state. For example, when Mesos is configured for <a href="high-availability.html">high availability</a>, the leading master will abort itself when it discovers it has been partitioned away from the Zookeeper quorum. This is a safety precaution to ensure the previous leader doesn't continue communicating in an unsafe state.</p>
<p>To ensure that such failures are handled appropriately, production deployments of Mesos typically use a <em>process supervisor</em> (such as systemd or supervisord) to detect when Mesos processes exit. The supervisor can be configured to restart the failed process automatically and/or to notify the cluster operator to investigate the situation.</p>
<h2 id="changing-the-master-quorum"><a class="header" href="#changing-the-master-quorum">Changing the master quorum</a></h2>
<p>The master leverages a <a href="replicated-log-internals.html">Paxos-based replicated log</a> as its storage backend (<code>--registry=replicated_log</code> is the only storage backend currently supported). Each master participates in the ensemble as a log replica. The <code>--quorum</code> flag determines a majority of the masters.</p>
<p>The following table shows the tolerance to master failures for each quorum size:</p>
<table><thead><tr><th align="right">Masters</th><th align="right">Quorum Size</th><th align="right">Failure Tolerance</th></tr></thead><tbody>
<tr><td align="right">1</td><td align="right">1</td><td align="right">0</td></tr>
<tr><td align="right">3</td><td align="right">2</td><td align="right">1</td></tr>
<tr><td align="right">5</td><td align="right">3</td><td align="right">2</td></tr>
<tr><td align="right">...</td><td align="right">...</td><td align="right">...</td></tr>
<tr><td align="right">2N - 1</td><td align="right">N</td><td align="right">N - 1</td></tr>
</tbody></table>
<p>It is recommended to run with 3 or 5 masters, when desiring high availability.</p>
<h3 id="note"><a class="header" href="#note">NOTE</a></h3>
<p>When configuring the quorum, it is essential to ensure that there are only so many masters running as specified in the table above. If additional masters are running, this violates the quorum and the log may be corrupted! As a result, it is recommended to gate the running of the master process with something that enforces a static whitelist of the master hosts. See <a href="https://issues.apache.org/jira/browse/MESOS-1546">MESOS-1546</a> for adding a safety whitelist within Mesos itself.</p>
<p>For online reconfiguration of the log, see: <a href="https://issues.apache.org/jira/browse/MESOS-683">MESOS-683</a>.</p>
<h3 id="increasing-the-quorum-size"><a class="header" href="#increasing-the-quorum-size">Increasing the quorum size</a></h3>
<p>As the size of a cluster grows, it may be desired to increase the quorum size for additional fault tolerance.</p>
<p>The following steps indicate how to increment the quorum size, using 3 -&gt; 5 masters as an example (quorum size 2 -&gt; 3):</p>
<ol>
<li>Initially, 3 masters are running with <code>--quorum=2</code></li>
<li>Restart the original 3 masters with <code>--quorum=3</code></li>
<li>Start 2 additional masters with <code>--quorum=3</code></li>
</ol>
<p>To increase the quorum by N, repeat this process to increment the quorum size N times.</p>
<p>NOTE: Currently, moving out of a single master setup requires wiping the replicated log
state and starting fresh. This will wipe all persistent data (e.g., agents, maintenance
information, quota information, etc). To move from 1 master to 3 masters:</p>
<ol>
<li>Stop the standalone master.</li>
<li>Remove the replicated log data (<code>replicated_log</code> under the <code>--work_dir</code>).</li>
<li>Start the original master and two new masters with <code>--quorum=2</code></li>
</ol>
<h3 id="decreasing-the-quorum-size"><a class="header" href="#decreasing-the-quorum-size">Decreasing the quorum size</a></h3>
<p>The following steps indicate how to decrement the quorum size, using 5 -&gt; 3 masters as an example (quorum size 3 -&gt; 2):</p>
<ol>
<li>Initially, 5 masters are running with <code>--quorum=3</code></li>
<li>Remove 2 masters from the cluster, ensure they will not be restarted (see NOTE section above). Now 3 masters are running with <code>--quorum=3</code></li>
<li>Restart the 3 masters with <code>--quorum=2</code></li>
</ol>
<p>To decrease the quorum by N, repeat this process to decrement the quorum size N times.</p>
<h3 id="replacing-a-master"><a class="header" href="#replacing-a-master">Replacing a master</a></h3>
<p>Please see the NOTE section above. So long as the failed master is guaranteed to not re-join the ensemble, it is safe to start a new master <em>with an empty log</em> and allow it to catch up.</p>
<h2 id="external-access-for-mesos-master"><a class="header" href="#external-access-for-mesos-master">External access for Mesos master</a></h2>
<p>If the default IP (or the command line arg <code>--ip</code>) is an internal IP, then external entities such as framework schedulers will be unable to reach the master. To address that scenario, an externally accessible IP:port can be setup via the <code>--advertise_ip</code> and <code>--advertise_port</code> command line arguments of <code>mesos-master</code>. If configured, external entities such as framework schedulers interact with the advertise_ip:advertise_port from where the request needs to be proxied to the internal IP:port on which the Mesos master is listening.</p>
<h2 id="http-requests-to-non-leading-master"><a class="header" href="#http-requests-to-non-leading-master">HTTP requests to non-leading master</a></h2>
<p>HTTP requests to some master endpoints (e.g., <a href="endpoints/master/state.html">/state</a>, <a href="endpoints/master/machine/down.html">/machine/down</a>) can only be answered by the leading master. Such requests made to a non-leading master will result in either a <code>307 Temporary Redirect</code> (with the location of the leading master) or <code>503 Service Unavailable</code> (if the master does not know who the current leader is).</p>
<h1 id="mesos-fetcher"><a class="header" href="#mesos-fetcher">Mesos Fetcher</a></h1>
<p>Mesos 0.23.0 introduced experimental support for the Mesos <em>fetcher cache</em>.</p>
<p>In this context we loosely regard the term &quot;downloading&quot; as to include copying
from local file systems.</p>
<h2 id="what-is-the-mesos-fetcher"><a class="header" href="#what-is-the-mesos-fetcher">What is the Mesos fetcher?</a></h2>
<p>The Mesos fetcher is a mechanism to download resources into the <a href="sandbox.html">sandbox
directory</a> of a task in preparation of running
the task. As part of a TaskInfo message, the framework ordering the task's
execution provides a list of <code>CommandInfo::URI</code> protobuf values, which becomes
the input to the Mesos fetcher.</p>
<p>The Mesos fetcher can copy files from a local filesytem and it also natively
supports the HTTP, HTTPS, FTP and FTPS protocols. If the requested URI is based
on some other protocol, then the fetcher tries to utilise a local Hadoop client
and hence supports any protocol supported by the Hadoop client, e.g., HDFS, S3.
See the agent <a href="configuration/agent.html">configuration documentation</a>
for how to configure the agent with a path to the Hadoop client.</p>
<p>By default, each requested URI is downloaded directly into the sandbox directory
and repeated requests for the same URI leads to downloading another copy of the
same resource. Alternatively, the fetcher can be instructed to cache URI
downloads in a dedicated directory for reuse by subsequent downloads.</p>
<p>The Mesos fetcher mechanism comprises of these two parts:</p>
<ol>
<li>
<p>The agent-internal Fetcher Process (in terms of libprocess) that controls and
coordinates all fetch actions. Every agent instance has exactly one internal
fetcher instance that is used by every kind of containerizer.</p>
</li>
<li>
<p>The external program <code>mesos-fetcher</code> that is invoked by the former. It
performs all network and disk operations except file deletions and file size
queries for cache-internal bookkeeping. It is run as an external OS process in
order to shield the agent process from I/O-related hazards. It takes
instructions in form of an environment variable containing a JSON object with
detailed fetch action descriptions.</p>
</li>
</ol>
<h2 id="the-fetch-procedure"><a class="header" href="#the-fetch-procedure">The fetch procedure</a></h2>
<p>Frameworks launch tasks by calling the scheduler driver method <code>launchTasks()</code>,
passing <code>CommandInfo</code> protobuf structures as arguments. This type of structure
specifies (among other things) a command and a list of URIs that need to be
&quot;fetched&quot; into the sandbox directory on the agent node as a precondition for
task execution. Hence, when the agent receives a request to launch a task, it
calls upon its fetcher, first, to provision the specified resources into the
sandbox directory. If fetching fails, the task is not started and the reported
task status is <code>TASK_FAILED</code>.</p>
<p>All URIs requested for a given task are fetched sequentially in a single
invocation of mesos-fetcher. Here, avoiding download concurrency reduces the
risk of bandwidth issues somewhat. However, multiple fetch operations can be
active concurrently due to multiple task launch requests.</p>
<h3 id="the-uri-protobuf-structure"><a class="header" href="#the-uri-protobuf-structure">The URI protobuf structure</a></h3>
<p>Before mesos-fetcher is started, the specific fetch actions to be performed for
each URI are determined based on the following protobuf structure. (See
<code>include/mesos/mesos.proto</code> for more details.)</p>
<pre><code>message CommandInfo {
  message URI {
    required string value = 1;
    optional bool executable = 2;
    optional bool extract = 3 [default = true];
    optional bool cache = 4;
    optional string output_file = 5;
  }
  ...
  optional string user = 5;
}
</code></pre>
<p>The field &quot;value&quot; contains the URI.</p>
<p>If the &quot;executable&quot; field is &quot;true&quot;, the &quot;extract&quot; field is ignored and
has no effect.</p>
<p>If the &quot;cache&quot; field is true, the fetcher cache is to be used for the URI.</p>
<p>If the &quot;output_file&quot; field is set, the fetcher will use that name for the copy
stored in the sandbox directory. &quot;output_file&quot; may contain a directory
component, in which case the path described must be a relative path.</p>
<h3 id="specifying-a-user-name"><a class="header" href="#specifying-a-user-name">Specifying a user name</a></h3>
<p>The framework may pass along a user name that becomes a fetch parameter. This
causes its executors and tasks to run under a specific user. However, if the
&quot;user&quot; field in the CommandInfo structure is specified, it takes precedence for
the affected task.</p>
<p>If a user name is specified either way, the fetcher first validates that it is
in fact a valid user name on the agent. If it is not, fetching fails right here.
Otherwise, the sandbox directory is assigned to the specified user as owner
(using <code>chown</code>) at the end of the fetch procedure, before task execution begins.</p>
<p>The user name in play has an important effect on caching.  Caching is managed on
a per-user base, i.e. the combination of user name and &quot;uri&quot; uniquely
identifies a cacheable fetch result. If no user name has been specified, this
counts for the cache as a separate user, too. Thus cache files for each valid
user are segregated from all others, including those without a specified user.</p>
<p>This means that the exact same URI will be downloaded and cached multiple times
if different users are indicated.</p>
<h3 id="executable-fetch-results"><a class="header" href="#executable-fetch-results">Executable fetch results</a></h3>
<p>By default, fetched files are not executable.</p>
<p>If the field &quot;executable&quot; is set to &quot;true&quot;, the fetch result will be changed to
be executable (by &quot;chmod&quot;) for every user. This happens at the end of the fetch
procedure, in the sandbox directory only. It does not affect any cache file.</p>
<h3 id="archive-extraction"><a class="header" href="#archive-extraction">Archive extraction</a></h3>
<p>If the &quot;extract&quot; field is &quot;true&quot;, which is the default, then files with
a recognized extension that hints at packed or compressed archives are unpacked
in the sandbox directory. These file extensions are recognized:</p>
<ul>
<li>.tar, .tar.gz, .tar.bz2, .tar.xz</li>
<li>.gz, .tgz, .tbz2, .txz, .zip</li>
</ul>
<p>In case the cache is bypassed, both the archive and the unpacked results will be
found together in the sandbox. In case a cache file is unpacked, only the
extraction result will be found in the sandbox.</p>
<p>The &quot;output_file&quot; field is useful here for cases where the URI ends with query
parameters, since these will otherwise end up in the file copied to the sandbox
and will subsequently fail to be recognized as archives.</p>
<h3 id="bypassing-the-cache"><a class="header" href="#bypassing-the-cache">Bypassing the cache</a></h3>
<p>By default, the URI field &quot;cache&quot; is not present. If this is the case or its
value is &quot;false&quot; the fetcher downloads directly into the sandbox directory.</p>
<p>The same also happens dynamically as a fallback strategy if anything goes wrong
when preparing a fetch operation that involves the cache. In this case, a
warning message is logged. Possible fallback conditions are:</p>
<ul>
<li>The server offering the URI does not respond or reports an error.</li>
<li>The URI's download size could not be determined.</li>
<li>There is not enough space in the cache, even after attempting to evict files.</li>
</ul>
<h3 id="fetching-through-the-cache"><a class="header" href="#fetching-through-the-cache">Fetching through the cache</a></h3>
<p>If the URI's &quot;cache&quot; field has the value &quot;true&quot;, then the fetcher cache is in
effect. If a URI is encountered for the first time (for the same user), it is
first downloaded into the cache, then copied to the sandbox directory from
there. If the same URI is encountered again, and a corresponding cache file is
resident in the cache or still en route into the cache, then downloading is
omitted and the fetcher proceeds directly to copying from the cache. Competing
requests for the same URI simply wait upon completion of the first request that
occurs. Thus every URI is downloaded at most once (per user) as long as it is
cached.</p>
<p>Every cache file stays resident for an unspecified amount of time and can be
removed at the fetcher's discretion at any moment, except while it is in direct
use:</p>
<ul>
<li>It is still being downloaded by this fetch procedure.</li>
<li>It is still being downloaded by a concurrent fetch procedure for a different
task.</li>
<li>It is being copied or extracted from the cache.</li>
</ul>
<p>Once a cache file has been removed, the related URI will thereafter be treated
as described above for the first encounter.</p>
<p>Unfortunately, there is no mechanism to refresh a cache entry in the current
experimental version of the fetcher cache. A future feature may force updates
based on checksum queries to the URI.</p>
<p>Recommended practice for now:</p>
<p>The framework should start using a fresh unique URI whenever the resource's
content has changed.</p>
<h3 id="determining-resource-sizes"><a class="header" href="#determining-resource-sizes">Determining resource sizes</a></h3>
<p>Before downloading a resource to the cache, the fetcher first determines the
size of the expected resource. It uses these methods depending on the nature of
the URI.</p>
<ul>
<li>Local file sizes are probed with systems calls (that follow symbolic links).</li>
<li>HTTP/HTTPS URIs are queried for the &quot;content-length&quot; field in the header. This
is performed by <code>curl</code>. The reported asset size must be greater than zero or
the URI is deemed invalid.</li>
<li>FTP/FTPS is not supported at the time of writing.</li>
<li>Everything else is queried by the local HDFS client.</li>
</ul>
<p>If any of this reports an error, the fetcher then falls back on bypassing the
cache as described above.</p>
<p>WARNING: Only URIs for which download sizes can be queried up front and for
which accurate sizes are reported reliably are eligible for any fetcher cache
involvement. If actual cache file sizes exceed the physical capacity of the
cache directory in any way, all further agent behavior is completely
unspecified. Do not use any cache feature with any URI for which you have any
doubts!</p>
<p>To mitigate this problem, cache files that have been found to be larger than
expected are deleted immediately after downloading and delivering the
requested content to the sandbox. Thus exceeding total capacity at least
does not accumulate over subsequent fetcher runs.</p>
<p>If you know for sure that size aberrations are within certain limits you can
specify a cache directory size that is sufficiently smaller than your actual
physical volume and fetching should work.</p>
<p>In case of cache files that are smaller then expected, the cache will
dynamically adjust its own bookkeeping according to actual sizes.</p>
<h3 id="cache-eviction"><a class="header" href="#cache-eviction">Cache eviction</a></h3>
<p>After determining the prospective size of a cache file and before downloading
it, the cache attempts to ensure that at least as much space as is needed for
this file is available and can be written into. If this is immediately the case,
the requested amount of space is simply marked as reserved. Otherwise, missing
space is freed up by &quot;cache eviction&quot;. This means that the cache removes files
at its own discretion until the given space target is met or exceeded.</p>
<p>The eviction process fails if too many files are in use and therefore not
evictable or if the cache is simply too small. Either way, the fetcher then
falls back on bypassing the cache for the given URI as described above.</p>
<p>If multiple evictions happen concurrently, each of them is pursuing its own
separate space goals. However, leftover freed up space from one effort is
automatically awarded to others.</p>
<h2 id="http-and-socks-proxy-settings"><a class="header" href="#http-and-socks-proxy-settings">HTTP and SOCKS proxy settings</a></h2>
<p>Sometimes it is desirable to use a proxy to download the file. The Mesos
fetcher uses libcurl internally for downloading content from
HTTP/HTTPS/FTP/FTPS servers, and libcurl can use a proxy automatically if
certain environment variables are set.</p>
<p>The respective environment variable name is <code>[protocol]_proxy</code>, where
<code>protocol</code> can be one of socks4, socks5, http, https.</p>
<p>For example, the value of the <code>http_proxy</code> environment variable would be used
as the proxy for fetching http contents, while <code>https_proxy</code> would be used for
fetching https contents. Pay attention that these variable names must be
entirely in lower case.</p>
<p>The value of the proxy variable is of the format
<code>[protocol://][user:password@]machine[:port]</code>, where <code>protocol</code> can be one of
socks4, socks5, http, https.</p>
<p>FTP/FTPS requests with a proxy also make use of an HTTP/HTTPS proxy. Even
though in general this constrains the available FTP protocol operations,
everything the fetcher uses is supported.</p>
<p>Your proxy settings can be placed in <code>/etc/default/mesos-slave</code>. Here is an
example:</p>
<pre><code>export http_proxy=https://proxy.example.com:3128
export https_proxy=https://proxy.example.com:3128
</code></pre>
<p>The fetcher will pick up these environment variable settings since the utility
program <code>mesos-fetcher</code> which it employs is a child of mesos-agent.</p>
<p>For more details, please check the
<a href="http://curl.haxx.se/libcurl/c/libcurl-tutorial.html">libcurl manual</a>.</p>
<h2 id="agent-flags"><a class="header" href="#agent-flags">Agent flags</a></h2>
<p>It is highly recommended to set these flags explicitly to values other than
their defaults or to not use the fetcher cache in production.</p>
<ul>
<li>&quot;fetcher_cache_size&quot;, default value: enough for testing.</li>
<li>&quot;fetcher_cache_dir&quot;, default value: somewhere inside the directory specified
by the &quot;work_dir&quot; flag, which is OK for testing.</li>
</ul>
<p>Recommended practice:</p>
<ul>
<li>Use a separate volume as fetcher cache. Do not specify a directory as fetcher
cache directory that competes with any other contributor for the underlying
volume's space.</li>
<li>Set the cache directory size flag of the agent to less than your actual cache
volume's physical size. Use a safety margin, especially if you do not know
for sure if all frameworks are going to be compliant.</li>
</ul>
<p>Ultimate remedy:</p>
<p>You can disable the fetcher cache entirely on each agent by setting its
&quot;fetcher_cache_size&quot; flag to zero bytes.</p>
<h2 id="future-features"><a class="header" href="#future-features">Future Features</a></h2>
<p>The following features would be relatively easy to implement additionally.</p>
<ul>
<li>Perform cache updates based on resource check sums. For example, query the md5
field in HTTP headers to determine when a resource at a URL has changed.</li>
<li>Respect HTTP cache-control directives.</li>
<li>Enable caching for ftp/ftps.</li>
<li>Use symbolic links or bind mounts to project cached resources into the
sandbox, read-only.</li>
<li>Have a choice whether to copy the extracted archive into the sandbox.</li>
<li>Have a choice whether to delete the archive after extraction bypassing the
cache.</li>
<li>Make the segregation of cache files by user optional.</li>
<li>Extract content while downloading when bypassing the cache.</li>
<li>Prefetch resources for subsequent tasks. This can happen concurrently with
running the present task, right after fetching its own resources.</li>
</ul>
<h2 id="implementation-details-2"><a class="header" href="#implementation-details-2">Implementation Details</a></h2>
<p>The <a href="fetcher-cache-internals.html">Mesos Fetcher Cache Internals</a> describes how the fetcher cache is implemented.</p>
<hr />
<h2>title: Apache Mesos - Domains and Regions
layout: documentation</h2>
<h1 id="regions-and-fault-domains"><a class="header" href="#regions-and-fault-domains">Regions and Fault Domains</a></h1>
<p>Starting with Mesos 1.5, it is possible to place Mesos masters and agents into
<em>domains</em>, which are logical groups of machines that share some characteristics.</p>
<p>Currently, fault domains are the only supported type of domains, which are
groups of machines with similar failure characteristics.</p>
<p>A fault domain is a 2 level hierarchy of regions and zones. The mapping from
fault domains to physical infrastructure is up to the operator to configure,
although it is recommended that machines in the same zones have low latency to
each other.</p>
<p>In cloud environments, regions and zones can be mapped to the &quot;region&quot; and
&quot;availability zone&quot; concepts exposed by most cloud providers, respectively.
In on-premise deployments, regions and zones can be mapped to data centers and
racks, respectively.</p>
<p>Schedulers may prefer to place network-intensive workloads in the same domain,
as this may improve performance. Conversely, a single failure that affects a
host in a domain may be more likely to affect other hosts in the same domain;
hence, schedulers may prefer to place workloads that require high availability
in multiple domains. For example, all the hosts in a single rack might lose
power or network connectivity simultaneously.</p>
<p>The <code>--domain</code> flag can be used to specify the fault domain of a master or
agent node. The value of this flag must be a file path or a JSON dictionary
with the key <code>fault_domain</code> and subkeys <code>region</code> and <code>zone</code> mapping to
arbitrary strings:</p>
<pre><code>mesos-master --domain='{&quot;fault_domain&quot;: {&quot;region&quot;: {&quot;name&quot;:&quot;eu&quot;}, &quot;zone&quot;: { &quot;name&quot;:&quot;rack1&quot;}}}'

mesos-agent  --domain='{&quot;fault_domain&quot;: {&quot;region&quot;: {&quot;name&quot;:&quot;eu&quot;}, &quot;zone&quot;: {&quot;name&quot;:&quot;rack2&quot;}}}'
</code></pre>
<p>Frameworks can learn about the domain of an agent by inspecting the <code>domain</code>
field in the received offer, which contains a <code>DomainInfo</code> that has the
same structure as the JSON dictionary above.</p>
<h1 id="constraints"><a class="header" href="#constraints">Constraints</a></h1>
<p>When configuring fault domains for the masters and agents, the following
constraints must be obeyed:</p>
<ul>
<li>
<p>If a mesos master is not configured with a domain, it will reject connection
attempts from agents with a domain.</p>
<p>This is done because the master is not able to determine whether or not the
agent would be remote in this case.</p>
</li>
<li>
<p>Agents with no configured domain are assumed to be in the same domain as the
master.</p>
<p>If this behaviour isn't desired, the <code>--require_agent_domain</code> flag on the
master can be used to enforce that domains are configured on all agents by
having the master reject all registration attempts by agents without a
configured domain.</p>
</li>
<li>
<p>If one master is configured with a domain, all other masters must be in the
same &quot;region&quot; to avoid cross-region quorum writes. It is recommended to put
them in different zones within that region for high availability.</p>
</li>
<li>
<p>The default DRF resource allocator will only offer resources from agents in
the same region as the master. To receive offers from all regions, a
framework must set the <code>REGION_AWARE</code> capability bit in its FrameworkInfo.</p>
</li>
</ul>
<h1 id="example"><a class="header" href="#example">Example</a></h1>
<p>A short example will serve to illustrate these concepts. WayForward Technologies
runs a successful website that allows users to purchase things that they want
to have.</p>
<p>To do this, it owns a data center in San Francisco, in which it runs a number of
custom Mesos frameworks. All agents within the data center are configured with
the same region <code>sf</code>, and the individual racks inside the data center are used
as zones.</p>
<p>The three mesos masters are placed in different server racks in the data center,
which gives them enough isolation to withstand events like a whole rack losing
power or network connectivity but still have low-enough latency for
quorum writes.</p>
<p>One of the provided services is a real-time view of the company's inventory.
The framework providing this service is placing all of its tasks in the same
zone as the database server, to take advantage of the high-speed, low-latency
link so it can always display the latest results.</p>
<p>During peak hours, it might happen that the computing power required to operate
the website exceeds the capacity of the data center. To avoid unnecessary
hardware purchases, WayForward Technologies contracted with a third-party cloud
provider TPC. The machines from this provider are placed in a different
region <code>tpc</code>, and the zones are configured to correspond to the availability
zones provided by TPC. All relevant frameworks are updated with the
<code>REGION_AWARE</code> bit in their <code>FrameworkInfo</code> and their scheduling logic is
updated so that they can schedule tasks in the cloud if required.</p>
<p>Non-region aware frameworks will now only receive offers from agents within
the data center, where the master nodes reside. Region-aware frameworks are
supposed to know when and if they should place their tasks in the data center
or with the cloud provider.</p>
<h1 id="performance-profiling"><a class="header" href="#performance-profiling">Performance Profiling</a></h1>
<p>This document over time will be home to various guides on how to use various profiling tools to do performance analysis of Mesos.</p>
<h2 id="flamescope"><a class="header" href="#flamescope">Flamescope</a></h2>
<p><a href="https://github.com/Netflix/flamescope">Flamescope</a> is a visualization tool for exploring different time ranges as <a href="https://github.com/brendangregg/FlameGraph">flamegraphs</a>. In order to use the tool, you first need to obtain stack traces, here's how to obtain a 60 second recording of the mesos master process at 100 hertz using Linux perf:</p>
<pre><code>$ sudo perf record --freq=100 --no-inherit --call-graph dwarf -p &lt;mesos-master-pid&gt; -- sleep 60
$ sudo perf script --header | c++filt &gt; mesos-master.stacks
$ gzip mesos-master.stacks
</code></pre>
<p>If you'd like to solicit help in analyzing the performance data, upload the <code>mesos-master.stacks.gz</code> to a publicly accessible location and file with <code>dev@mesos.apache.org</code> for analysis, or send the file over <a href="http://mesos.slack.com">slack</a> to the #performance channel.</p>
<p>Alternatively, to do the analysis yourself, place mesos-master.stacks into the <code>examples</code> folder of a flamescope git checkout.</p>
<h1 id="memory-profiling-with-mesos-and-jemalloc"><a class="header" href="#memory-profiling-with-mesos-and-jemalloc">Memory Profiling with Mesos and Jemalloc</a></h1>
<p>On Linux systems, Mesos is able to leverage the memory-profiling capabilities of
the <a href="http://jemalloc.net">jemalloc</a> general-purpose allocator to provide
powerful debugging tools for investigating memory-related issues.</p>
<p>These include detailed real-time statistics of the current memory usage, as well
as information about the location and frequency of individual allocations.</p>
<p>This generally works by having libprocess detect at runtime whether the current
process is using jemalloc as its memory allocator, and if so enable a number of
HTTP endpoints described below that allow operators to generate the desired data
at runtime.</p>
<p><a name="requirements"></a></p>
<h2 id="requirements"><a class="header" href="#requirements">Requirements</a></h2>
<p>A prerequisite for memory profiling is a suitable allocator. Currently only
jemalloc is supported, which can be connected via one of the following ways.</p>
<p>The recommended method is to specify the <code>--enable-jemalloc-allocator</code>
compile-time flag, which causes the <code>mesos-master</code> and <code>mesos-agent</code> binaries
to be statically linked against a bundled version of jemalloc that will be
compiled with the correct compile-time flags.</p>
<p>Alternatively and analogous to other bundled dependencies of Mesos, it is of
course also possible to use a <em>suitable</em> custom version of jemalloc with the
<code>--with-jemalloc=&lt;/path-to-jemalloc&gt;</code> flag.</p>
<p><strong>NOTE:</strong> Suitable here means that jemalloc should have been built with the
<code>--enable-stats</code> and <code>--enable-prof</code> flags, and that the string
<code>prof:true;prof_active:false</code> is part of the malloc configuration. The latter
condition can be satisfied either at configuration or at run-time, see the
section on <code>MALLOC_CONF</code> below.</p>
<p>The third way is to use the <code>LD_PRELOAD</code> mechanism to preload a <code>libjemalloc.so</code>
shared library that is present on the system at runtime. The <code>MemoryProfiler</code>
class in libprocess will automatically detect this and enable its memory
profiling support.</p>
<p>The generated profile dumps will be written to a random directory under <code>TMPDIR</code>
if set, otherwise in a subdirectory of <code>/tmp</code>.</p>
<p>Finally, note that since jemalloc was designed to be used in highly concurrent
allocation scenarios, it can improve performance over the default system
allocator. In this case, it can be beneficial to build Mesos with jemalloc even
if there is no intention to use the memory profiling functionality.</p>
<h2 id="usage-1"><a class="header" href="#usage-1">Usage</a></h2>
<p>There are two independent sets of data that can be collected from jemalloc:
memory statistics and heap profiling information.</p>
<p>Using any of the endpoints described below
<a href="memory-profiling.html#requirements">requires the jemalloc allocator</a> and starting the <code>mesos-agent</code>
or <code>mesos-master</code> binary with the option <code>--memory_profiling=true</code> (or setting
the environment variable <code>LIBPROCESS_MEMORY_PROFILING=true</code> for other binaries
using libprocess).</p>
<h3 id="memory-statistics"><a class="header" href="#memory-statistics">Memory Statistics</a></h3>
<p>The <code>/statistics</code> endpoint returns exact statistics about the memory usage in
JSON format, for example the number of bytes currently allocated and the size
distribution of these allocations.</p>
<p>It takes no parameters and will return the results in JSON format:</p>
<pre><code>http://example.org:5050/memory-profiler/statistics
</code></pre>
<p>Be aware that the returned JSON is quite large, so when accessing this endpoint
from a terminal, it is advisable to redirect the results into a file.</p>
<h3 id="heap-profiling"><a class="header" href="#heap-profiling">Heap Profiling</a></h3>
<p>The profiling done by jemalloc works by sampling from the calls to <code>malloc()</code>
according to a configured probability distribution, and storing stack traces for
the sampled calls in a separate memory area. These can then be dumped into files
on the filesystem, so-called heap profiles.</p>
<p>To start a profiling run one would access the <code>/start</code> endpoint:</p>
<pre><code>http://example.org:5050/memory-profiler/start?duration=5mins
</code></pre>
<p>followed by downloading one of the generated files described below after the
duration has elapsed. The remaining time of the current profiling run can be
verified via the <code>/state</code> endpoint:</p>
<pre><code>http://example.org:5050/memory-profiler/state
</code></pre>
<p>Since profiling information is stored process-global by jemalloc, only a single
concurrent profiling run is allowed. Additionally, only the results of the most
recently finished run are stored on disk.</p>
<p>The profile collection can also be stopped early with the <code>/stop</code> endpoint:</p>
<pre><code>http://example.org:5050/memory-profiler/stop
</code></pre>
<p>To analyze the generated profiling data, the results are offered in three
different formats.</p>
<h4 id="raw-profile"><a class="header" href="#raw-profile">Raw profile</a></h4>
<pre><code>http://example.org:5050/memory-profiler/download/raw
</code></pre>
<p>This returns a file in a plain text format containing the raw backtraces
collected, i.e., lists of memory addresses. It can be interactively analyzed
and rendered using the <code>jeprof</code> tool provided by the jemalloc project. For more
information on this file format, check out <a href="http://jemalloc.net/jemalloc.3.html#heap_profile_format">the official jemalloc
documentation</a>.</p>
<h4 id="symbolized-profile"><a class="header" href="#symbolized-profile">Symbolized profile</a></h4>
<pre><code>http://example.org:5050/memory-profiler/download/text
</code></pre>
<p>This is similar to the raw format above, except that <code>jeprof</code> is called on the
host machine to attempt to read symbol information from the current binary and
replace raw memory addresses in the profile by human-readable symbol names.</p>
<p>Usage of this endpoint requires that <code>jeprof</code> is present on the host machine
and on the <code>PATH</code>, and no useful information will be generated unless the binary
contains symbol information.</p>
<h4 id="call-graph"><a class="header" href="#call-graph">Call graph</a></h4>
<pre><code>http://example.org:5050/memory-profiler/download/graph
</code></pre>
<p>This endpoint returns an image in SVG format that shows a graphical
representation of the samples backtraces.</p>
<p>Usage of this endpoint requires that <code>jeprof</code> and <code>dot</code> are present on the host
machine and on the <code>PATH</code> of mesos, and no useful information will be generated
unless the binary contains symbol information.</p>
<h4 id="overview-2"><a class="header" href="#overview-2">Overview</a></h4>
<p>Which of these is needed will depend on the circumstances of the application
deployment and of the bug that is investigated.</p>
<p>For example, the call graph presents information in a visual, immediately useful
form, but is difficult to filter and post-process if non-default output options
are desired.</p>
<p>On the other hand, in many debian-like environments symbol information is by
default stripped from binaries to save space and shipped in separate packages.
In such an environment, if it is not permitted to install additional packages on
the host running Mesos, one would store the raw profiles and enrich them with
symbol information locally.</p>
<h2 id="jeprof-installation"><a class="header" href="#jeprof-installation">Jeprof Installation</a></h2>
<p>As described above, the <code>/download/text</code> and <code>/download/graph</code> endpoints require
the <code>jeprof</code> program installed on the host system. Where possible, it is
recommended to install <code>jeprof</code> through the system package manager, where it is
usually packaged alongside with jemalloc itself.</p>
<p>Alternatively, a copy of the script can be found under
<code>3rdparty/jemalloc-5.0.1/bin/jeprof</code> in the build directory, or can be
downloaded directly from the internet using a command like:</p>
<pre><code>$ curl https://raw.githubusercontent.com/jemalloc/jemalloc/dev/bin/jeprof.in | sed s/@jemalloc_version@/5.0.1/ &gt;jeprof
</code></pre>
<p>Note that <code>jeprof</code> is just a perl script that post-processes the raw profiles.
It has no connection to the jemalloc library besides being distributed in the
same package. In particular, it is generally not required to have matching
versions of jemalloc and <code>jeprof</code>.</p>
<p>If <code>jeprof</code> is installed manually, one also needs to take care to install the
necessary dependencies. In particular, this include the <code>perl</code> interpreter to
execute the script itself and the <code>dot</code> binary to generate graph files.</p>
<h2 id="command-line-usage"><a class="header" href="#command-line-usage">Command-line Usage</a></h2>
<p>In some circumstances, it might be desired to automate the downloading of heap
profiles by writing a simple script. A simple example for how this might look
like this:</p>
<pre><code>#!/bin/bash

SECONDS=600
HOST=example.org:5050

curl ${HOST}/memory-profiler/start?duration=${SECONDS}
sleep $((${SECONDS} + 1))
wget ${HOST}/memory-profiler/download/raw
</code></pre>
<p>A more sophisticated script would additionally store the <code>id</code> value returned by
the call to <code>/start</code> and pass it as a paremter to <code>/download</code>, to ensure that a
new run was not started in the meantime.</p>
<h2 id="using-the-malloc_conf-interface"><a class="header" href="#using-the-malloc_conf-interface">Using the <code>MALLOC_CONF</code> Interface</a></h2>
<p>The jemalloc allocator provides a native interface to control the memory
profiling behaviour. The usual way to provide settings through this interface is
by setting the environment variable <code>MALLOC_CONF</code>.</p>
<p><strong>NOTE:</strong> If libprocess detects that memory profiling was started through
<code>MALLOC_CONF</code>, it will reject starting a profiling run of its own to avoid
interference.</p>
<p>The <code>MALLOC_CONF</code> interface provides a number of options that are not exposed by
libprocess, like generating heap profiles automatically after a certain amount
of memory has been allocated, or whenever memory usage reaches a new high-water
mark. The full list of settings is described on the
<a href="http://jemalloc.net/jemalloc.3.html">jemalloc man page</a>.</p>
<p>On the other hand, features like starting and stopping the profiling at runtime
or getting the information provided by the <code>/statistics</code> endpoint can not be
achieved through the <code>MALLOC_CONF</code> interface.</p>
<p>For example, to create a dump automatically for every 1 GiB worth of recorded
allocations, one might use the configuration:</p>
<pre><code>MALLOC_CONF=&quot;prof:true,prof_prefix:/path/to/folder,lg_prof_interval=20&quot;
</code></pre>
<p>To debug memory allocations during early startup, profiling can be activated
before accessing the <code>/start</code> endpoint:</p>
<pre><code>MALLOC_CONF=&quot;prof:true,prof_active:true&quot;
</code></pre>
<h1 id="mesos-attributes--resources"><a class="header" href="#mesos-attributes--resources">Mesos Attributes &amp; Resources</a></h1>
<p>Mesos has two basic methods to describe the agents that comprise a cluster. One of these is managed by the Mesos master, the other is simply passed onwards to the frameworks using the cluster.</p>
<h2 id="types"><a class="header" href="#types">Types</a></h2>
<p>The types of values that are supported by Attributes and Resources in Mesos are scalar, ranges, sets and text.</p>
<p>The following are the definitions of these types:</p>
<pre><code>scalar : floatValue

floatValue : ( intValue ( &quot;.&quot; intValue )? ) | ...

intValue : [0-9]+

range : &quot;[&quot; rangeValue ( &quot;,&quot; rangeValue )* &quot;]&quot;

rangeValue : scalar &quot;-&quot; scalar

set : &quot;{&quot; text ( &quot;,&quot; text )* &quot;}&quot;

text : [a-zA-Z0-9_/.-]
</code></pre>
<h2 id="attributes"><a class="header" href="#attributes">Attributes</a></h2>
<p>Attributes are key-value pairs (where value is optional) that Mesos passes along when it sends offers to frameworks. An attribute value supports three different <em>types</em>: scalar, range or text.</p>
<pre><code>attributes : attribute ( &quot;;&quot; attribute )*

attribute : text &quot;:&quot; ( scalar | range | text )
</code></pre>
<p>Note that setting multiple attributes corresponding to the same key is highly
discouraged (and might be disallowed in future), as this complicates attribute-
based filtering of offers, both on schedulers side and on the Mesos side.</p>
<h2 id="resources-2"><a class="header" href="#resources-2">Resources</a></h2>
<p>Mesos can manage three different <em>types</em> of resources: scalars, ranges, and sets.  These are used to represent the different resources that a Mesos agent has to offer.  For example, a scalar resource type could be used to represent the amount of memory on an agent. Scalar resources are represented using floating point numbers to allow fractional values to be specified (e.g., &quot;1.5 CPUs&quot;). Mesos only supports three decimal digits of precision for scalar resources (e.g., reserving &quot;1.5123 CPUs&quot; is considered equivalent to reserving &quot;1.512 CPUs&quot;). For GPUs, Mesos only supports whole number values.</p>
<p>Resources can be specified either with a JSON array or a semicolon-delimited string of key-value pairs.  If, after examining the examples below, you have questions about the format of the JSON, inspect the <code>Resource</code> protobuf message definition in <code>include/mesos/mesos.proto</code>.</p>
<p>As JSON:</p>
<pre><code>[
  {
    &quot;name&quot;: &quot;&lt;resource_name&gt;&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: &lt;resource_value&gt;
    }
  },
  {
    &quot;name&quot;: &quot;&lt;resource_name&gt;&quot;,
    &quot;type&quot;: &quot;RANGES&quot;,
    &quot;ranges&quot;: {
      &quot;range&quot;: [
        {
          &quot;begin&quot;: &lt;range_beginning&gt;,
          &quot;end&quot;: &lt;range_ending&gt;
        },
        ...
      ]
    }
  },
  {
    &quot;name&quot;: &quot;&lt;resource_name&gt;&quot;,
    &quot;type&quot;: &quot;SET&quot;,
    &quot;set&quot;: {
      &quot;item&quot;: [
        &quot;&lt;first_item&gt;&quot;,
        ...
      ]
    },
    &quot;role&quot;: &quot;&lt;role_name&gt;&quot;
  },
  ...
]
</code></pre>
<p>As a list of key-value pairs:</p>
<pre><code>resources : resource ( &quot;;&quot; resource )*

resource : key &quot;:&quot; ( scalar | range | set )

key : text ( &quot;(&quot; resourceRole &quot;)&quot; )?

resourceRole : text | &quot;*&quot;
</code></pre>
<p>Note that <code>resourceRole</code> must be a valid role name; see the <a href="roles.html">roles</a> documentation for details.</p>
<h2 id="predefined-uses--conventions"><a class="header" href="#predefined-uses--conventions">Predefined Uses &amp; Conventions</a></h2>
<p>There are several kinds of resources that have predefined behavior:</p>
<ul>
<li><code>cpus</code></li>
<li><code>gpus</code></li>
<li><code>disk</code></li>
<li><code>mem</code></li>
<li><code>ports</code></li>
</ul>
<p>Note that <code>disk</code> and <code>mem</code> resources are specified in megabytes. The master's user interface will convert resource values into a more human-readable format: for example, the value <code>15000</code> will be displayed as <code>14.65GB</code>.</p>
<p>An agent without <code>cpus</code> and <code>mem</code> resources will not have its resources advertised to any frameworks.</p>
<h2 id="examples-2"><a class="header" href="#examples-2">Examples</a></h2>
<p>By default, Mesos will try to autodetect the resources available at the local machine when <code>mesos-agent</code> starts up. Alternatively, you can explicitly configure which resources an agent should make available.</p>
<p>Here are some examples of how to configure the resources at a Mesos agent:</p>
<pre><code>--resources='cpus:24;gpus:2;mem:24576;disk:409600;ports:[21000-24000,30000-34000];bugs(debug_role):{a,b,c}'

--resources='[{&quot;name&quot;:&quot;cpus&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:24}},{&quot;name&quot;:&quot;gpus&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:2}},{&quot;name&quot;:&quot;mem&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:24576}},{&quot;name&quot;:&quot;disk&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:409600}},{&quot;name&quot;:&quot;ports&quot;,&quot;type&quot;:&quot;RANGES&quot;,&quot;ranges&quot;:{&quot;range&quot;:[{&quot;begin&quot;:21000,&quot;end&quot;:24000},{&quot;begin&quot;:30000,&quot;end&quot;:34000}]}},{&quot;name&quot;:&quot;bugs&quot;,&quot;type&quot;:&quot;SET&quot;,&quot;set&quot;:{&quot;item&quot;:[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]},&quot;role&quot;:&quot;debug_role&quot;}]'
</code></pre>
<p>Or given a file <code>resources.txt</code> containing the following:</p>
<pre><code>[
  {
    &quot;name&quot;: &quot;cpus&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 24
    }
  },
  {
    &quot;name&quot;: &quot;gpus&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 2
    }
  },
  {
    &quot;name&quot;: &quot;mem&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 24576
    }
  },
  {
    &quot;name&quot;: &quot;disk&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 409600
    }
  },
  {
    &quot;name&quot;: &quot;ports&quot;,
    &quot;type&quot;: &quot;RANGES&quot;,
    &quot;ranges&quot;: {
      &quot;range&quot;: [
        {
          &quot;begin&quot;: 21000,
          &quot;end&quot;: 24000
        },
        {
          &quot;begin&quot;: 30000,
          &quot;end&quot;: 34000
        }
      ]
    }
  },
  {
    &quot;name&quot;: &quot;bugs&quot;,
    &quot;type&quot;: &quot;SET&quot;,
    &quot;set&quot;: {
      &quot;item&quot;: [
        &quot;a&quot;,
        &quot;b&quot;,
        &quot;c&quot;
      ]
    },
    &quot;role&quot;: &quot;debug_role&quot;
  }
]
</code></pre>
<p>You can do:</p>
<pre><code>$ path/to/mesos-agent --resources=file:///path/to/resources.txt ...
</code></pre>
<p>In this case, we have five resources of three different types: scalars, a range, and a set.  There are scalars called <code>cpus</code>, <code>gpus</code>, <code>mem</code> and <code>disk</code>, a range called <code>ports</code>, and a set called <code>bugs</code>. <code>bugs</code> is assigned to the role <code>debug_role</code>, while the other resources do not specify a role and are thus assigned to the default role.</p>
<p>Note: the &quot;default role&quot; can be set by the <code>--default_role</code> flag.</p>
<ul>
<li>scalar called <code>cpus</code>, with the value <code>24</code></li>
<li>scalar called <code>gpus</code>, with the value <code>2</code></li>
<li>scalar called <code>mem</code>, with the value <code>24576</code></li>
<li>scalar called <code>disk</code>, with the value <code>409600</code></li>
<li>range called <code>ports</code>, with values <code>21000</code> through <code>24000</code> and <code>30000</code> through <code>34000</code> (inclusive)</li>
<li>set called <code>bugs</code>, with the values <code>a</code>, <code>b</code> and <code>c</code>, assigned to the role <code>debug_role</code></li>
</ul>
<p>To configure the attributes of a Mesos agent, you can use the <code>--attributes</code> command-line flag of <code>mesos-agent</code>:</p>
<pre><code>--attributes='rack:abc;zone:west;os:centos5;level:10;keys:[1000-1500]'
</code></pre>
<p>That will result in configuring the following five attributes:</p>
<ul>
<li><code>rack</code> with text value <code>abc</code></li>
<li><code>zone</code> with text value <code>west</code></li>
<li><code>os</code> with text value <code>centos5</code></li>
<li><code>level</code> with scalar value 10</li>
<li><code>keys</code> with range value <code>1000</code> through <code>1500</code> (inclusive)</li>
</ul>
<hr />
<h2>title: Apache Mesos - Roles
layout: documentation</h2>
<h1 id="roles"><a class="header" href="#roles">Roles</a></h1>
<p>Many modern host-level operating systems (e.g. Linux, BSDs, etc) support
multiple users. Similarly, Mesos is a multi-user cluster management system,
with the expectation of a single Mesos cluster managing an organization's
resources and servicing the organization's users.</p>
<p>As such, Mesos has to address a number of requirements related to resource
management:</p>
<ul>
<li>Fair sharing of the resources amongst users</li>
<li>Providing resource guarantees to users (e.g. quota, priorities, isolation)</li>
<li>Providing accurate resource accounting
<ul>
<li>How many resources are allocated / utilized / etc?</li>
<li>Per-user accounting</li>
</ul>
</li>
</ul>
<p>In Mesos, we refer to these &quot;users&quot; as <strong>roles</strong>. More precisely, a <strong>role</strong>
within Mesos refers to a resource consumer within the cluster. This resource
consumer could represent a user within an organization, but it could also
represent a team, a group, a service, a framework, etc.</p>
<p>Schedulers subscribe to one or more roles in order to receive resources and
schedule work on behalf of the resource consumer(s) they are servicing.</p>
<p>Some examples of resource allocation guarantees that Mesos provides:</p>
<ul>
<li>Guaranteeing that a role is allocated a specified amount of resources
(via <a href="quota.html">quota</a>).</li>
<li>Ensuring that some (or all) of the resources on a particular agent
are allocated to a particular role (via <a href="reservation.html">reservations</a>).</li>
<li>Ensuring that resources are fairly shared between roles
(via <a href="https://www.cs.berkeley.edu/%7Ealig/papers/drf.pdf">DRF</a>).</li>
<li>Expressing that some roles should receive a higher relative share of the
cluster (via <a href="weights.html">weights</a>).</li>
</ul>
<h2 id="roles-and-access-control"><a class="header" href="#roles-and-access-control">Roles and access control</a></h2>
<p>There are two ways to control which roles a framework is allowed to subscribe
to. First, ACLs can be used to specify which framework principals can subscribe
to which roles. For more information, see the <a href="authorization.html">authorization</a>
documentation.</p>
<p>Second, a <em>role whitelist</em> can be configured by passing the <code>--roles</code> flag to
the Mesos master at startup. This flag specifies a comma-separated list of role
names. If the whitelist is specified, only roles that appear in the whitelist
can be used. To change the whitelist, the Mesos master must be restarted. Note
that in a high-availability deployment of Mesos, you should take care to ensure
that all Mesos masters are configured with the same whitelist.</p>
<p>In Mesos 0.26 and earlier, you should typically configure <em>both</em> ACLs and the
whitelist, because in these versions of Mesos, any role that does not appear in
the whitelist cannot be used.</p>
<p>In Mesos 0.27, this behavior has changed: if <code>--roles</code> is not specified, the
whitelist permits <em>any role name</em> to be used. Hence, in Mesos 0.27, the
recommended practice is to only use ACLs to define which roles can be used; the
<code>--roles</code> command-line flag is deprecated.</p>
<h2 id="associating-frameworks-with-roles"><a class="header" href="#associating-frameworks-with-roles">Associating frameworks with roles</a></h2>
<p>A framework specifies which roles it would like to subscribe to when it
subscribes with the master. This is done via the <code>roles</code> field in
<code>FrameworkInfo</code>. A framework can also change which roles it is
subscribed to by reregistering with an updated <code>FrameworkInfo</code>.</p>
<p>As a user, you can typically specify which role(s) a framework will
subscribe to when you start the framework. How to do this depends on the
user interface of the framework you're using. For example, a single user
scheduler might take a <code>--mesos_role</code> command-line flag and a multi-user
scheduler might take a <code>--mesos-roles</code> command-line flag or sync with
the organization's LDAP system to automatically adjust which roles it is
subscribed to as the organization's structure changes.</p>
<h3 id="subscribing-to-multiple-roles"><a class="header" href="#subscribing-to-multiple-roles">Subscribing to multiple roles</a></h3>
<p>As noted above, a framework can subscribe to multiple roles
simultaneously. Frameworks that want to do this must opt-in to the
<code>MULTI_ROLE</code> capability.</p>
<p>When a framework is offered resources, those resources are associated
with exactly <em>one</em> of the roles it has subscribed to; the framework can
determine which role an offer is for by consulting the
<code>allocation_info.role</code> field in the <code>Offer</code> or the
<code>allocation_info.role</code> field in each offered <code>Resource</code> (in the current
implementation, all the resources in a single <code>Offer</code> will be allocated
to the same role).</p>
<p><a id="roles-multiple-frameworks"></a></p>
<h3 id="multiple-frameworks-in-the-same-role"><a class="header" href="#multiple-frameworks-in-the-same-role">Multiple frameworks in the same role</a></h3>
<p>Multiple frameworks can be subscribed to the same role. This can be useful:
for example, one framework can create a persistent volume and write data to
it. Once the task that writes data to the persistent volume has finished,
the volume will be offered to other frameworks subscribed to the same role;
this might give a second (&quot;consumer&quot;) framework the opportunity to launch a
task that reads the data produced by the first (&quot;producer&quot;) framework.</p>
<p>However, configuring multiple frameworks to use the same role should be done
with caution, because all the frameworks will have access to any resources that
have been reserved for that role. For example, if a framework stores sensitive
information on a persistent volume, that volume might be offered to a different
framework subscribed to the same role. Similarly, if one framework creates a
persistent volume, another framework subscribed to the same role might &quot;steal&quot;
the volume and use it to launch a task of its own. In general, multiple
frameworks sharing the same role should be prepared to collaborate with one
another to ensure that role-specific resources are used appropriately.</p>
<h2 id="associating-resources-with-roles"><a class="header" href="#associating-resources-with-roles">Associating resources with roles</a></h2>
<p>A resource is assigned to a role using a <em>reservation</em>. Resources can either be
reserved <em>statically</em> (when the agent that hosts the resource is started) or
<em>dynamically</em>: frameworks and operators can specify that a certain resource
should subsequently be reserved for use by a given role. For more information,
see the <a href="reservation.html">reservation</a> documentation.</p>
<h2 id="default-role"><a class="header" href="#default-role">Default role</a></h2>
<p>The role named <code>*</code> is special. Unreserved resources are currently represented
as having the special <code>*</code> role (the idea being that <code>*</code> matches any role). By
default, all the resources at an agent node are unreserved (this can be changed
via the <code>--default_role</code> command-line flag when starting the agent).</p>
<p>In addition, when a framework registers without providing a
<code>FrameworkInfo.role</code>, it is assigned to the <code>*</code> role. In Mesos 1.3, frameworks
should use the <code>FrameworkInfo.roles</code> field, which does not assign a default of
<code>*</code>, but frameworks can still specify <code>*</code> explicitly if desired. Frameworks
and operators cannot make reservations to the <code>*</code> role.</p>
<h2 id="invalid-role-names"><a class="header" href="#invalid-role-names">Invalid role names</a></h2>
<p>A role name must be a valid directory name, so it cannot:</p>
<ul>
<li>Be an empty string</li>
<li>Be <code>.</code> or <code>..</code></li>
<li>Start with <code>-</code></li>
<li>Contain any slash, backspace, or whitespace character</li>
</ul>
<h2 id="roles-and-resource-allocation"><a class="header" href="#roles-and-resource-allocation">Roles and resource allocation</a></h2>
<p>By default, the Mesos master uses weighted Dominant Resource Fairness (wDRF) to
allocate resources. In particular, this implementation of wDRF first identifies
which <em>role</em> is furthest below its fair share of the role's dominant resource.
Each of the frameworks subscribed to that role are then offered additional
resources in turn.</p>
<p>The resource allocation process can be customized by assigning
<em><a href="weights.html">weights</a></em> to roles: a role with a weight of 2 will be allocated
twice the fair share of a role with a weight of 1. By default, every role has a
weight of 1. Weights can be configured using the
<a href="endpoints/master/weights.html">/weights</a> operator endpoint, or else using the
deprecated <code>--weights</code> command-line flag when starting the Mesos master.</p>
<h2 id="roles-and-quota"><a class="header" href="#roles-and-quota">Roles and quota</a></h2>
<p>In order to guarantee that a role is allocated a specific amount of resources,
quota can be specified via the <a href="endpoints/master/quota.html">/quota</a> endpoint.</p>
<p>The resource allocator will first attempt to satisfy the quota requirements,
before fairly sharing the remaining resources. For more information, see the
<a href="quota.html">quota</a> documentation.</p>
<h2 id="role-vs-principal"><a class="header" href="#role-vs-principal">Role vs. Principal</a></h2>
<p>A principal identifies an entity that interacts with Mesos; principals are
similar to user names. For example, frameworks supply a principal when they
register with the Mesos master, and operators provide a principal when using the
operator HTTP endpoints. An entity may be required to
<a href="authentication.html">authenticate</a> with its principal in order to prove its
identity, and the principal may be used to <a href="authorization.html">authorize</a> actions
performed by an entity, such as <a href="reservation.html">resource reservation</a> and
<a href="persistent-volume.html">persistent volume</a> creation/destruction.</p>
<p>Roles, on the other hand, are used exclusively for resource allocation, as
covered above.</p>
<hr />
<h2>title: Apache Mesos - Weights
layout: documentation</h2>
<h1 id="weights"><a class="header" href="#weights">Weights</a></h1>
<p>In Mesos, <strong>weights</strong> can be used to control the relative share of cluster
resources that is offered to different <a href="roles.html">roles</a>.</p>
<p>In Mesos 0.28 and earlier, weights can only be configured by specifying
the <code>--weights</code> command-line flag when starting the Mesos master. If a
role does not have a weight specified in the <code>--weights</code> flag, then the default
value (1.0) will be used. Weights cannot be changed without updating the flag
and restarting all Mesos masters.</p>
<p>Mesos 1.0 contains a <a href="endpoints/master/weights.html">/weights</a> operator endpoint
that allows weights to be changed at runtime. The <code>--weights</code> command-line flag
is deprecated.</p>
<h1 id="operator-http-endpoint"><a class="header" href="#operator-http-endpoint">Operator HTTP Endpoint</a></h1>
<p>The master <code>/weights</code> HTTP endpoint enables operators to configure weights. The
endpoint currently offers a REST-like interface and supports the following operations:</p>
<ul>
<li><a href="weights.html#putRequest">Updating</a> weights with PUT.</li>
<li><a href="weights.html#getRequest">Querying</a> the currently set weights with GET.</li>
</ul>
<p>The endpoint can optionally use authentication and authorization. See the
<a href="authentication.html">authentication guide</a> for details.</p>
<p><a name="putRequest"></a></p>
<h2 id="update"><a class="header" href="#update">Update</a></h2>
<p>The operator can update the weights by sending an HTTP PUT request to the <code>/weights</code>
endpoint.</p>
<p>An example request to the <code>/weights</code> endpoint could look like this (using the
JSON file below):</p>
<pre><code>$ curl -d @weights.json -X PUT http://&lt;master-ip&gt;:&lt;port&gt;/weights
</code></pre>
<p>For example, to set a weight of <code>2.0</code> for <code>role1</code> and set a weight of <code>3.5</code>
for <code>role2</code>, the operator can use the following <code>weights.json</code>:</p>
<pre><code>    [
      {
        &quot;role&quot;: &quot;role1&quot;,
        &quot;weight&quot;: 2.0
      },
      {
        &quot;role&quot;: &quot;role2&quot;,
        &quot;weight&quot;: 3.5
      }
    ]
</code></pre>
<p>If the master is configured with an explicit <a href="roles.html">role whitelist</a>, the
request is only valid if all specified roles exist in the role whitelist.</p>
<p>Weights are now persisted in the registry on cluster bootstrap and after any
updates.  Once the weights are persisted in the registry, any Mesos master that
subsequently starts with <code>--weights</code> still specified will emit a warning and use
the registry value instead.</p>
<p>The operator will receive one of the following HTTP response codes:</p>
<ul>
<li><code>200 OK</code>: Success (the update request was successful).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., invalid JSON, non-positive weights).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
</ul>
<p><a name="getRequest"></a></p>
<h2 id="query"><a class="header" href="#query">Query</a></h2>
<p>The operator can query the configured weights by sending an HTTP GET request
to the <code>/weights</code> endpoint.</p>
<pre><code>$ curl -X GET http://&lt;master-ip&gt;:&lt;port&gt;/weights
</code></pre>
<p>The response message body includes a JSON representation of the current
configured weights, for example:</p>
<pre><code>    [
      {
        &quot;role&quot;: &quot;role2&quot;,
        &quot;weight&quot;: 3.5
      },
      {
        &quot;role&quot;: &quot;role1&quot;,
        &quot;weight&quot;: 2.0
      }
    ]
</code></pre>
<p>The operator will receive one of the following HTTP response codes:</p>
<ul>
<li><code>200 OK</code>: Success.</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
</ul>
<hr />
<h2>title: Apache Mesos - Quota
layout: documentation</h2>
<h1 id="quota"><a class="header" href="#quota">Quota</a></h1>
<p>When multiple users are sharing a cluster, the operator may want to set limits
on how many resources each user can use. Quota addresses this need and allows
operators to set these limits on a per-role basis.</p>
<ul>
<li><a href="quota.html#supported-resources">Supported Resources</a></li>
<li><a href="quota.html#updating-quotas">Setting Quotas</a></li>
<li><a href="quota.html#viewing-quotas">Viewing Quotas</a></li>
<li><a href="quota.html#deprecated-quota-guarantees">Deprecated: Quota Guarantees</a></li>
<li><a href="quota.html#implementation-notes">Implementation Notes</a></li>
</ul>
<h2 id="supported-resources"><a class="header" href="#supported-resources">Supported Resources</a></h2>
<p>The following resources have quota support:</p>
<ul>
<li><code>cpus</code></li>
<li><code>mem</code></li>
<li><code>disk</code></li>
<li><code>gpus</code></li>
<li>any custom resource of type <code>SCALAR</code></li>
</ul>
<p>The following resources do not have quota support:</p>
<ul>
<li><code>ports</code></li>
<li>any custom resource of type <code>RANGES</code> or <code>SET</code></li>
</ul>
<h2 id="updating-quotas"><a class="header" href="#updating-quotas">Updating Quotas</a></h2>
<p>By default, every role has no resource limits. To modify the resource limits
for one or more roles, the v1 API <code>UPDATE_QUOTA</code> call is used. Note that this
call applies the update in an all-or-nothing manner, so that if one of the
role's quota updates is invalid or unauthorized, the entire request will not
go through.</p>
<p>Example:</p>
<pre><code>curl --request POST \
     --url http://&lt;master-ip&gt;:&lt;master-port&gt;/api/v1/ \
     --header 'Content-Type: application/json' \
     --data '{
               &quot;type&quot;: &quot;UPDATE_QUOTA&quot;,
               &quot;update_quota&quot;: {
                 &quot;force&quot;: false,
                 &quot;quota_configs&quot;: [
                   {
                     &quot;role&quot;: &quot;dev&quot;,
                     &quot;limits&quot;: {
                       &quot;cpus&quot;: { &quot;value&quot;: 10 },
                       &quot;mem&quot;:  { &quot;value&quot;: 2048 },
                       &quot;disk&quot;: { &quot;value&quot;: 4096 }
                     }
                   },
                   {
                     &quot;role&quot;: &quot;test&quot;,
                     &quot;limits&quot;: {
                       &quot;cpus&quot;: { &quot;value&quot;: 1 },
                       &quot;mem&quot;:  { &quot;value&quot;: 256 },
                       &quot;disk&quot;: { &quot;value&quot;: 512 }
                     }
                   }
                 ]
               }
             }'
</code></pre>
<ul>
<li>Note that the request will be denied if the current quota consumption is above
the provided limit. This check can be overriden by setting <code>force</code> to <code>true</code>.</li>
<li>Note that the master will attempt to rescind a sufficient number of offers to
ensure that the role cannot exceed its limits.</li>
</ul>
<h2 id="viewing-quotas"><a class="header" href="#viewing-quotas">Viewing Quotas</a></h2>
<h3 id="web-ui"><a class="header" href="#web-ui">Web UI</a></h3>
<p>The 'Roles' tab in the web ui displays resource accounting information for all
known roles. This includes the configured quota and the quota consumption.</p>
<h3 id="api"><a class="header" href="#api">API</a></h3>
<p>There are several endpoints for viewing quota related information.</p>
<p>The v1 API <code>GET_QUOTA</code> call will return the quota configuration:</p>
<pre><code>$ curl --request POST \
     --url http://&lt;master-ip&gt;:&lt;master-port&gt;/api/v1/ \
     --header 'Content-Type: application/json' \
     --header 'Accept: application/json' \
     --data '{ &quot;type&quot;: &quot;GET_QUOTA&quot; }'
</code></pre>
<p>Response:</p>
<pre><code>{
  &quot;type&quot;: &quot;GET_QUOTA&quot;,
  &quot;get_quota&quot;: {
    &quot;status&quot;: {
      &quot;infos&quot;: [
        {
          &quot;configs&quot; : [
            {
              &quot;role&quot;: &quot;dev&quot;,
              &quot;limits&quot;: {
                &quot;cpus&quot;: { &quot;value&quot;: 10.0 },
                &quot;mem&quot;:  { &quot;value&quot;: 2048.0 },
                &quot;disk&quot;: { &quot;value&quot;: 4096.0 }
              }
            },
            {
              &quot;role&quot;: &quot;test&quot;,
              &quot;limits&quot;: {
                &quot;cpus&quot;: { &quot;value&quot;: 1.0 },
                &quot;mem&quot;:  { &quot;value&quot;: 256.0 },
                &quot;disk&quot;: { &quot;value&quot;: 512.0 }
              }
            }
          ]
        }
      ]
    }
  }
}
</code></pre>
<p>To also view the quota consumption, use the <code>/roles</code> endpoint:</p>
<pre><code>$ curl http://&lt;master-ip&gt;:&lt;master-port&gt;/roles
</code></pre>
<p>Response</p>
<pre><code>{
  &quot;roles&quot;: [
    {
      &quot;name&quot;: &quot;dev&quot;,
      &quot;weight&quot;: 1.0,
      &quot;quota&quot;:
      {
        &quot;role&quot;: &quot;dev&quot;,
        &quot;limit&quot;: {
          &quot;cpus&quot;: 10.0,
          &quot;mem&quot;:  2048.0,
          &quot;disk&quot;: 4096.0
        },
        &quot;consumed&quot;: {
          &quot;cpus&quot;: 2.0,
          &quot;mem&quot;:  1024.0,
          &quot;disk&quot;: 2048.0
        }
      },
      &quot;allocated&quot;: {
        &quot;cpus&quot;: 2.0,
        &quot;mem&quot;:  1024.0,
        &quot;disk&quot;: 2048.0
      },
      &quot;offered&quot;: {},
      &quot;reserved&quot;: {
        &quot;cpus&quot;: 2.0,
        &quot;mem&quot;:  1024.0,
        &quot;disk&quot;: 2048.0
      },
      &quot;frameworks&quot;: []
    }
  ]
}
</code></pre>
<h3 id="quota-consumption"><a class="header" href="#quota-consumption">Quota Consumption</a></h3>
<p>A role's quota consumption consists of its allocations and reservations.
In other words, even if reservations are not allocated, they are included
in the quota consumption. Offered resources are not charged against quota.</p>
<h3 id="metrics"><a class="header" href="#metrics">Metrics</a></h3>
<p>The following metric keys are exposed for quota:</p>
<ul>
<li><code>allocator/mesos/quota/roles/&lt;role&gt;/resources/&lt;resource&gt;/guarantee</code></li>
<li><code>allocator/mesos/quota/roles/&lt;role&gt;/resources/&lt;resource&gt;/limit</code></li>
<li>A quota consumption metric will be added via
<a href="https://issues.apache.org/jira/browse/MESOS-9123">MESOS-9123</a>.</li>
</ul>
<h2 id="deprecated-quota-guarantees"><a class="header" href="#deprecated-quota-guarantees">Deprecated: Quota Guarantees</a></h2>
<p>Prior to Mesos 1.9, the quota related APIs only exposed quota &quot;guarantees&quot;
which ensured a minimum amount of resources would be available to a role.
Setting guarantees also set implicit quota limits. In Mesos 1.9+, quota
limits are now exposed directly per the above documentation.</p>
<p>Quota guarantees are now deprecated in favor of using only quota limits.
Enforcement of quota guarantees required that Mesos holds back enough
resources to meet all of the unsatisfied quota guarantees. Since Mesos is
moving towards an optimistic offer model (to improve multi-role / multi-
scheduler scalability, see
<a href="https://issues.apache.org/jira/browse/MESOS-1607">MESOS-1607</a>), it will
become no longer possible to enforce quota guarantees by holding back
resources. In such a model, quota limits are simple to enforce, but quota
guarantees would require a complex &quot;effective limit&quot; propagation model to
leave space for unsatisfied guarantees.</p>
<p>For these reasons, quota guarantees, while still functional in Mesos 1.9,
are now deprecated. A combination of limits and priority based preemption
will be simpler in an optimistic offer model.</p>
<p>For documentation on quota guarantees, please see the previous
documentation: <a href="https://github.com/apache/mesos/blob/1.8.0/docs/quota.md">https://github.com/apache/mesos/blob/1.8.0/docs/quota.md</a></p>
<h2 id="implementation-notes"><a class="header" href="#implementation-notes">Implementation Notes</a></h2>
<ul>
<li>Quota is not supported on nested roles (e.g. <code>eng/prod</code>).</li>
<li>During failover, in order to correctly enforce limits, the allocator
will be paused and will not issue offers until at least 80% agents
re-register or 10 minutes elapses. These parameters will be made
configurable: <a href="https://issues.apache.org/jira/browse/MESOS-4073">MESOS-4073</a></li>
<li>Quota is SUPPORTED for the default <code>*</code> role now <a href="https://issues.apache.org/jira/browse/MESOS-3938">MESOS-3938</a>.</li>
</ul>
<hr />
<h2>title: Apache Mesos - Reservation
layout: documentation</h2>
<h1 id="reservation"><a class="header" href="#reservation">Reservation</a></h1>
<p>Mesos provides mechanisms to <strong>reserve</strong> resources in specific slaves.
The concept was first introduced with <strong>static reservation</strong> in 0.14.0
which enabled operators to specify the reserved resources on slave startup.
This was extended with <strong>dynamic reservation</strong> in 0.23.0 which enabled operators
and authorized <strong>frameworks</strong> to dynamically reserve resources in the cluster.</p>
<p>In both types of reservations, resources are reserved for a <a href="roles.html"><strong>role</strong></a>.</p>
<p><a name="static-reservation"></a></p>
<h2 id="static-reservation"><a class="header" href="#static-reservation">Static Reservation</a></h2>
<p>An operator can configure a slave with resources reserved for a role.
The reserved resources are specified via the <code>--resources</code> flag.
For example, suppose we have 12 CPUs and 6144 MB of RAM available on a slave and
that we want to reserve 8 CPUs and 4096 MB of RAM for the <code>ads</code> role.
We start the slave like so:</p>
<pre><code>    $ mesos-slave \
      --master=&lt;ip&gt;:&lt;port&gt; \
      --resources=&quot;cpus:4;mem:2048;cpus(ads):8;mem(ads):4096&quot;
</code></pre>
<p>We now have 8 CPUs and 4096 MB of RAM reserved for <code>ads</code> on this slave.</p>
<p><strong>CAVEAT:</strong> In order to modify a static reservation, the operator must drain and
restart the slave with the new configuration specified in the
<code>--resources</code> flag.</p>
<p>It's often more convenient to specify the total resources available on
the slave as unreserved via the <code>--resources</code> flag and manage reservations
dynamically (see below) via the master HTTP endpoints. However static
reservation provides a way for the operator to more deterministically control
the reservations (roles, amount, principals) before the agent is exposed to the
master and frameworks. One use case is for the operator to dedicate entire
agents for specific roles.</p>
<h2 id="dynamic-reservation"><a class="header" href="#dynamic-reservation">Dynamic Reservation</a></h2>
<p>As mentioned in <a href="reservation.html#static-reservation">Static Reservation</a>, specifying
the reserved resources via the <code>--resources</code> flag makes the reservation static.
That is, statically reserved resources cannot be reserved for another role nor
be unreserved. Dynamic reservation enables operators and authorized frameworks
to reserve and unreserve resources after slave-startup.</p>
<ul>
<li><code>Offer::Operation::Reserve</code> and <code>Offer::Operation::Unreserve</code> messages are
available for <strong>frameworks</strong> to send back via the <code>acceptOffers</code> API as a
response to a resource offer.</li>
<li><code>/reserve</code> and <code>/unreserve</code> HTTP endpoints allow <strong>operators</strong> to manage
dynamic reservations through the master.</li>
</ul>
<p>In the following sections, we will walk through examples of each of the
interfaces described above.</p>
<p>If two dynamic reservations are made for the same role at a single slave (using
the same labels, if any; see below), the reservations will be combined by adding
together the resources reserved by each request. This will result in a single
reserved resource at the slave. Similarly, &quot;partial&quot; unreserve operations are
allowed: an unreserve operation can release some but not all of the resources at
a slave that have been reserved for a role. In this case, the unreserved
resources will be subtracted from the previous reservation and any remaining
resources will still be reserved.</p>
<p>Dynamic reservations cannot be unreserved if they are still being used by a
running task or if a <a href="persistent-volume.html">persistent volume</a> has been created
using the reserved resources. In the latter case, the volume should be destroyed
before unreserving the resources.</p>
<h2 id="authorization"><a class="header" href="#authorization">Authorization</a></h2>
<p>By default, frameworks and operators are authorized to reserve resources for
any role and to unreserve dynamically reserved resources.
<a href="authorization.html">Authorization</a> allows this behavior to be limited so that
resources can only be reserved for particular roles, and only particular
resources can be unreserved. For these operations to be authorized, the
framework or operator should provide a <code>principal</code> to identify itself. To use
authorization with reserve/unreserve operations, the Mesos master must be
configured with the appropriate ACLs. For more information, see the
<a href="authorization.html">authorization documentation</a>.</p>
<p>Similarly, agents by default can register with the master with resources that
are statically reserved for arbitrary roles.
With <a href="authorization.html">authorization</a>,
the master can be configured to use the <code>reserve_resources</code> ACL to check that
the agent's <code>principal</code> is allowed to statically reserve resources for specific
roles.</p>
<h2 id="reservation-labels"><a class="header" href="#reservation-labels">Reservation Labels</a></h2>
<p>Dynamic reservations can optionally include a list of <em>labels</em>, which are
arbitrary key-value pairs. Labels can be used to associate arbitrary metadata
with a resource reservation. For example, frameworks can use labels to identify
the intended purpose for a portion of the resources that have been reserved at a
given slave. Note that two reservations with different labels will not be
combined together into a single reservation, even if the reservations are at the
same slave and use the same role.</p>
<h2 id="reservation-refinement"><a class="header" href="#reservation-refinement">Reservation Refinement</a></h2>
<p>Hierarhical roles such as <code>eng/backend</code> enable the delegation of resources
down a hierarchy, and reservation refinement is the mechanism with which
<strong>reservations</strong> are delegated down the hierarchy. For example, a reservation
(static or dynamic) for <code>eng</code> can be <strong>refined</strong> to <code>eng/backend</code>. When such
a reservation is unreserved, they are returned to the previous owner. In this
case it would be returned to <code>eng</code>. Reservation refinements can also &quot;skip&quot;
levels. For example, <code>eng</code> can be <strong>refined</strong> directly to <code>eng/backend/db</code>.
Again, unreserving such a reservation is returned to its previous owner <code>eng</code>.</p>
<p><strong>NOTE:</strong> Frameworks need to enable the <code>RESERVATION_REFINEMENT</code> capability
in order to be offered, and to create refined reservations</p>
<h2 id="listing-reservations"><a class="header" href="#listing-reservations">Listing Reservations</a></h2>
<p>Information about the reserved resources at each slave in the cluster can be
found by querying the <a href="endpoints/master/slaves.html">/slaves</a> master endpoint
(under the <code>reserved_resources_full</code> key).</p>
<p>The same information can also be found in the <a href="endpoints/slave/state.html">/state</a>
endpoint on the agent (under the <code>reserved_resources_full</code> key). The agent
endpoint is useful to confirm if a reservation has been propagated to the
agent (which can fail in the event of network partition or master/agent
restarts).</p>
<h2 id="examples-3"><a class="header" href="#examples-3">Examples</a></h2>
<h3 id="framework-scheduler-api"><a class="header" href="#framework-scheduler-api">Framework Scheduler API</a></h3>
<p><a name="offer-operation-reserve"></a></p>
<h4 id="offeroperationreserve-without-reservation_refinement"><a class="header" href="#offeroperationreserve-without-reservation_refinement"><code>Offer::Operation::Reserve</code> (<strong>without</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework can reserve resources through the resource offer cycle. The
reservation role must match the offer's allocation role. Suppose we
receive a resource offer with 12 CPUs and 6144 MB of RAM unreserved, allocated
to role <code>&quot;engineering&quot;</code>.</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 12 },
          &quot;role&quot;: &quot;*&quot;,
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 6144 },
          &quot;role&quot;: &quot;*&quot;,
        }
      ]
    }
</code></pre>
<p>We can reserve 8 CPUs and 4096 MB of RAM by sending the following
<code>Offer::Operation</code> message. <code>Offer::Operation::Reserve</code> has a <code>resources</code> field
which we specify with the resources to be reserved. We must explicitly set the
resources' <code>role</code> field to the offer's allocation role. The required value of
the <code>principal</code> field depends on whether or not the framework provided a
principal when it registered with the master. If a principal was provided, then
the resources' <code>principal</code> field must be equal to the framework's principal.
If no principal was provided during registration, then the resources'
<code>principal</code> field can take any value, or can be left unset. Note that the
<code>principal</code> field determines the &quot;reserver principal&quot; when
<a href="authorization.html">authorization</a> is enabled, even if authentication is
disabled.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::RESERVE,
      &quot;reserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          }
        ]
      }
    }
</code></pre>
<p>If the reservation is successful, a subsequent resource offer will contain the
following reserved resources:</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
      ]
    }
</code></pre>
<h4 id="offeroperationunreserve-without-reservation_refinement"><a class="header" href="#offeroperationunreserve-without-reservation_refinement"><code>Offer::Operation::Unreserve</code> (<strong>without</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework can unreserve resources through the resource offer cycle.
In <a href="reservation.html#offer-operation-reserve">Offer::Operation::Reserve</a>, we reserved 8 CPUs
and 4096 MB of RAM on a particular slave for one of our subscribed roles
(e.g. <code>&quot;engineering&quot;</code>). The master will continue to only offer these reserved
resources to the reservation's <code>role</code>. Suppose we would like to unreserve
these resources. First, we receive a resource offer (copy/pasted from above):</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
      ]
    }
</code></pre>
<p>We can unreserve the 8 CPUs and 4096 MB of RAM by sending the following
<code>Offer::Operation</code> message. <code>Offer::Operation::Unreserve</code> has a <code>resources</code>
field which we can use to specify the resources to be unreserved.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::UNRESERVE,
      &quot;unreserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          }
        ]
      }
    }
</code></pre>
<p>The unreserved resources may now be offered to other frameworks.</p>
<p><a name="offer-operation-reserve-reservation-refinement"></a></p>
<h4 id="offeroperationreserve-with-reservation_refinement"><a class="header" href="#offeroperationreserve-with-reservation_refinement"><code>Offer::Operation::Reserve</code> (<strong>with</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework that wants to create a refined reservation needs to enable
the <code>RESERVATION_REFINEMENT</code> capability. Doing so will allow the framework
to use the <code>reservations</code> field in the <code>Resource</code> message in order to
<strong>push</strong> a refined reservation.</p>
<p>Since reserved resources are offered to any of the child roles under the role
for which they are reserved for, they can get <strong>allocated</strong> to say,
<code>&quot;engineering/backend&quot;</code> while being <strong>reserved</strong> for <code>&quot;engineering&quot;</code>.
It can then be refined to be <strong>reserved</strong> for <code>&quot;engineering/backend&quot;</code>.</p>
<p>Note that the refined reservation role must match the offer's allocation role.</p>
<p>Suppose we receive a resource offer with 12 CPUs and 6144 MB of RAM reserved to
<code>&quot;engineering&quot;</code>, allocated to role <code>&quot;engineering/backend&quot;</code>.</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 12 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            }
          ]
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 6144 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            }
          ]
        }
      ]
    }
</code></pre>
<p>Take note of the fact that <code>role</code> and <code>reservation</code> are not set, and that there
is a new field called <code>reservations</code> which represents the reservation state.
With <code>RESERVATION_REFINEMENT</code> enabled, the framework receives resources in this
new format where solely the <code>reservations</code> field is used for the reservation
state, rather than <code>role</code>/<code>reservation</code> pair from pre-<code>RESERVATION_REFINEMENT</code>.</p>
<p>We can reserve 8 CPUs and 4096 MB of RAM to <code>&quot;engineering/backend&quot;</code> by sending
the following <code>Offer::Operation</code> message. <code>Offer::Operation::Reserve</code> has
a <code>resources</code> field which we specify with the resources to be reserved.
We must <strong>push</strong> a new <code>ReservationInfo</code> message onto the back of
the <code>reservations</code> field. We must explicitly set the reservation's' <code>role</code> field
to the offer's allocation role. The optional value of the <code>principal</code> field
depends on whether or not the framework provided a principal when it registered
with the master. If a principal was provided, then the resources' <code>principal</code>
field must be equal to the framework's principal. If no principal was provided
during registration, then the resources' <code>principal</code> field can take any value,
or can be left unset.  Note that the <code>principal</code> field determines
the &quot;reserver principal&quot; when <a href="authorization.html">authorization</a> is enabled, even
if authentication is disabled.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::RESERVE,
      &quot;reserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          }
        ]
      }
    }
</code></pre>
<p>If the reservation is successful, a subsequent resource offer will contain the
following reserved resources:</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
      ]
    }
</code></pre>
<h4 id="offeroperationunreserve-with-reservation_refinement"><a class="header" href="#offeroperationunreserve-with-reservation_refinement"><code>Offer::Operation::Unreserve</code> (<strong>with</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework can unreserve resources through the resource offer cycle.
In <a href="reservation.html#offer-operation-reserve-reservation-refinement">Offer::Operation::Reserve</a>,
we reserved 8 CPUs and 4096 MB of RAM on a particular slave for one of our
subscribed roles (i.e. <code>&quot;engineering/backend&quot;</code>), previously reserved for
<code>&quot;engineering&quot;</code>. When we unreserve these resources, they are returned to
<code>&quot;engineering&quot;</code>, by the last <code>ReservationInfo</code> added to
the <code>reservations</code> field being <strong>popped</strong>. First, we receive a resource offer
(copy/pasted from above):</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
      ]
    }
</code></pre>
<p>We can unreserve the 8 CPUs and 4096 MB of RAM by sending the following
<code>Offer::Operation</code> message. <code>Offer::Operation::Unreserve</code> has a <code>resources</code>
field which we can use to specify the resources to be unreserved.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::UNRESERVE,
      &quot;unreserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          },
        ]
      }
    }
</code></pre>
<p>The resources will now be reserved for <code>&quot;engineering&quot;</code> again, and may now be
offered to <code>&quot;engineering&quot;</code> role itself, or other roles under <code>&quot;engineering&quot;</code>.</p>
<h3 id="operator-http-endpoints"><a class="header" href="#operator-http-endpoints">Operator HTTP Endpoints</a></h3>
<p>As described above, dynamic reservations can be made by a framework scheduler,
typically in response to a resource offer. However, dynamic reservations can
also be created and deleted by sending HTTP requests to the <code>/reserve</code> and
<code>/unreserve</code> endpoints, respectively. This capability is intended for use by
operators and administrative tools.</p>
<h4 id="reserve-since-0250"><a class="header" href="#reserve-since-0250"><code>/reserve</code> (since 0.25.0)</a></h4>
<p>Suppose we want to reserve 8 CPUs and 4096 MB of RAM for the <code>ads</code> role on a
slave with id=<code>&lt;slave_id&gt;</code> (note that it is up to the user to find the ID of the
slave that hosts the desired resources; the request will fail if sufficient
unreserved resources cannot be found on the slave). In this case, the principal
that must be included in the <code>reservation</code> field of the reserved resources
depends on the status of HTTP authentication on the master. If HTTP
authentication is enabled, then the principal in the reservation should match
the authenticated principal provided in the request's HTTP headers. If HTTP
authentication is disabled, then the principal in the reservation can take any
value, or can be left unset. Note that the <code>principal</code> field determines the
&quot;reserver principal&quot; when <a href="authorization.html">authorization</a> is enabled, even if
HTTP authentication is disabled.</p>
<p>We send an HTTP POST request to the master's
<a href="endpoints/master/reserve.html">/reserve</a> endpoint like so:</p>
<pre><code>    $ curl -i \
      -u &lt;operator_principal&gt;:&lt;password&gt; \
      -d slaveId=&lt;slave_id&gt; \
      -d resources='[
        {
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;operator_principal&gt;,
            }
          ]
        },
        {
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;operator_principal&gt;,
            }
          ]
        }
      ]' \
      -X POST http://&lt;ip&gt;:&lt;port&gt;/master/reserve
</code></pre>
<p>The user receives one of the following HTTP responses:</p>
<ul>
<li><code>202 Accepted</code>: Request accepted (see below).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., missing parameters).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
<li><code>409 Conflict</code>: Insufficient resources to satisfy the reserve operation.</li>
</ul>
<p>This endpoint returns the 202 ACCEPTED HTTP status code, which indicates that
the reserve operation has been validated successfully by the master. The
request is then forwarded asynchronously to the Mesos slave where the reserved
resources are located. That asynchronous message may not be delivered or
reserving resources at the slave might fail, in which case no resources will be
reserved. To determine if a reserve operation has succeeded, the user can
examine the state of the appropriate Mesos slave (e.g., via the slave's
<a href="endpoints/slave/state.html">/state</a> HTTP endpoint).</p>
<h4 id="unreserve-since-0250"><a class="header" href="#unreserve-since-0250"><code>/unreserve</code> (since 0.25.0)</a></h4>
<p>Suppose we want to unreserve the resources that we dynamically reserved above.
We can send an HTTP POST request to the master's
<a href="endpoints/master/unreserve.html">/unreserve</a> endpoint like so:</p>
<pre><code>    $ curl -i \
      -u &lt;operator_principal&gt;:&lt;password&gt; \
      -d slaveId=&lt;slave_id&gt; \
      -d resources='[
        {
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;reserver_principal&gt;,
            }
          ]
        },
        {
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;reserver_principal&gt;,
            }
          ]
        }
      ]' \
      -X POST http://&lt;ip&gt;:&lt;port&gt;/master/unreserve
</code></pre>
<p>Note that <code>reserver_principal</code> is the principal that was used to make the
reservation, while <code>operator_principal</code> is the principal that is attempting to
perform the unreserve operation---in some cases, these principals might be the
same. The <code>operator_principal</code> must be <a href="authorization.html">authorized</a> to
unreserve reservations made by <code>reserver_principal</code>.</p>
<p>The user receives one of the following HTTP responses:</p>
<ul>
<li><code>202 Accepted</code>: Request accepted (see below).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., missing parameters).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
<li><code>409 Conflict</code>: Insufficient resources to satisfy the unreserve operation.</li>
</ul>
<p>This endpoint returns the 202 ACCEPTED HTTP status code, which indicates that
the unreserve operation has been validated successfully by the master. The
request is then forwarded asynchronously to the Mesos slave where the reserved
resources are located. That asynchronous message may not be delivered or
unreserving resources at the slave might fail, in which case no resources will
be unreserved. To determine if an unreserve operation has succeeded, the user
can examine the state of the appropriate Mesos slave (e.g., via the slave's
<a href="endpoints/slave/state.html">/state</a> HTTP endpoint).</p>
<hr />
<h2>title: Apache Mesos - Shared Persistent Volumes
layout: documentation</h2>
<h1 id="shared-persistent-volumes"><a class="header" href="#shared-persistent-volumes">Shared Persistent Volumes</a></h1>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>By default, <a href="persistent-volume.html">persistent volumes</a> provide
<em>exclusive</em> access: once a task is launched using a persistent volume,
no other tasks can use that volume, and the volume will not appear in
any resource offers until the task that is using it has finished.</p>
<p>In some cases, it can be useful to share a volume between multiple tasks
running on the same agent. For example, this could be used to
efficiently share a large data set between multiple data analysis tasks.</p>
<h2 id="creating-shared-volumes"><a class="header" href="#creating-shared-volumes">Creating Shared Volumes</a></h2>
<p>Shared persistent volumes are created using the same workflow as normal
persistent volumes: by starting with a
<a href="reservation.html">reserved resource</a> and applying a <code>CREATE</code> operation,
either via the framework scheduler API or the
<a href="endpoints/master/create-volumes.html">/create-volumes</a> HTTP endpoint. To
create a shared volume, set the <code>shared</code> field during volume creation.</p>
<p>For example, suppose a framework subscribed to the <code>&quot;engineering&quot;</code> role
receives a resource offer containing 2048MB of dynamically reserved disk:</p>
<pre><code>{
  &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &quot;engineering&quot;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      }
    }
  ]
}
</code></pre>
<p>The framework can create a shared persistent volume using this disk
resource via the following offer operation:</p>
<pre><code>{
  &quot;type&quot; : Offer::Operation::CREATE,
  &quot;create&quot;: {
    &quot;volumes&quot; : [
      {
        &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
        &quot;name&quot; : &quot;disk&quot;,
        &quot;type&quot; : &quot;SCALAR&quot;,
        &quot;scalar&quot; : { &quot;value&quot; : 2048 },
        &quot;role&quot; : &quot;engineering&quot;,
        &quot;reservation&quot; : {
          &quot;principal&quot; : &lt;framework_principal&gt;
        },
        &quot;disk&quot;: {
          &quot;persistence&quot;: {
            &quot;id&quot; : &lt;persistent_volume_id&gt;
          },
          &quot;volume&quot; : {
            &quot;container_path&quot; : &lt;container_path&gt;,
            &quot;mode&quot; : &lt;mode&gt;
          }
        },
        &quot;shared&quot; : {
        }
      }
    ]
  }
}
</code></pre>
<p>Note that the <code>shared</code> field has been set (to an empty JSON object),
which indicates that the <code>CREATE</code> operation will create a shared volume.</p>
<h2 id="using-shared-volumes"><a class="header" href="#using-shared-volumes">Using Shared Volumes</a></h2>
<p>To be eligible to receive resource offers that contain shared volumes, a
framework must enable the <code>SHARED_RESOURCES</code> capability in the
<code>FrameworkInfo</code> it provides when it registers with the master.
Frameworks that do <em>not</em> enable this capability will not be offered
shared resources.</p>
<p>When a framework receives a resource offer, it can determine whether a
volume is shared by checking if the <code>shared</code> field has been set. Unlike
normal persistent volumes, a shared volume that is in use by a task will
continue to be offered to the frameworks subscribed to the volume's role;
this gives those frameworks the opportunity to launch additional tasks
that can access the volume. A framework can also launch multiple tasks
that access the volume using a single <code>ACCEPT</code> call.</p>
<p>Note that Mesos does not provide any isolation or concurrency control
between the tasks that are sharing a volume. Framework developers should
ensure that tasks that access the same volume do not conflict with one
another. This can be done via careful application-level concurrency
control, or by ensuring that the tasks access the volume in a read-only
manner. Mesos provides support for read-only access to volumes: as
described in the <a href="persistent-volume.html">persistent volume</a>
documentation, tasks that are launched on a volume can specify a <code>mode</code>
of <code>&quot;RO&quot;</code> to use the volume in read-only mode.</p>
<h3 id="destroying-shared-volumes"><a class="header" href="#destroying-shared-volumes">Destroying Shared Volumes</a></h3>
<p>A persistent volume, whether shared or not, can only be destroyed if no
running or pending tasks have been launched using the volume. For
non-shared volumes, it is usually easy to determine when it is safe to
delete a volume. For shared volumes, the framework(s) that have launched
tasks using the volume typically need to coordinate to ensure (e.g., via
reference counting) that a volume is no longer being used before it is
destroyed.</p>
<h3 id="resource-allocation"><a class="header" href="#resource-allocation">Resource Allocation</a></h3>
<p>TODO: how do shared volumes influence resource allocation?</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li>
<p><a href="https://issues.apache.org/jira/browse/MESOS-3421">MESOS-3421</a>
contains additional information about the implementation of this
feature.</p>
</li>
<li>
<p>Talk at MesosCon Europe 2016 on August 31, 2016 entitled
&quot;<a href="http://schd.ws/hosted_files/mesosconeu2016/08/MesosConEurope2016PPVv1.0.pdf">Practical Persistent Volumes</a>&quot;.</p>
</li>
</ul>
<hr />
<h2>title: Apache Mesos - Oversubscription
layout: documentation</h2>
<h1 id="oversubscription"><a class="header" href="#oversubscription">Oversubscription</a></h1>
<p>High-priority user-facing services are typically provisioned on large clusters
for peak load and unexpected load spikes. Hence, for most of time, the
provisioned resources remain underutilized. Oversubscription takes advantage of
temporarily unused resources to execute best-effort tasks such as background
analytics, video/image processing, chip simulations, and other low priority
jobs.</p>
<h2 id="how-does-it-work-1"><a class="header" href="#how-does-it-work-1">How does it work?</a></h2>
<p>Oversubscription was introduced in Mesos 0.23.0 and adds two new agent
components: a Resource Estimator and a Quality of Service (QoS) Controller,
alongside extending the existing resource allocator, resource monitor, and
Mesos agent. The new components and their interactions are illustrated below.</p>
<p><img src="images/oversubscription-overview.jpg" alt="Oversubscription overview" /></p>
<h3 id="resource-estimation"><a class="header" href="#resource-estimation">Resource estimation</a></h3>
<ul>
<li>
<p>(1) The first step is to identify the amount of oversubscribed resources.
The resource estimator taps into the resource monitor and periodically gets
usage statistics via <code>ResourceStatistic</code> messages. The resource estimator
applies logic based on the collected resource statistics to determine the
amount of oversubscribed resources. This can be a series of control algorithms
based on measured resource usage slack (allocated but unused resources) and
allocation slack.</p>
</li>
<li>
<p>(2) The agent keeps polling estimates from the resource estimator and tracks
the latest estimate.</p>
</li>
<li>
<p>(3) The agent will send the total amount of oversubscribed resources to the
master when the latest estimate is different from the previous estimate.</p>
</li>
</ul>
<h3 id="resource-tracking--scheduling-algorithm"><a class="header" href="#resource-tracking--scheduling-algorithm">Resource tracking &amp; scheduling algorithm</a></h3>
<ul>
<li>(4) The allocator keeps track of the oversubscribed resources separately
from regular resources and annotate those resources as <code>revocable</code>. It is up
to the resource estimator to determine which types of resources can be
oversubscribed. It is recommended only to oversubscribe <em>compressible</em>
resources such as cpu shares, bandwidth, etc.</li>
</ul>
<h3 id="frameworks-1"><a class="header" href="#frameworks-1">Frameworks</a></h3>
<ul>
<li>(5) Frameworks can choose to launch tasks on revocable resources by using
the regular <code>launchTasks()</code> API. To safe-guard frameworks that are not
designed to deal with preemption, only frameworks registering with the
<code>REVOCABLE_RESOURCES</code> capability set in its framework info will receive offers
with revocable resources.  Further more, revocable resources cannot be
dynamically reserved and persistent volumes should not be created on revocable
disk resources.</li>
</ul>
<h3 id="task-launch"><a class="header" href="#task-launch">Task launch</a></h3>
<ul>
<li>The revocable task is launched as usual when the <code>runTask</code> request is received
on the agent. The resources will still be marked as revocable and isolators
can take appropriate actions, if certain resources need to be setup differently
for revocable and regular tasks.</li>
</ul>
<blockquote>
<p>NOTE: If any resource used by a task or executor is
revocable, the whole container is treated as a revocable container and can
therefore be killed or throttled by the QoS Controller.</p>
</blockquote>
<h3 id="interference-detection"><a class="header" href="#interference-detection">Interference detection</a></h3>
<ul>
<li>(6) When the revocable task is running, it is important to constantly
monitor the original task running on those resources and guarantee
performance based on an SLA.  In order to react to detected interference, the
QoS controller needs to be able to kill or throttle running revocable tasks.</li>
</ul>
<h2 id="enabling-frameworks-to-use-oversubscribed-resources"><a class="header" href="#enabling-frameworks-to-use-oversubscribed-resources">Enabling frameworks to use oversubscribed resources</a></h2>
<p>Frameworks planning to use oversubscribed resources need to register with the
<code>REVOCABLE_RESOURCES</code> capability set:</p>
<pre><code class="language-{.cpp}">FrameworkInfo framework;
framework.set_name(&quot;Revocable framework&quot;);

framework.add_capabilities()-&gt;set_type(
    FrameworkInfo::Capability::REVOCABLE_RESOURCES);
</code></pre>
<p>From that point on, the framework will start to receive revocable resources in
offers.</p>
<blockquote>
<p>NOTE: That there is no guarantee that the Mesos cluster has oversubscription
enabled. If not, no revocable resources will be offered. See below for
instructions how to configure Mesos for oversubscription.</p>
</blockquote>
<h3 id="launching-tasks-using-revocable-resources"><a class="header" href="#launching-tasks-using-revocable-resources">Launching tasks using revocable resources</a></h3>
<p>Launching tasks using revocable resources is done through the existing
<code>launchTasks</code> API. Revocable resources will have the <code>revocable</code> field set. See
below for an example offer with regular and revocable resources.</p>
<pre><code class="language-{.json}">{
  &quot;id&quot;: &quot;20150618-112946-201330860-5050-2210-0000&quot;,
  &quot;framework_id&quot;: &quot;20141119-101031-201330860-5050-3757-0000&quot;,
  &quot;agent_id&quot;: &quot;20150618-112946-201330860-5050-2210-S1&quot;,
  &quot;hostname&quot;: &quot;foobar&quot;,
  &quot;resources&quot;: [
    {
      &quot;name&quot;: &quot;cpus&quot;,
      &quot;type&quot;: &quot;SCALAR&quot;,
      &quot;scalar&quot;: {
        &quot;value&quot;: 2.0
      },
      &quot;role&quot;: &quot;*&quot;
    }, {
      &quot;name&quot;: &quot;mem&quot;,
      &quot;type&quot;: &quot;SCALAR&quot;,
      &quot;scalar&quot;: {
        &quot;value&quot;: 512.0
      },
      &quot;role&quot;: &quot;*&quot;
    },
    {
      &quot;name&quot;: &quot;cpus&quot;,
      &quot;type&quot;: &quot;SCALAR&quot;,
      &quot;scalar&quot;: {
        &quot;value&quot;: 0.45
      },
      &quot;role&quot;: &quot;*&quot;,
      &quot;revocable&quot;: {}
    }
  ]
}
</code></pre>
<h2 id="writing-a-custom-resource-estimator"><a class="header" href="#writing-a-custom-resource-estimator">Writing a custom resource estimator</a></h2>
<p>The resource estimator estimates and predicts the total resources used on the
agent and informs the master about resources that can be oversubscribed. By
default, Mesos comes with a <code>noop</code> and a <code>fixed</code> resource estimator. The <code>noop</code>
estimator only provides an empty estimate to the agent and stalls, effectively
disabling oversubscription. The <code>fixed</code> estimator doesn't use the actual
measured slack, but oversubscribes the node with fixed resource amount (defined
via a command line flag).</p>
<p>The interface is defined below:</p>
<pre><code class="language-{.cpp}">class ResourceEstimator
{
public:
  // Initializes this resource estimator. This method needs to be
  // called before any other member method is called. It registers
  // a callback in the resource estimator. The callback allows the
  // resource estimator to fetch the current resource usage for each
  // executor on agent.
  virtual Try&lt;Nothing&gt; initialize(
      const lambda::function&lt;process::Future&lt;ResourceUsage&gt;()&gt;&amp; usage) = 0;

  // Returns the current estimation about the *maximum* amount of
  // resources that can be oversubscribed on the agent. A new
  // estimation will invalidate all the previously returned
  // estimations. The agent will be calling this method periodically
  // to forward it to the master. As a result, the estimator should
  // respond with an estimate every time this method is called.
  virtual process::Future&lt;Resources&gt; oversubscribable() = 0;
};
</code></pre>
<h2 id="writing-a-custom-qos-controller"><a class="header" href="#writing-a-custom-qos-controller">Writing a custom QoS controller</a></h2>
<p>The interface for implementing custom QoS Controllers is defined below:</p>
<pre><code class="language-{.cpp}">class QoSController
{
public:
  // Initializes this QoS Controller. This method needs to be
  // called before any other member method is called. It registers
  // a callback in the QoS Controller. The callback allows the
  // QoS Controller to fetch the current resource usage for each
  // executor on agent.
  virtual Try&lt;Nothing&gt; initialize(
      const lambda::function&lt;process::Future&lt;ResourceUsage&gt;()&gt;&amp; usage) = 0;

  // A QoS Controller informs the agent about corrections to carry
  // out, but returning futures to QoSCorrection objects. For more
  // information, please refer to mesos.proto.
  virtual process::Future&lt;std::list&lt;QoSCorrection&gt;&gt; corrections() = 0;
};
</code></pre>
<blockquote>
<p>NOTE The QoS Controller must not block <code>corrections()</code>. Back the QoS
Controller with its own libprocess actor instead.</p>
</blockquote>
<p>The QoS Controller informs the agent that particular corrective actions need to
be made. Each corrective action contains information about executor or task and
the type of action to perform.</p>
<p>Mesos comes with a <code>noop</code> and a <code>load</code> qos controller. The <code>noop</code> controller
does not provide any corrections, thus does not assure any quality of service
for regular tasks. The <code>load</code> controller is ensuring the total system load
doesn't exceed a configurable thresholds and as a result try to avoid the cpu
congestion on the node. If the load is above the thresholds controller evicts
all the revocable executors. These thresholds are configurable via two module
parameters <code>load_threshold_5min</code> and <code>load_threshold_15min</code>. They represent
standard unix load averages in the system. 1 minute system load is ignored,
since for oversubscription use case it can be a misleading signal.</p>
<pre><code class="language-{.proto}">message QoSCorrection {
  enum Type {
    KILL = 1; // Terminate an executor.
  }

  message Kill {
    optional FrameworkID framework_id = 1;
    optional ExecutorID executor_id = 2;
  }

  required Type type = 1;
  optional Kill kill = 2;
}
</code></pre>
<h2 id="configuring-oversubscription"><a class="header" href="#configuring-oversubscription">Configuring oversubscription</a></h2>
<p>Five new flags has been added to the agent:</p>
<table class="table table-striped">
  <thead>
    <tr>
      <th width="30%">
        Flag
      </th>
      <th>
        Explanation
      </th>
  </thead>
<tr>
    <td>
      --oversubscribed_resources_interval=VALUE
    </td>
    <td>
      The agent periodically updates the master with the current estimation
about the total amount of oversubscribed resources that are allocated and
available. The interval between updates is controlled by this flag. (default:
15secs)
    </td>
  </tr>
<tr>
    <td>
      --qos_controller=VALUE
    </td>
    <td>
      The name of the QoS Controller to use for oversubscription.
    </td>
  </tr>
<tr>
    <td>
      --qos_correction_interval_min=VALUE
    </td>
    <td>
      The agent polls and carries out QoS corrections from the QoS Controller
based on its observed performance of running tasks. The smallest interval
between these corrections is controlled by this flag. (default: 0ns)
    </td>
  </tr>
<tr>
    <td>
      --resource_estimator=VALUE
    </td>
    <td>
      The name of the resource estimator to use for oversubscription.
    </td>
  </tr>
</table>
<p>The <code>fixed</code> resource estimator is enabled as follows:</p>
<pre><code>--resource_estimator=&quot;org_apache_mesos_FixedResourceEstimator&quot;

--modules='{
  &quot;libraries&quot;: {
    &quot;file&quot;: &quot;/usr/local/lib64/libfixed_resource_estimator.so&quot;,
    &quot;modules&quot;: {
      &quot;name&quot;: &quot;org_apache_mesos_FixedResourceEstimator&quot;,
      &quot;parameters&quot;: {
        &quot;key&quot;: &quot;resources&quot;,
        &quot;value&quot;: &quot;cpus:14&quot;
      }
    }
  }
}'
</code></pre>
<p>In the example above, a fixed amount of 14 cpus will be offered as revocable
resources.</p>
<p>The <code>load</code> qos controller is enabled as follows:</p>
<pre><code>--qos_controller=&quot;org_apache_mesos_LoadQoSController&quot;

--qos_correction_interval_min=&quot;20secs&quot;

--modules='{
  &quot;libraries&quot;: {
    &quot;file&quot;: &quot;/usr/local/lib64/libload_qos_controller.so&quot;,
    &quot;modules&quot;: {
      &quot;name&quot;: &quot;org_apache_mesos_LoadQoSController&quot;,
      &quot;parameters&quot;: [
        {
          &quot;key&quot;: &quot;load_threshold_5min&quot;,
          &quot;value&quot;: &quot;6&quot;
        },
        {
	  &quot;key&quot;: &quot;load_threshold_15min&quot;,
	  &quot;value&quot;: &quot;4&quot;
        }
      ]
    }
  }
}'
</code></pre>
<p>In the example above, when standard unix system load average for 5 minutes will
be above 6, or for 15 minutes will be above 4 then agent will evict all the
<code>revocable</code> executors. <code>LoadQoSController</code> will be effectively run every 20
seconds.</p>
<p>To install a custom resource estimator and QoS controller, please refer to the
<a href="modules.html">modules documentation</a>.</p>
<hr />
<h2>title: Apache Mesos - Authentication
layout: documentation</h2>
<h1 id="authentication"><a class="header" href="#authentication">Authentication</a></h1>
<p>Authentication permits only trusted entities to interact with a Mesos cluster. Authentication can be used by Mesos in three ways:</p>
<ol>
<li>To require that frameworks be authenticated in order to register with the master.</li>
<li>To require that agents be authenticated in order to register with the master.</li>
<li>To require that operators be authenticated to use many <a href="endpoints/index.html">HTTP endpoints</a>.</li>
</ol>
<p>Authentication is disabled by default. When authentication is enabled, operators
can configure Mesos to either use the default authentication module or to use a
<em>custom</em> authentication module.</p>
<p>The default Mesos authentication module uses the
<a href="http://asg.web.cmu.edu/sasl/">Cyrus SASL</a> library.  SASL is a flexible
framework that allows two endpoints to authenticate with each other using a
variety of methods. By default, Mesos uses
<a href="https://en.wikipedia.org/wiki/CRAM-MD5">CRAM-MD5</a> authentication.</p>
<h2 id="credentials-principals-and-secrets"><a class="header" href="#credentials-principals-and-secrets">Credentials, Principals, and Secrets</a></h2>
<p>When using the default CRAM-MD5 authentication method, an entity that wants to
authenticate with Mesos must provide a <em>credential</em>, which consists of a
<em>principal</em> and a <em>secret</em>. The principal is the identity that the entity would
like to use; the secret is an arbitrary string that is used to verify that
identity. Principals are similar to user names, while secrets are similar to
passwords.</p>
<p>Principals are used primarily for authentication and
<a href="authorization.html">authorization</a>; note that a principal is different from a
framework's <em>user</em>, which is the operating system account used by the agent to
run executors, and the framework's <em><a href="roles.html">roles</a></em>, which are used to
determine which resources a framework can use.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>Authentication is configured by specifying command-line flags when starting the
Mesos master and agent processes. For more information, refer to the
<a href="configuration.html">configuration</a> documentation.</p>
<h3 id="master-1"><a class="header" href="#master-1">Master</a></h3>
<ul>
<li>
<p><code>--[no-]authenticate</code> - If <code>true</code>, only authenticated frameworks are allowed
to register. If <code>false</code> (the default), unauthenticated frameworks are also
allowed to register.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readonly</code> - If <code>true</code>, authentication is required to
make HTTP requests to the read-only HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-only endpoints are those which cannot be used to modify
the state of the cluster.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readwrite</code> - If <code>true</code>, authentication is required
to make HTTP requests to the read-write HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-write endpoints are those which can be used to modify the
state of the cluster.</p>
</li>
<li>
<p><code>--[no-]authenticate_agents</code> - If <code>true</code>, only authenticated agents are
allowed to register. If <code>false</code> (the default), unauthenticated agents are also
allowed to register.</p>
</li>
<li>
<p><code>--authentication_v0_timeout</code> - The timeout within which an authentication is
expected to complete against a v0 framework or agent. This does not apply to
the v0 or v1 HTTP APIs.(default: <code>15secs</code>)</p>
</li>
<li>
<p><code>--authenticators</code> - Specifies which authenticator module to use.  The default
is <code>crammd5</code>, but additional modules can be added using the <code>--modules</code>
option.</p>
</li>
<li>
<p><code>--http_authenticators</code> - Specifies which HTTP authenticator module to use.
The default is <code>basic</code> (basic HTTP authentication), but additional modules can
be added using the <code>--modules</code> option.</p>
</li>
<li>
<p><code>--credentials</code> - The path to a text file which contains a list of accepted
credentials.  This may be optional depending on the authenticator being used.</p>
</li>
</ul>
<h3 id="agent-1"><a class="header" href="#agent-1">Agent</a></h3>
<ul>
<li>
<p><code>--authenticatee</code> - Analog to the master's <code>--authenticators</code> option to
specify what module to use.  Defaults to <code>crammd5</code>.</p>
</li>
<li>
<p><code>--credential</code> - Just like the master's <code>--credentials</code> option except that
only one credential is allowed. This credential is used to identify the agent
to the master.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readonly</code> - If <code>true</code>, authentication is required to
make HTTP requests to the read-only HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-only endpoints are those which cannot be used to modify
the state of the agent.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readwrite</code> - If <code>true</code>, authentication is required
to make HTTP requests to the read-write HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-write endpoints are those which can be used to modify the
state of the agent. Note that for backward compatibility reasons, the V1
executor API is not affected by this flag.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_executors</code> - If <code>true</code>, authentication is required
to make HTTP requests to the V1 executor API. If <code>false</code> (the default), that
API can be used without authentication. If this flag is <code>true</code> and custom
HTTP authenticators are not specified, then the default <code>JWT</code> authenticator is
loaded to handle executor authentication.</p>
</li>
<li>
<p><code>--http_authenticators</code> - Specifies which HTTP authenticator module to use.
The default is <code>basic</code>, but additional modules can be added using the
<code>--modules</code> option.</p>
</li>
<li>
<p><code>--http_credentials</code> - The path to a text file which contains a list (in JSON
format) of accepted credentials.  This may be optional depending on the
authenticator being used.</p>
</li>
<li>
<p><code>--authentication_backoff_factor</code> - The agent will time out its authentication
with the master based on exponential backoff. The timeout will be randomly
chosen within the range <code>[min, min + factor*2^n]</code> where <code>n</code> is the number of
failed attempts. To tune these parameters, set the
<code>--authentication_timeout_[min|max|factor]</code> flags. (default: 1secs)</p>
</li>
<li>
<p><code>--authentication_timeout_min</code> - The minimum amount of time the agent waits
before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 5secs)</p>
</li>
<li>
<p><code>--authentication_timeout_max</code> - The maximum amount of time the agent waits
before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 1mins)</p>
</li>
</ul>
<h3 id="scheduler-driver"><a class="header" href="#scheduler-driver">Scheduler Driver</a></h3>
<ul>
<li>
<p><code>--authenticatee</code> - Analog to the master's <code>--authenticators</code> option to
specify what module to use.  Defaults to <code>crammd5</code>.</p>
</li>
<li>
<p><code>--authentication_backoff_factor</code> - The scheduler will time out its
authentication with the master based on exponential backoff. The timeout will
be randomly chosen within the range <code>[min, min + factor*2^n]</code> where <code>n</code> is
the number of failed attempts. To tune these parameters, set the
<code>--authentication_timeout_[min|max|factor]</code> flags. (default: 1secs)</p>
</li>
<li>
<p><code>--authentication_timeout_min</code> - The minimum amount of time the scheduler
waits before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 5secs)</p>
</li>
<li>
<p><code>--authentication_timeout_max</code> - The maximum amount of time the scheduler
waits before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 1mins)</p>
</li>
</ul>
<h3 id="multiple-http-authenticators"><a class="header" href="#multiple-http-authenticators">Multiple HTTP Authenticators</a></h3>
<p>Multiple HTTP authenticators may be loaded into the Mesos master and agent. In
order to load multiple authenticators, specify them as a comma-separated list
using the <code>--http_authenticators</code> flag. The authenticators will be called
serially, and the result of the first successful authentication attempt will be
returned.</p>
<p>If you wish to specify the default basic HTTP authenticator in addition to
custom authenticator modules, add the name <code>basic</code> to your authenticator list.
To specify the default JWT HTTP authenticator in addition to custom
authenticator modules, add the name <code>jwt</code> to your authenticator list.</p>
<h3 id="executor"><a class="header" href="#executor">Executor</a></h3>
<p>If HTTP executor authentication is enabled on the agent, then all requests from
HTTP executors must be authenticated. This includes the default executor, HTTP
command executors, and custom HTTP executors. By default, the agent's JSON web
token (JWT) HTTP authenticator is loaded to handle executor authentication on
both the executor and operator API endpoints. Note that command and custom
executors not using the HTTP API will remain unauthenticated.</p>
<p>When a secret key is loaded via the <code>--jwt_secret_key</code> flag, the agent will
generate a default JWT for each executor before it is launched. This token is
passed into the executor's environment via the
<code>MESOS_EXECUTOR_AUTHENTICATION_TOKEN</code> environment variable. In order to
authenticate with the agent, the executor should place this token into the
<code>Authorization</code> header of all its requests as follows:</p>
<pre><code>    Authorization: Bearer MESOS_EXECUTOR_AUTHENTICATION_TOKEN
</code></pre>
<p>In order to upgrade an existing cluster to require executor authentication, the
following procedure should be followed:</p>
<ol>
<li>
<p>Upgrade all agents, and provide each agent with a cryptographic key via the
<code>--jwt_secret_key</code> flag. This key will be used to sign executor
authentication tokens using the HMAC-SHA256 procedure.</p>
</li>
<li>
<p>Before executor authentication can be enabled successfully, all HTTP
executors must have executor authentication tokens in their environment and
support authentication. To accomplish this, executors which were already
running before the upgrade must be restarted. This could either be done all
at once, or the cluster may be left in this intermediate state while
executors gradually turn over.</p>
</li>
<li>
<p>Once all running default/HTTP command executors have been launched by
upgraded agents, and any custom HTTP executors have been upgraded, the agent
processes can be restarted with the <code>--authenticate_http_executors</code> flag set.
This will enable required HTTP executor authentication, and since all
executors now have authentication tokens and support authentication, their
requests to the agent will authenticate successfully.</p>
</li>
</ol>
<p>Note that HTTP executors make use of the agent operator API in order to make
nested container calls. This means that authentication of the v1 agent operator
API should not be enabled (via <code>--authenticate_http_readwrite</code>) when HTTP
executor authentication is disabled, or HTTP executors will not be able to
function correctly.</p>
<h3 id="framework"><a class="header" href="#framework">Framework</a></h3>
<p>If framework authentication is enabled, each framework must be configured to
supply authentication credentials when registering with the Mesos master. How to
configure this differs between frameworks; consult your framework's
documentation for more information.</p>
<p>As a framework developer, supporting authentication is straightforward: the
scheduler driver handles the details of authentication when a <code>Credential</code>
object is passed to its constructor. To enable <a href="authorization.html">authorization</a>
based on the authenticated principal, the framework developer should also copy
the <code>Credential.principal</code> into <code>FrameworkInfo.principal</code> when registering.</p>
<h2 id="cram-md5-example"><a class="header" href="#cram-md5-example">CRAM-MD5 Example</a></h2>
<ol>
<li>
<p>Create the master's credentials file with the following content:</p>
<pre><code> {
   &quot;credentials&quot; : [
     {
       &quot;principal&quot;: &quot;principal1&quot;,
       &quot;secret&quot;: &quot;secret1&quot;
     },
     {
       &quot;principal&quot;: &quot;principal2&quot;,
       &quot;secret&quot;: &quot;secret2&quot;
     }
   ]
 }
</code></pre>
</li>
<li>
<p>Start the master using the credentials file (assuming the file is <code>/home/user/credentials</code>):</p>
<pre><code> ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos --authenticate --authenticate_agents --credentials=/home/user/credentials
</code></pre>
</li>
<li>
<p>Create another file with a single credential in it (<code>/home/user/agent_credential</code>):</p>
<pre><code> {
   &quot;principal&quot;: &quot;principal1&quot;,
   &quot;secret&quot;: &quot;secret1&quot;
 }
</code></pre>
</li>
<li>
<p>Start the agent:</p>
<pre><code> ./bin/mesos-agent.sh --master=127.0.0.1:5050 --credential=/home/user/agent_credential
</code></pre>
</li>
<li>
<p>Your new agent should have now successfully authenticated with the master.</p>
</li>
<li>
<p>You can test out framework authentication using one of the test frameworks
provided with Mesos as follows:</p>
<pre><code> MESOS_AUTHENTICATE=true DEFAULT_PRINCIPAL=principal2 DEFAULT_SECRET=secret2 ./src/test-framework --master=127.0.0.1:5050
</code></pre>
</li>
</ol>
<hr />
<h2>title: Apache Mesos - Authorization
layout: documentation</h2>
<h1 id="authorization-1"><a class="header" href="#authorization-1">Authorization</a></h1>
<p>In Mesos, the authorization subsystem allows the operator to configure the
actions that certain principals are allowed to perform. For example, the
operator can use authorization to ensure that principal <code>foo</code> can only register
frameworks subscribed to role <code>bar</code>, and no other principals can register
frameworks subscribed to any roles.</p>
<p>A reference implementation <em>local authorizer</em> provides basic security for most
use cases. This authorizer is configured using Access Control Lists (ACLs).
Alternative implementations could express their authorization rules in
different ways. The local authorizer is used if the
<a href="configuration/master.html"><code>--authorizers</code></a> flag is not specified (or manually set to
the default value <code>local</code>) and ACLs are specified via the
<a href="configuration.html"><code>--acls</code></a> flag.</p>
<p>This document is divided into two main sections. The first section explores the
concepts necessary to successfully configure the local authorizer. The second
briefly discusses how to implement a custom authorizer; this section is not
directed at operators but at engineers who wish to build their own authorizer
back end.</p>
<h2 id="http-executor-authorization"><a class="header" href="#http-executor-authorization">HTTP Executor Authorization</a></h2>
<p>When the agent's <code>--authenticate_http_executors</code> flag is set, HTTP executors are
required to authenticate with the HTTP executor API. When they do so, a simple
implicit authorization rule is applied. In plain language, the rule states that
executors can only perform actions on themselves. More specifically, an
executor's authenticated principal must contain claims with keys <code>fid</code>, <code>eid</code>,
and <code>cid</code>, with values equal to the currently-running executor's framework ID,
executor ID, and container ID, respectively. By default, an authentication token
containing these claims is injected into the executor's environment (see the
<a href="authentication.html">authentication documentation</a> for more information).</p>
<p>Similarly, when the agent's <code>--authenticate_http_readwrite</code> flag is set, HTTP
executor's are required to authenticate with the HTTP operator API when making
calls such as <code>LAUNCH_NESTED_CONTAINER</code>. In this case, executor authorization is
performed via the loaded authorizer module, if present. The default Mesos local
authorizer applies a simple implicit authorization rule, requiring that the
executor's principal contain a claim with key <code>cid</code> and a value equal to the
currently-running executor's container ID.</p>
<h2 id="local-authorizer"><a class="header" href="#local-authorizer">Local Authorizer</a></h2>
<h3 id="role-vs-principal-1"><a class="header" href="#role-vs-principal-1">Role vs. Principal</a></h3>
<p>A principal identifies an entity (i.e., a framework or an operator) that
interacts with Mesos. A role, on the other hand, is used to associate resources
with frameworks in various ways. A useful analogy can be made with user
management in the Unix world: principals correspond to usernames, while roles
approximately correspond to groups. For more information about roles, see the
<a href="roles.html">roles documentation</a>.</p>
<p>In a real-world organization, principals and roles might be used to represent
various individuals or groups; for example, principals could correspond to
people responsible for particular frameworks, while roles could correspond to
departments within the organization which run frameworks on the cluster. To
illustrate this point, consider a company that wants to allocate datacenter
resources amongst multiple departments, one of which is the accounting
department. Here is a possible scenario in which the accounting department
launches a Mesos framework and then attempts to destroy a persistent volume:</p>
<ul>
<li>An accountant launches their framework, which authenticates with the Mesos
master using its <code>principal</code> and <code>secret</code>. Here, let the framework principal
be <code>payroll-framework</code>; this principal represents the trusted identity of the
framework.</li>
<li>The framework now sends a registration message to the master. This message
includes a <code>FrameworkInfo</code> object containing a <code>principal</code> and <code>roles</code>; in
this case, it will use a single role named <code>accounting</code>. The principal in
this message must be <code>payroll-framework</code>, to match the one used by the
framework for authentication.</li>
<li>The master consults the local authorizer, which in turn looks through its ACLs
to see if it has a <code>RegisterFramework</code> ACL which authorizes the principal
<code>payroll-framework</code> to register with the <code>accounting</code> role. It does find such
an ACL, the framework registers successfully. Now that the framework is
subscribed to the <code>accounting</code> role, any <a href="weights.html">weights</a>,
<a href="reservation.html">reservations</a>, <a href="persistent-volume.html">persistent volumes</a>,
or <a href="quota.html">quota</a> associated with the accounting department's role will
apply when allocating resources to this role within the framework. This
allows operators to control the resource consumption of this department.</li>
<li>Suppose the framework has created a persistent volume on an agent which it
now wishes to destroy. The framework sends an <code>ACCEPT</code> call containing an
offer operation which will <code>DESTROY</code> the persistent volume.</li>
<li>However, datacenter operators have decided that they don't want the accounting
frameworks to delete volumes. Rather, the operators will manually remove the
accounting department's persistent volumes to ensure that no important
financial data is deleted accidentally. To accomplish this, they have set a
<code>DestroyVolume</code> ACL which asserts that the principal <code>payroll-framework</code> can
destroy volumes created by a <code>creator_principal</code> of <code>NONE</code>; in other words,
this framework cannot destroy persistent volumes, so the operation will be
refused.</li>
</ul>
<h3 id="acls"><a class="header" href="#acls">ACLs</a></h3>
<p>When authorizing an action, the local authorizer proceeds through a list of
relevant rules until it finds one that can either grant or deny permission to
the subject making the request. These rules are configured with Access Control
Lists (ACLs) in the case of the local authorizer. The ACLs are defined with a
JSON-based language via the <a href="configuration.html"><code>--acls</code></a> flag.</p>
<p>Each ACL consist of an array of JSON objects. Each of these objects has two
entries. The first, <code>principals</code>, is common to all actions and describes the
subjects which wish to perform the given action. The second entry varies among
actions and describes the object on which the action will be executed. Both
entries are specified with the same type of JSON object, known as <code>Entity</code>. The
local authorizer works by comparing <code>Entity</code> objects, so understanding them is
key to writing good ACLs.</p>
<p>An <code>Entity</code> is essentially a container which can either hold a particular value
or specify the special types <code>ANY</code> or <code>NONE</code>.</p>
<p>A global field which affects all ACLs can be set. This field is called
<code>permissive</code> and it defines the behavior when no ACL applies to the request
made. If set to <code>true</code> (which is the default) it will allow by default all
non-matching requests, if set to <code>false</code> it will reject all non-matching
requests.</p>
<p>Note that when setting <code>permissive</code> to <code>false</code> a number of standard operations
(e.g., <code>run_tasks</code> or <code>register_frameworks</code>) will require ACLs in order to work.
There are two ways to disallow unauthorized uses on specific operations:</p>
<ol>
<li>
<p>Leave <code>permissive</code> set to <code>true</code> and disallow <code>ANY</code> principal to perform
actions to all objects except the ones explicitly allowed.
Consider the <a href="authorization.html#disallowExample">example below</a> for details.</p>
</li>
<li>
<p>Set <code>permissive</code> to <code>false</code> but allow <code>ANY</code> principal to perform the
action on <code>ANY</code> object. This needs to be done for all actions which should
work without being checked against ACLs. A template doing this for all
actions can be found in <a href="../examples/acls_template.json">acls_template.json</a>.</p>
</li>
</ol>
<p>More information about the structure of the ACLs can be found in
<a href="https://github.com/apache/mesos/blob/master/include/mesos/authorizer/acls.proto">their definition</a>
inside the Mesos source code.</p>
<p>ACLs are compared in the order that they are specified. In other words,
if an ACL allows some action and a later ACL forbids it, the action is
allowed; likewise, if the ACL forbidding the action appears earlier than the
one allowing the action, the action is forbidden. If no ACLs match a request,
the request is authorized if the ACLs are permissive (which is the default
behavior). If <code>permissive</code> is explicitly set to false, all non-matching requests
are declined.</p>
<h3 id="authorizable-actions"><a class="header" href="#authorizable-actions">Authorizable Actions</a></h3>
<p>Currently, the local authorizer configuration format supports the following
entries, each representing an authorizable action:</p>
<table class="table table-striped">
<thead>
<tr>
  <th>Action Name</th>
  <th>Subject</th>
  <th>Object</th>
  <th>Description</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>register_frameworks</code></td>
  <td>Framework principal.</td>
  <td>Resource <a href="roles.html">roles</a> of
      the framework.
  </td>
  <td>(Re-)registering of frameworks.</td>
</tr>
<tr>
  <td><code>run_tasks</code></td>
  <td>Framework principal.</td>
  <td>UNIX user to launch the task as.</td>
  <td>Launching tasks/executors by a framework.</td>
</tr>
<tr>
  <td><code>teardown_frameworks</code></td>
  <td>Operator username.</td>
  <td>Principals whose frameworks can be shutdown by the operator.</td>
  <td>Tearing down frameworks.</td>
</tr>
<tr>
  <td><code>reserve_resources</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Resource role of the reservation.</td>
  <td><a href="reservation.html">Reserving</a> resources.</td>
</tr>
<tr>
  <td><code>unreserve_resources</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Principals whose resources can be unreserved by the operator.</td>
  <td><a href="reservation.html">Unreserving</a> resources.</td>
</tr>
<tr>
  <td><code>create_volumes</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Resource role of the volume.</td>
  <td>Creating
      <a href="persistent-volume.html">volumes</a>.
  </td>
</tr>
<tr>
  <td><code>destroy_volumes</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Principals whose volumes can be destroyed by the operator.</td>
  <td>Destroying
      <a href="persistent-volume.html">volumes</a>.
  </td>
</tr>
<tr>
  <td><code>resize_volume</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Resource role of the volume.</td>
  <td>Growing or shrinking
      <a href="persistent-volume.html">persistent volumes</a>.
  </td>
</tr>
<tr>
  <td><code>create_block_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the block disk.</td>
  <td>Creating a block disk.</td>
</tr>
<tr>
  <td><code>destroy_block_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the block disk.</td>
  <td>Destroying a block disk.</td>
</tr>
<tr>
  <td><code>create_mount_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the mount disk.</td>
  <td>Creating a mount disk.</td>
</tr>
<tr>
  <td><code>destroy_mount_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the mount disk.</td>
  <td>Destroying a mount disk.</td>
</tr>
<tr>
  <td><code>get_quotas</code></td>
  <td>Operator username.</td>
  <td>Resource role whose quota status will be queried.</td>
  <td>Querying <a href="quota.html">quota</a> status.</td>
</tr>
<tr>
  <td><code>update_quotas</code></td>
  <td>Operator username.</td>
  <td>Resource role whose quota will be updated.</td>
  <td>Modifying <a href="quota.html">quotas</a>.</td>
</tr>
<tr>
  <td><code>view_roles</code></td>
  <td>Operator username.</td>
  <td>Resource roles whose information can be viewed by the operator.</td>
  <td>Querying <a href="roles.html">roles</a>
      and <a href="weights.html">weights</a>.
  </td>
</tr>
<tr>
  <td><code>get_endpoints</code></td>
  <td>HTTP username.</td>
  <td>HTTP endpoints the user should be able to access using the HTTP "GET"
      method.</td>
  <td>Performing an HTTP "GET" on an endpoint.</td>
</tr>
<tr>
  <td><code>update_weights</code></td>
  <td>Operator username.</td>
  <td>Resource roles whose weights can be updated by the operator.</td>
  <td>Updating <a href="weights.html">weights</a>.</td>
</tr>
<tr>
  <td><code>view_frameworks</code></td>
  <td>HTTP user.</td>
  <td>UNIX user of whom executors can be viewed.</td>
  <td>Filtering http endpoints.</td>
</tr>
<tr>
  <td><code>view_executors</code></td>
  <td>HTTP user.</td>
  <td>UNIX user of whom executors can be viewed.</td>
  <td>Filtering http endpoints.</td>
</tr>
<tr>
  <td><code>view_tasks</code></td>
  <td>HTTP user.</td>
  <td>UNIX user of whom executors can be viewed.</td>
  <td>Filtering http endpoints.</td>
</tr>
<tr>
  <td><code>access_sandboxes</code></td>
  <td>Operator username.</td>
  <td>Operating system user whose executor/task sandboxes can be accessed.</td>
  <td>Access task sandboxes.</td>
</tr>
<tr>
  <td><code>access_mesos_logs</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>Access Mesos logs.</td>
</tr>
<tr>
  <td><code>register_agents</code></td>
  <td>Agent principal.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      agent (re-)registration.
  </td>
  <td>(Re-)registration of agents.</td>
</tr>
<tr>
  <td><code>get_maintenance_schedules</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>View the maintenance schedule of the machines used by Mesos.</td>
</tr>
<tr>
  <td><code>update_maintenance_schedules</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>Modify the maintenance schedule of the machines used by Mesos.</td>
</tr>
<tr>
  <td><code>start_maintenances</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>Starts maintenance on a machine. This will make a machine and its agents
      unavailable.
  </td>
</tr>
<tr>
  <td><code>stop_maintenances</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use the types ANY and NONE to
      allow/deny access to the log.
  </td>
  <td>Ends maintenance on a machine.</td>
</tr>
<tr>
  <td><code>get_maintenance_statuses</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use the types ANY and NONE to
      allow/deny access to the log.
  </td>
  <td>View if a machine is in maintenance or not.</td>
</tr>
</tbody>
</table>
<h3 id="authorizable-http-endpoints"><a class="header" href="#authorizable-http-endpoints">Authorizable HTTP endpoints</a></h3>
<p>The <code>get_endpoints</code> action covers:</p>
<ul>
<li><code>/files/debug</code></li>
<li><code>/logging/toggle</code></li>
<li><code>/metrics/snapshot</code></li>
<li><code>/slave(id)/containers</code></li>
<li><code>/slave(id)/containerizer/debug</code></li>
<li><code>/slave(id)/monitor/statistics</code></li>
</ul>
<h3 id="examples-4"><a class="header" href="#examples-4">Examples</a></h3>
<p>Consider for example the following ACL: Only principal <code>foo</code> can register
frameworks subscribed to the <code>analytics</code> role. All principals can register
frameworks subscribing to any other roles (including the principal <code>foo</code>
since permissive is the default behavior).</p>
<pre><code class="language-json">{
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;type&quot;: &quot;NONE&quot;
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>Principal <code>foo</code> can register frameworks subscribed to the <code>analytics</code> and
<code>ads</code> roles and no other role. Any other principal (or framework without
a principal) can register frameworks subscribed to any roles.</p>
<pre><code class="language-json">{
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;, &quot;ads&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;type&quot;: &quot;NONE&quot;
                             }
                           }
                         ]
}
</code></pre>
<p>Only principal <code>foo</code> and no one else can register frameworks subscribed to the
<code>analytics</code> role. Any other principal (or framework without a principal) can
register frameworks subscribed to any other roles.</p>
<pre><code class="language-json">{
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;type&quot;: &quot;NONE&quot;
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>Principal <code>foo</code> can register frameworks subscribed to the <code>analytics</code> role
and no other roles. No other principal can register frameworks subscribed to
any roles, including <code>*</code>.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>In the following example <code>permissive</code> is set to <code>false</code>; hence, principals can
only run tasks as operating system users <code>guest</code> or <code>bar</code>, but not as any other
user.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;type&quot;: &quot;ANY&quot; },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;guest&quot;, &quot;bar&quot;] }
                 }
               ]
}
</code></pre>
<p>Principals <code>foo</code> and <code>bar</code> can run tasks as the agent operating system user
<code>alice</code> and no other user. No other principal can run tasks.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;values&quot;: [&quot;foo&quot;, &quot;bar&quot;] },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;alice&quot;] }
                 }
               ]
}
</code></pre>
<p>Principal <code>foo</code> can run tasks only as the agent operating system user <code>guest</code>
and no other user. Any other principal (or framework without a principal) can
run tasks as any user.</p>
<pre><code class="language-json">{
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;values&quot;: [&quot;foo&quot;] },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;guest&quot;] }
                 },
                 {
                   &quot;principals&quot;: { &quot;values&quot;: [&quot;foo&quot;] },
                   &quot;users&quot;: { &quot;type&quot;: &quot;NONE&quot; }
                 }
               ]
}
</code></pre>
<p>No principal can run tasks as the agent operating system user <code>root</code>. Any
principal (or framework without a principal) can run tasks as any other user.</p>
<pre><code class="language-json">{
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;type&quot;: &quot;NONE&quot; },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;root&quot;] }
                 }
               ]
}
</code></pre>
<p>The order in which the rules are defined is important. In the following
example, the ACLs effectively forbid anyone from tearing down frameworks even
though the intention clearly is to allow only <code>admin</code> to shut them down:</p>
<pre><code class="language-json">{
  &quot;teardown_frameworks&quot;: [
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;NONE&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           },
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;admin&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           }
                         ]
}
</code></pre>
<p><a name="disallowExample"></a>
The previous ACL can be fixed as follows:</p>
<pre><code class="language-json">{
  &quot;teardown_frameworks&quot;: [
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;admin&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           },
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;NONE&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           }
                         ]
}
</code></pre>
<p>The <code>ops</code> principal can teardown any framework using the
<a href="endpoints/master/teardown.html">/teardown</a> HTTP endpoint. No other principal can
teardown any frameworks.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;teardown_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;ops&quot;]
                             },
                             &quot;framework_principals&quot;: {
                               &quot;type&quot;: &quot;ANY&quot;
                             }
                           }
                         ]
}
</code></pre>
<p>The principal <code>foo</code> can reserve resources for any role, and no other principal
can reserve resources.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;reserve_resources&quot;: [
                         {
                           &quot;principals&quot;: {
                             &quot;values&quot;: [&quot;foo&quot;]
                           },
                           &quot;roles&quot;: {
                             &quot;type&quot;: &quot;ANY&quot;
                           }
                         }
                       ]
}
</code></pre>
<p>The principal <code>foo</code> cannot reserve resources, and any other principal (or
framework without a principal) can reserve resources for any role.</p>
<pre><code class="language-json">{
  &quot;reserve_resources&quot;: [
                         {
                           &quot;principals&quot;: {
                             &quot;values&quot;: [&quot;foo&quot;]
                           },
                           &quot;roles&quot;: {
                             &quot;type&quot;: &quot;NONE&quot;
                           }
                         }
                       ]
}
</code></pre>
<p>The principal <code>foo</code> can reserve resources only for roles <code>prod</code> and <code>dev</code>, and
no other principal (or framework without a principal) can reserve resources for
any role.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;reserve_resources&quot;: [
                         {
                           &quot;principals&quot;: {
                             &quot;values&quot;: [&quot;foo&quot;]
                           },
                           &quot;roles&quot;: {
                             &quot;values&quot;: [&quot;prod&quot;, &quot;dev&quot;]
                           }
                         }
                       ]
}
</code></pre>
<p>The principal <code>foo</code> can unreserve resources reserved by itself and by the
principal <code>bar</code>. The principal <code>bar</code>, however, can only unreserve its own
resources. No other principal can unreserve resources.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;unreserve_resources&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;reserver_principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;, &quot;bar&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;bar&quot;]
                             },
                             &quot;reserver_principals&quot;: {
                               &quot;values&quot;: [&quot;bar&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>The principal <code>foo</code> can create persistent volumes for any role, and no other
principal can create persistent volumes.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;create_volumes&quot;: [
                      {
                        &quot;principals&quot;: {
                          &quot;values&quot;: [&quot;foo&quot;]
                        },
                        &quot;roles&quot;: {
                          &quot;type&quot;: &quot;ANY&quot;
                        }
                      }
                    ]
}
</code></pre>
<p>The principal <code>foo</code> cannot create persistent volumes for any role, and any
other principal can create persistent volumes for any role.</p>
<pre><code class="language-json">{
  &quot;create_volumes&quot;: [
                      {
                        &quot;principals&quot;: {
                          &quot;values&quot;: [&quot;foo&quot;]
                        },
                        &quot;roles&quot;: {
                          &quot;type&quot;: &quot;NONE&quot;
                        }
                      }
                    ]
}
</code></pre>
<p>The principal <code>foo</code> can create persistent volumes only for roles <code>prod</code> and
<code>dev</code>, and no other principal can create persistent volumes for any role.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;create_volumes&quot;: [
                      {
                        &quot;principals&quot;: {
                          &quot;values&quot;: [&quot;foo&quot;]
                        },
                        &quot;roles&quot;: {
                          &quot;values&quot;: [&quot;prod&quot;, &quot;dev&quot;]
                        }
                      }
                    ]
}
</code></pre>
<p>The principal <code>foo</code> can destroy volumes created by itself and by the principal
<code>bar</code>. The principal <code>bar</code>, however, can only destroy its own volumes. No other
principal can destroy volumes.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;destroy_volumes&quot;: [
                       {
                         &quot;principals&quot;: {
                           &quot;values&quot;: [&quot;foo&quot;]
                         },
                         &quot;creator_principals&quot;: {
                           &quot;values&quot;: [&quot;foo&quot;, &quot;bar&quot;]
                         }
                       },
                       {
                         &quot;principals&quot;: {
                           &quot;values&quot;: [&quot;bar&quot;]
                         },
                         &quot;creator_principals&quot;: {
                           &quot;values&quot;: [&quot;bar&quot;]
                         }
                       }
                     ]
}
</code></pre>
<p>The principal <code>ops</code> can query quota status for any role. The principal <code>foo</code>,
however, can only query quota status for <code>foo-role</code>. No other principal can
query quota status.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;get_quotas&quot;: [
                  {
                    &quot;principals&quot;: {
                      &quot;values&quot;: [&quot;ops&quot;]
                    },
                    &quot;roles&quot;: {
                      &quot;type&quot;: &quot;ANY&quot;
                    }
                  },
                  {
                    &quot;principals&quot;: {
                      &quot;values&quot;: [&quot;foo&quot;]
                    },
                    &quot;roles&quot;: {
                      &quot;values&quot;: [&quot;foo-role&quot;]
                    }
                  }
                ]
}
</code></pre>
<p>The principal <code>ops</code> can update quota information (set or remove) for any role.
The principal <code>foo</code>, however, can only update quota for <code>foo-role</code>. No other
principal can update quota.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;update_quotas&quot;: [
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;ops&quot;]
                       },
                       &quot;roles&quot;: {
                         &quot;type&quot;: &quot;ANY&quot;
                       }
                     },
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;foo&quot;]
                       },
                       &quot;roles&quot;: {
                         &quot;values&quot;: [&quot;foo-role&quot;]
                       }
                     }
                   ]
}
</code></pre>
<p>The principal <code>ops</code> can reach all HTTP endpoints using the <em>GET</em>
method. The principal <code>foo</code>, however, can only use the HTTP <em>GET</em> on
the <code>/logging/toggle</code> and <code>/monitor/statistics</code> endpoints.  No other
principals can use <em>GET</em> on any endpoints.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;get_endpoints&quot;: [
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;ops&quot;]
                       },
                       &quot;paths&quot;: {
                         &quot;type&quot;: &quot;ANY&quot;
                       }
                     },
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;foo&quot;]
                       },
                       &quot;paths&quot;: {
                         &quot;values&quot;: [&quot;/logging/toggle&quot;, &quot;/monitor/statistics&quot;]
                       }
                     }
                   ]
}
</code></pre>
<h2 id="implementing-an-authorizer"><a class="header" href="#implementing-an-authorizer">Implementing an Authorizer</a></h2>
<p>In case you plan to implement your own authorizer <a href="modules.html">module</a>, the
authorization interface consists of three parts:</p>
<p>First, the <code>authorization::Request</code> protobuf message represents a request to be
authorized. It follows the
<em><a href="https://en.wikipedia.org/wiki/Subject%E2%80%93verb%E2%80%93object">Subject-Verb-Object</a></em>
pattern, where a <em>subject</em> ---commonly a principal---attempts to perform an
<em>action</em> on a given <em>object</em>.</p>
<p>Second, the
<code>Future&lt;bool&gt; mesos::Authorizer::authorized(const mesos::authorization::Request&amp; request)</code>
interface defines the entry point for authorizer modules (and the local
authorizer). A call to <code>authorized()</code> returns a future that indicates the result
of the (asynchronous) authorization operation. If the future is set to true, the
request was authorized successfully; if it was set to false, the request was
rejected. A failed future indicates that the request could not be processed at
the moment and it can be retried later.</p>
<p>The <code>authorization::Request</code> message is defined in authorizer.proto:</p>
<pre><code class="language-protoc">message Request {
  optional Subject subject = 1;
  optional Action  action  = 2;
  optional Object  object  = 3;
}

message Subject {
  optional string value = 1;
}

message Object {
  optional string value = 1;
  optional FrameworkInfo framework_info = 2;
  optional Task task = 3;
  optional TaskInfo task_info = 4;
  optional ExecutorInfo executor_info = 5;
  optional MachineID machine_id = 11;
}
</code></pre>
<p><code>Subject</code> or <code>Object</code> are optional fiels; if they are not set they
will only match an ACL with ANY or NONE in the
corresponding location. This allows users to construct the following requests:
<em>Can everybody perform action <strong>A</strong> on object <strong>O</strong>?</em>, or <em>Can principal <strong>Z</strong>
execute action <strong>X</strong> on all objects?</em>.</p>
<p><code>Object</code> has several optional fields of which, depending on the action,
one or more fields must be set
(e.g., the <code>view_executors</code> action expects the <code>executor_info</code> and
<code>framework_info</code> to be set).</p>
<p>The <code>action</code> field of the <code>Request</code> message is an enum. It is kept optional---
even though a valid action is necessary for every request---to allow for
backwards compatibility when adding new fields (see
<a href="https://issues.apache.org/jira/browse/MESOS-4997">MESOS-4997</a> for details).</p>
<p>Third, the <code>ObjectApprover</code> interface. In order to support efficient
authorization of large objects and multiple objects a user can request an
<code>ObjectApprover</code> via
<code>Future&lt;shared_ptr&lt;const ObjectApprover&gt;&gt; getApprover(const authorization::Subject&amp; subject, const authorization::Action&amp; action)</code>.
The resulting <code>ObjectApprover</code> provides
<code>Try&lt;bool&gt; approved(const ObjectApprover::Object&amp; object)</code> to synchronously
check whether objects are authorized. The <code>ObjectApprover::Object</code> follows the
structure of the <code>Request::Object</code> above.</p>
<pre><code class="language-cpp">struct Object
{
  const std::string* value;
  const FrameworkInfo* framework_info;
  const Task* task;
  const TaskInfo* task_info;
  const ExecutorInfo* executor_info;
  const MachineID* machine_id;
};
</code></pre>
<p>As the fields take pointer to each entity the <code>ObjectApprover::Object</code> does not
require the entity to be copied.</p>
<p>Authorizer must ensure that <code>ObjectApprover</code>s returned by <code>getApprover(...)</code> method
are valid throughout their whole lifetime. This is relied upon by parts of Mesos code
(Scheduler API, Operator API events and so on) that have a need to frequently authorize
a limited number of long-lived authorization subjects.
This code on the Mesos side, on its part, must ensure that it does not store
<code>ObjectApprover</code> for authorization subjects that it no longer uses (i.e. that it
does not leak <code>ObjectApprover</code>s).</p>
<p>NOTE: As the <code>ObjectApprover</code> is run synchronously in a different actor process
<code>ObjectApprover.approved()</code> call must not block!</p>
<hr />
<h2>title: Apache Mesos - SSL in Mesos
layout: documentation</h2>
<h1 id="ssl-in-mesos"><a class="header" href="#ssl-in-mesos">SSL in Mesos</a></h1>
<p>By default, all the messages that flow through the Mesos cluster are
unencrypted, making it possible for anyone with access to the cluster to
intercept and potentially control arbitrary tasks.</p>
<p>SSL/TLS support was added to libprocess in Mesos 0.23.0, which encrypts the
data that Mesos uses for network communication between Mesos components.
Additionally, HTTPS support was added to the Mesos WebUI.</p>
<h1 id="build-configuration"><a class="header" href="#build-configuration">Build Configuration</a></h1>
<p>There are currently two implementations of the
<a href="https://github.com/apache/mesos/blob/master/3rdparty/libprocess/include/process/socket.hpp">libprocess socket interface</a>
that support SSL.</p>
<p>The first implementation, added in Mesos 0.23.0, uses
<a href="https://github.com/libevent/libevent">libevent</a>.
Specifically it relies on the <code>libevent-openssl</code> library that wraps <code>openssl</code>.</p>
<p>The second implementation, added in Mesos 1.10.0, is a generic socket
wrapper which only relies on the OpenSSL (1.1+) library.</p>
<p>Before building Mesos from source, assuming you have installed the
required <a href="ssl.html#Dependencies">Dependencies</a>, you can modify your configure line
to enable SSL as follows:</p>
<pre><code>../configure --enable-ssl
# Or:
../configure --enable-libevent --enable-ssl
</code></pre>
<h1 id="runtime-configuration"><a class="header" href="#runtime-configuration">Runtime Configuration</a></h1>
<p>TLS support in Mesos can be configured with different levels of security. This section aims to help
Mesos operators to better understand the trade-offs involved in them.</p>
<p>On a high level, one can imagine to choose between three available layers of security, each
providing additional security guarantees but also increasing the deployment complexity.</p>
<ol>
<li>
<p><code>LIBPROCESS_SSL_ENABLED=true</code>. This provides external clients (e.g. curl) with the ability to
connect to Mesos HTTP endpoints securely via TLS, verifying that the server certificate is valid
and trusted.</p>
</li>
<li>
<p><code>LIBPROCESS_SSL_VERIFY_SERVER_CERT=true</code>. In addition to the above, this ensures that Mesos components
themselves are verifying the presence of valid and trusted server certificates when making
outgoing connections. This prevents man-in-the-middle attacks on communications between Mesos
components, and on communications between a Mesos component and an external server.</p>
<p><strong>WARNING:</strong> This setting only makes sense if <code>LIBPROCESS_SSL_ENABLE_DOWNGRADE</code> is set
to <code>false</code>, otherwise a malicious actor can simply bypass certificate verification by
downgrading to a non-TLS connection.</p>
</li>
<li>
<p><code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT=true</code>. In addition to the above, this enforces the use of TLS
client certificates on all connections to any Mesos component. This ensures that only trusted
clients can connect to any Mesos component, preventing reception of forged or malformed messages.</p>
<p>This implies that all schedulers or other clients (including the web browsers used by human
operators) that are supposed to connect to any endpoint of a Mesos component must be provided
with valid client certificates.</p>
<p><strong>WARNING:</strong> As above, this setting only makes sense if <code>LIBPROCESS_SSL_ENABLE_DOWNGRADE</code> is
set to <code>false</code>.</p>
</li>
</ol>
<p>For secure usage, it is recommended to set <code>LIBPROCESS_SSL_ENABLED=true</code>,
<code>LIBPROCESS_SSL_VERIFY_SERVER_CERT=true</code>, <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME=openssl</code>
and <code>LIBPROCESS_SSL_ENABLE_DOWNGRADE=false</code>. This provides a good trade-off
between security and usability.</p>
<p>It is not recommended in general to expose Mesos components to the public internet, but in cases
where they are the use of <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code> is strongly suggested.</p>
<h1 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h1>
<p>Once you have successfully built and installed your new binaries, here are the environment variables that are applicable to the <code>Master</code>, <code>Agent</code>, <code>Framework Scheduler/Executor</code>, or any <code>libprocess process</code>:</p>
<p><strong>NOTE:</strong> Prior to 1.0, the SSL related environment variables used to be prefixed by <code>SSL_</code>. However, we found that they may collide with other programs and lead to unexpected results (e.g., openssl, see <a href="https://issues.apache.org/jira/browse/MESOS-5863">MESOS-5863</a> for details). To be backward compatible, we accept environment variables prefixed by both <code>SSL_</code> or <code>LIBPROCESS_SSL_</code>. New users should use the <code>LIBPROCESS_SSL_</code> version.</p>
<h4 id="libprocess_ssl_enabledfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enabledfalse0true1-defaultfalse0">LIBPROCESS_SSL_ENABLED=(false|0,true|1) [default=false|0]</a></h4>
<p>Turn on or off SSL. When it is turned off it is the equivalent of default Mesos with libevent as the backing for events. All sockets default to the non-SSL implementation. When it is turned on, the default configuration for sockets is SSL. This means outgoing connections will use SSL, and incoming connections will be expected to speak SSL as well. None of the below flags are relevant if SSL is not enabled.  If SSL is enabled, <code>LIBPROCESS_SSL_CERT_FILE</code> and <code>LIBPROCESS_SSL_KEY_FILE</code> must be supplied.</p>
<h4 id="libprocess_ssl_support_downgradefalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_support_downgradefalse0true1-defaultfalse0">LIBPROCESS_SSL_SUPPORT_DOWNGRADE=(false|0,true|1) [default=false|0]</a></h4>
<p>Control whether or not non-SSL connections can be established. If this is enabled
<strong>on the accepting side</strong>, then the accepting side will downgrade to a non-SSL socket if the
connecting side is attempting to communicate via non-SSL. (e.g. HTTP).</p>
<p>If this is enabled <strong>on the connecting side</strong>, then the connecting side will retry on a non-SSL
socket if establishing the SSL connection failed.</p>
<p>See <a href="ssl.html#Upgrading">Upgrading Your Cluster</a> for more details.</p>
<h4 id="libprocess_ssl_key_filepath-to-key"><a class="header" href="#libprocess_ssl_key_filepath-to-key">LIBPROCESS_SSL_KEY_FILE=(path to key)</a></h4>
<p>The location of the private key used by OpenSSL.</p>
<pre><code>// For example, to generate a key with OpenSSL:
openssl genrsa -des3 -f4 -passout pass:some_password -out key.pem 4096
</code></pre>
<h4 id="libprocess_ssl_cert_filepath-to-certificate"><a class="header" href="#libprocess_ssl_cert_filepath-to-certificate">LIBPROCESS_SSL_CERT_FILE=(path to certificate)</a></h4>
<p>The location of the certificate that will be presented.</p>
<pre><code>// For example, to generate a root certificate with OpenSSL:
// (assuming the signing key already exists in `key.pem`)
openssl req -new -x509 -passin pass:some_password -days 365 -keyout key.pem -out cert.pem
</code></pre>
<h4 id="libprocess_ssl_verify_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_verify_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_VERIFY_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This is a legacy alias for the <code>LIBPROCESS_SSL_VERIFY_SERVER_CERT</code> setting.</p>
<h4 id="libprocess_ssl_verify_server_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_verify_server_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_VERIFY_SERVER_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This setting only affects the behaviour of libprocess in TLS client mode.</p>
<p>If this is true, a remote server is required to present a server certificate,
and the presented server certificates will be verified. That means
it will be checked that the certificate is cryptographically valid,
was generated by a trusted CA, and contains the correct hostname.</p>
<p>If this is false, a remote server is still required to present a server certificate (unless
an anonymous cipher is used), but the presented server certificates will not be verified.</p>
<p><strong>NOTE:</strong> When <code>LIBPROCESS_SSL_REQUIRE_CERT</code> is true, <code>LIBPROCESS_SSL_VERIFY_CERT</code> is automatically
set to true for backwards compatibility reasons.</p>
<h4 id="libprocess_ssl_require_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_require_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_REQUIRE_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This is a legacy alias for the <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code> setting.</p>
<h4 id="libprocess_ssl_require_client_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_require_client_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_REQUIRE_CLIENT_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This setting only affects the behaviour of libprocess in TLS server mode.</p>
<p>If this is true, enforce that certificates must be presented by connecting clients. This means all
connections (including external tooling trying to access HTTP endpoints, like web browsers etc.)
must present valid certificates in order to establish a connection.</p>
<p><strong>NOTE:</strong> The specifics of what it means for the certificate to &quot;contain the correct hostname&quot;
depend on the selected value of <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME</code>.</p>
<p><strong>NOTE:</strong> If this is set to false, client certificates are not verified even if they are presented
and <code>LIBPROCESS_SSL_VERIFY_CERT</code> is set to true.</p>
<h4 id="libprocess_ssl_verify_depthn-default4"><a class="header" href="#libprocess_ssl_verify_depthn-default4">LIBPROCESS_SSL_VERIFY_DEPTH=(N) [default=4]</a></h4>
<p>The maximum depth used to verify certificates. The default is 4. See the OpenSSL documentation or contact your system administrator to learn why you may want to change this.</p>
<h4 id="libprocess_ssl_verify_ipaddfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_verify_ipaddfalse0true1-defaultfalse0">LIBPROCESS_SSL_VERIFY_IPADD=(false|0,true|1) [default=false|0]</a></h4>
<p>Enable IP address verification in the certificate subject alternative name extension. When set
to <code>true</code> the peer certificate verification will be able to use the IP address of a peer connection.</p>
<p>The specifics on when a certificate containing an IP address will we accepted depend on the
selected value of the <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME</code>.</p>
<h4 id="libprocess_ssl_ca_dirpath-to-ca-directory"><a class="header" href="#libprocess_ssl_ca_dirpath-to-ca-directory">LIBPROCESS_SSL_CA_DIR=(path to CA directory)</a></h4>
<p>The directory used to find the certificate authority / authorities. You can specify <code>LIBPROCESS_SSL_CA_DIR</code> or <code>LIBPROCESS_SSL_CA_FILE</code> depending on how you want to restrict your certificate authorization.</p>
<h4 id="libprocess_ssl_ca_filepath-to-ca-file"><a class="header" href="#libprocess_ssl_ca_filepath-to-ca-file">LIBPROCESS_SSL_CA_FILE=(path to CA file)</a></h4>
<p>The file used to find the certificate authority. You can specify <code>LIBPROCESS_SSL_CA_DIR</code> or <code>LIBPROCESS_SSL_CA_FILE</code> depending on how you want to restrict your certificate authorization.</p>
<h4 id="libprocess_ssl_ciphersaccepted-ciphers-separated-by--defaultaes128-shaaes256-sharc4-shadhe-rsa-aes128-shadhe-dss-aes128-shadhe-rsa-aes256-shadhe-dss-aes256-sha"><a class="header" href="#libprocess_ssl_ciphersaccepted-ciphers-separated-by--defaultaes128-shaaes256-sharc4-shadhe-rsa-aes128-shadhe-dss-aes128-shadhe-rsa-aes256-shadhe-dss-aes256-sha">LIBPROCESS_SSL_CIPHERS=(accepted ciphers separated by ':') [default=AES128-SHA:AES256-SHA:RC4-SHA:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA:DHE-RSA-AES256-SHA:DHE-DSS-AES256-SHA]</a></h4>
<p>A list of <code>:</code>-separated ciphers. Use these if you want to restrict or open up the accepted ciphers for OpenSSL. Read the OpenSSL documentation or contact your system administrators to see whether you want to override the default values.</p>
<h4 id="libprocess_ssl_enable_ssl_v3false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_ssl_v3false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_SSL_V3=(false|0,true|1) [default=false|0]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_0false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_tls_v1_0false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_TLS_V1_0=(false|0,true|1) [default=false|0]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_1false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_tls_v1_1false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_TLS_V1_1=(false|0,true|1) [default=false|0]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_2false0true1-defaulttrue1"><a class="header" href="#libprocess_ssl_enable_tls_v1_2false0true1-defaulttrue1">LIBPROCESS_SSL_ENABLE_TLS_V1_2=(false|0,true|1) [default=true|1]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_3false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_tls_v1_3false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_TLS_V1_3=(false|0,true|1) [default=false|0]</a></h4>
<p>The above switches enable / disable the specified protocols. By default only TLS V1.2 is enabled. SSL V2 is always disabled; there is no switch to enable it. The mentality here is to restrict security by default, and force users to open it up explicitly. Many older version of the protocols have known vulnerabilities, so only enable these if you fully understand the risks.
TLS V1.3 is not supported yet and should not be enabled. <a href="https://issues.apache.org/jira/browse/MESOS-9730">MESOS-9730</a>.
<em>SSLv2 is disabled completely because modern versions of OpenSSL disable it using multiple compile time configuration options.</em>
#<a name="Dependencies"></a>Dependencies</p>
<h4 id="libprocess_ssl_ecdh_curveautolist-of-curves-separated-by--defaultauto"><a class="header" href="#libprocess_ssl_ecdh_curveautolist-of-curves-separated-by--defaultauto">LIBPROCESS_SSL_ECDH_CURVE=(auto|list of curves separated by ':') [default=auto]</a></h4>
<p>List of elliptic curves which should be used for ECDHE-based cipher suites, in preferred order. Available values depend on the OpenSSL version used. Default value <code>auto</code> allows OpenSSL to pick the curve automatically.
OpenSSL versions prior to <code>1.0.2</code> allow for the use of only one curve; in those cases, <code>auto</code> defaults to <code>prime256v1</code>.</p>
<h4 id="libprocess_ssl_hostname_validation_schemelegacyopenssl-defaultlegacy"><a class="header" href="#libprocess_ssl_hostname_validation_schemelegacyopenssl-defaultlegacy">LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME=(legacy|openssl) [default=legacy]</a></h4>
<p>This flag is used to select the scheme by which the hostname validation check works.</p>
<p>Since hostname validation is part of certificate verification, this flag has no
effect unless one of <code>LIBPROCESS_SSL_VERIFY_SERVER_CERT</code> or <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code>
is set to true.</p>
<p>Currently, it is possible to choose between two schemes:</p>
<ul>
<li>
<p><code>openssl</code>:</p>
<p>In client mode: Perform the hostname validation checks during the TLS handshake.
If the client connects via hostname, accept the certificate if it contains
the hostname as common name (CN) or as a subject alternative name (SAN).
If the client connects via IP address and <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is true,
accept the certificate if it contains the IP as a subject alternative name.</p>
<p><strong>NOTE:</strong> If the client connects via IP address and <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is false,
the connection attempt cannot succeed.</p>
<p>In server mode: Do not perform any hostname validation checks.</p>
<p>This setting requires OpenSSL &gt;= 1.0.2 to be used.</p>
</li>
<li>
<p><code>legacy</code>:</p>
<p>Use a custom hostname validation algorithm that is run after the connection is established,
and immediately close the connection if it fails.</p>
<p>In both client and server mode:
Do a reverse DNS lookup on the peer IP. If <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is set to <code>false</code>,
accept the certificate if it contains the first result of that lookup as either the common name
or as a subject alternative name. If <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is set to <code>true</code>,
additionally accept the certificate if it contains the peer IP as a subject alternative name.</p>
</li>
</ul>
<p>It is suggested that operators choose the 'openssl' setting unless they have
applications relying on the legacy behaviour of the 'libprocess' scheme. It is
using standardized APIs (<code>X509_VERIFY_PARAM_check_{host,ip}</code>) provided by OpenSSL to
make hostname validation more uniform across applications. It is also more secure,
since attackers that are able to forge a DNS or rDNS result can launch a successful
man-in-the-middle attack on the 'legacy' scheme.</p>
<h3 id="libevent"><a class="header" href="#libevent">libevent</a></h3>
<p>If building with <code>--enable-libevent</code>, we require the OpenSSL support from
libevent. The suggested version of libevent is
<a href="https://github.com/libevent/libevent/releases/tag/release-2.0.22-stable"><code>2.0.22-stable</code></a>.
As new releases come out we will try to maintain compatibility.</p>
<pre><code>// For example, on OSX:
brew install libevent
</code></pre>
<h3 id="openssl"><a class="header" href="#openssl">OpenSSL</a></h3>
<p>We require <a href="https://github.com/openssl/openssl">OpenSSL</a>.
There are multiple branches of OpenSSL that are being maintained by the
community. Since security requires being vigilant, we recommend reading
the release notes for the current releases of OpenSSL and deciding on a
version within your organization based on your security needs.</p>
<p>When building with libevent, Mesos is not too deeply dependent on specific
OpenSSL versions, so there is room for you to make security decisions as
an organization. When building without libevent, OpenSSL 1.1+ is required,
because Mesos makes use of APIs introduced in later versions of OpenSSL.</p>
<p>Please ensure the <code>event2</code> (when building with libevent) and
<code>openssl</code> headers are available for building Mesos.</p>
<pre><code>// For example, on OSX:
brew install openssl
</code></pre>
<h1 id="a-nameupgradingaupgrading-your-cluster"><a class="header" href="#a-nameupgradingaupgrading-your-cluster"><a name="Upgrading"></a>Upgrading Your Cluster</a></h1>
<p><em>There is no SSL specific requirement for upgrading different components in a specific order.</em></p>
<p>The recommended strategy is to restart all your components to enable SSL with downgrades support enabled. Once all components have SSL enabled, then do a second restart of all your components to disable downgrades. This strategy will allow each component to be restarted independently at your own convenience with no time restrictions. It will also allow you to try SSL in a subset of your cluster.</p>
<p><strong>NOTE:</strong> While different components in your cluster are serving SSL vs non-SSL traffic, any relative links in the WebUI may be broken. Please see the <a href="ssl.html#WebUI">WebUI</a> section for details. Here are sample commands for upgrading your cluster:</p>
<pre><code>// Restart each component with downgrade support (master, agent, framework):
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_SUPPORT_DOWNGRADE=true LIBPROCESS_SSL_KEY_FILE=&lt;path-to-your-private-key&gt; LIBPROCESS_SSL_CERT_FILE=&lt;path-to-your-certificate&gt; &lt;Any other LIBPROCESS_SSL_* environment variables you may choose&gt; &lt;your-component (e.g. bin/master.sh)&gt; &lt;your-flags&gt;

// Restart each component WITHOUT downgrade support (master, agent, framework):
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_SUPPORT_DOWNGRADE=false LIBPROCESS_SSL_KEY_FILE=&lt;path-to-your-private-key&gt; LIBPROCESS_SSL_CERT_FILE=&lt;path-to-your-certificate&gt; &lt;Any other LIBPROCESS_SSL_* environment variables you may choose&gt; &lt;your-component (e.g. bin/master.sh)&gt; &lt;your-flags&gt;
</code></pre>
<p>Executors must be able to access the SSL environment variables and the files referred to by those variables. Environment variables can be provided to an executor by specifying <code>CommandInfo.environment</code> or by using the agent's <code>--executor_environment_variables</code> command line flag. If the agent and the executor are running in separate containers, <code>ContainerInfo.volumes</code> can be used to mount SSL files from the host into the executor's container.</p>
<p>The end state is a cluster that is only communicating with SSL.</p>
<p><strong>NOTE:</strong> Any tools you may use that communicate with your components must be able to speak SSL, or they will be denied. You may choose to maintain <code>LIBPROCESS_SSL_SUPPORT_DOWNGRADE=true</code> for some time as you upgrade your internal tooling. The advantage of <code>LIBPROCESS_SSL_SUPPORT_DOWNGRADE=true</code> is that all components that speak SSL will do so, while other components may still communicate over insecure channels.</p>
<h1 id="a-namewebuiawebui"><a class="header" href="#a-namewebuiawebui"><a name="WebUI"></a>WebUI</a></h1>
<p>The default Mesos WebUI uses relative links. Some of these links transition between endpoints served by the master and agents. The WebUI currently does not have enough information to change the 'http' vs 'https' links based on whether the target endpoint is currently being served by an SSL-enabled binary. This may cause certain links in the WebUI to be broken when a cluster is in a transition state between SSL and non-SSL. Any tools that hit these endpoints will still be able to access them as long as they hit the endpoint using the right protocol, or the <code>LIBPROCESS_SSL_SUPPORT_DOWNGRADE</code> option is set to true.</p>
<p><strong>NOTE:</strong> Frameworks with their own WebUI will need to add HTTPS support separately.</p>
<h3 id="certificates"><a class="header" href="#certificates">Certificates</a></h3>
<p>Most browsers have built in protection that guard transitions between pages served using different certificates. For this reason you may choose to serve both the master and agent endpoints using a common certificate that covers multiple hostnames. If you do not do this, certain links, such as those to agent sandboxes, may seem broken as the browser treats the transition between differing certificates transition as unsafe.</p>
<hr />
<h2>title: Apache Mesos - Secrets Handling
layout: documentation</h2>
<h1 id="secrets"><a class="header" href="#secrets">Secrets</a></h1>
<p>Starting 1.4.0 release, Mesos allows tasks to populate environment variables and
file volumes with secret contents that are retrieved using a secret-resolver
interface. It also allows specifying image-pull secrets for private container
registry. This allows users to avoid exposing critical secrets in task
definitions. Secrets are fetched/resolved using a secret-resolver module (see
below).</p>
<p>NOTE: Secrets are only supported for Mesos containerizer and not for the Docker
containerizer.</p>
<h2 id="secrets-message"><a class="header" href="#secrets-message">Secrets Message</a></h2>
<p>Secrets can be specified using the following protobuf message:</p>
<pre><code>message Secret {
  enum Type {
    UNKNOWN = 0;
    REFERENCE = 1;
    VALUE = 2;
  }

  message Reference {
    required string name = 1;
    optional string key = 2;
  }

  message Value {
    required bytes data = 1;
  }

  optional Type type = 1;

  optional Reference reference = 2;
  optional Value value = 3;
}
</code></pre>
<p>Secrets can be of type <code>reference</code> or <code>value</code> (only one of <code>reference</code> and <code>value</code> must be set).
A secret reference can be used by modules to refer to a secret stored in a secure back-end.
The <code>key</code> field can be used to reference a single value within a secret containing arbitrary key-value pairs.</p>
<p>For example, given a back-end secret store with a secret named &quot;/my/secret&quot; containing the following key-value pairs:</p>
<pre><code>{
  &quot;username&quot;: &quot;my-user&quot;,
  &quot;password&quot;: &quot;my-password
}
</code></pre>
<p>The username could be referred to in a <code>Secret</code> by specifying &quot;my/secret&quot; for the <code>name</code> and &quot;username&quot; for the <code>key</code>.</p>
<p>Secret also supports pass-by-value where the value of a secret can be directly
passed in the message.</p>
<h2 id="environment-based-secrets"><a class="header" href="#environment-based-secrets">Environment-based Secrets</a></h2>
<p>Environment variables can either be traditional value-based or secret-based. For
the latter, one can specify a secret as part of environment definition as shown
in the following example:</p>
<pre><code>{
  &quot;variables&quot; : [
    {
      &quot;name&quot;: &quot;MY_SECRET_ENV&quot;,
      &quot;type&quot;: &quot;SECRET&quot;,
      &quot;secret&quot;: {
        &quot;type&quot;: &quot;REFERENCE&quot;,
        &quot;reference&quot;: {
          &quot;name&quot;: &quot;/my/secret&quot;,
          &quot;key&quot;: &quot;username&quot;
        }
      }
    },
    {
      &quot;name&quot;: &quot;MY_NORMAL_ENV&quot;,
      &quot;value&quot;: &quot;foo&quot;
    }
  ]
}
</code></pre>
<h2 id="file-based-secrets"><a class="header" href="#file-based-secrets">File-based Secrets</a></h2>
<p>A new <code>volume/secret</code> isolator is available to create secret-based files inside
the task container. To use a secret, one can specify a new volume as follows:</p>
<pre><code>{
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;container_path&quot;: &quot;path/to/secret/file&quot;,
  &quot;source&quot;:
  {
    &quot;type&quot;: &quot;SECRET&quot;,
    &quot;secret&quot;: {
      &quot;type&quot;: &quot;REFERENCE&quot;,
      &quot;reference&quot;: {
        &quot;name&quot;: &quot;/my/secret&quot;,
        &quot;key&quot;: &quot;username&quot;
      }
    }
  }
}
</code></pre>
<p>This will create a tmpfs-based file mount in the container at &quot;path/to/secret/file&quot; which will contain the secret text fetched from the back-end secret store.</p>
<p>The <code>volume/secret</code> isolator is not enabled by default. To enable it, it must be specified in <code>--isolator=volume/secret</code> agent flag.</p>
<h2 id="image-pull-secrets"><a class="header" href="#image-pull-secrets">Image-pull Secrets</a></h2>
<p>Currently, image-pull secrets only support Docker images for Mesos
containerizer. Appc images are not supported.
One can store Docker config containing credentials to authenticate with Docker registry in the secret store.
The secret is expected to be a Docker config file in JSON format with UTF-8 character encoding.
The secret can then be referenced in the <code>Image</code> protobuf as follows:</p>
<pre><code>{
  &quot;type&quot;: &quot;DOCKER&quot;,
  &quot;docker&quot;:
  message Docker {
    &quot;name&quot;: &quot;&lt;REGISTRY_HOST&gt;/path/to/image&quot;,
    &quot;secret&quot;: {
      &quot;type&quot;: &quot;REFERENCE&quot;,
      &quot;reference&quot;: {
        &quot;name&quot;: &quot;/my/secret/docker/config&quot;
      }
    }
  }
}
</code></pre>
<h2 id="secretresolver-module"><a class="header" href="#secretresolver-module">SecretResolver Module</a></h2>
<p>The SecretResolver module is called from Mesos agent to fetch/resolve any image-pull, environment-based, or file-based secrets. (See <a href="modules.html">Mesos Modules</a> for more information on using Mesos modules).</p>
<pre><code>class SecretResolver
{
  virtual process::Future&lt;Secret::Value&gt; resolve(const Secret&amp; secret) const;
};
</code></pre>
<p>The default implementation simply resolves value-based Secrets. A custom secret-resolver module can be specified using the <code>--secret_resolver=&lt;module-name&gt;</code> agent flag.</p>
<hr />
<h2>title: Apache Mesos - Containerizers
layout: documentation</h2>
<h1 id="containerizers-1"><a class="header" href="#containerizers-1">Containerizers</a></h1>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>Containerizers are used to run tasks in 'containers', which in turn are
used to:</p>
<ul>
<li>Isolate a task from other running tasks.</li>
<li>'Contain' tasks to run in limited resource runtime environment.</li>
<li>Control a task's resource usage (e.g., CPU, memory) programatically.</li>
<li>Run software in a pre-packaged file system image, allowing it to run in
different environments.</li>
</ul>
<h2 id="types-of-containerizers"><a class="header" href="#types-of-containerizers">Types of containerizers</a></h2>
<p>Mesos plays well with existing container technologies (e.g., docker) and also
provides its own container technology. It also supports composing different
container technologies (e.g., docker and mesos).</p>
<p>Mesos implements the following containerizers:</p>
<ul>
<li><a href="containerizers.html#Composing">Composing</a></li>
<li><a href="containerizers.html#Docker">Docker</a></li>
<li><a href="containerizers.html#Mesos">Mesos (default)</a></li>
</ul>
<p>User can specify the types of containerizers to use via the agent flag
<code>--containerizers</code>.</p>
<p><a name="Composing"></a></p>
<h3 id="composing-containerizer"><a class="header" href="#composing-containerizer">Composing containerizer</a></h3>
<p>This feature allows multiple container technologies to play together. It is
enabled when you configure the <code>--containerizers</code> agent flag with multiple comma
seperated containerizer names (e.g., <code>--containerizers=mesos,docker</code>). The order
of the comma separated list is important as the first containerizer that
supports the task's container configuration will be used to launch the task.</p>
<p>Use cases:</p>
<ul>
<li>For testing tasks with different types of resource isolations. Since 'mesos'
containerizers have more isolation abilities, a framework can use composing
containerizer to test a task using 'mesos' containerizer's controlled
environment and at the same time test it to work with 'docker' containers by
just changing the container parameters for the task.</li>
</ul>
<p><a name="Docker"></a></p>
<h3 id="docker-containerizer"><a class="header" href="#docker-containerizer">Docker containerizer</a></h3>
<p>Docker containerizer allows tasks to be run inside docker container. This
containerizer is enabled when you configure the agent flag as
<code>--containerizers=docker</code>.</p>
<p>Use cases:</p>
<ul>
<li>If a task needs to be run with the tooling that comes with the docker package.</li>
<li>If Mesos agent is running inside a docker container.</li>
</ul>
<p>For more details, see
<a href="docker-containerizer.html">Docker Containerizer</a>.</p>
<p><a name="Mesos"></a></p>
<h3 id="mesos-containerizer"><a class="header" href="#mesos-containerizer">Mesos containerizer</a></h3>
<p>This containerizer allows tasks to be run with an array of pluggable isolators
provided by Mesos. This is the native Mesos containerizer solution and is
enabled when you configure the agent flag as <code>--containerizers=mesos</code>.</p>
<p>Use cases:</p>
<ul>
<li>Allow Mesos to control the task's runtime environment without depending on
other container technologies (e.g., docker).</li>
<li>Want fine grained operating system controls (e.g., cgroups/namespaces provided
by Linux).</li>
<li>Want Mesos's latest container technology features.</li>
<li>Need additional resource controls like disk usage limits, which
might not be provided by other container technologies.</li>
<li>Want to add custom isolation for tasks.</li>
</ul>
<p>For more details, see
<a href="mesos-containerizer.html">Mesos Containerizer</a>.</p>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li><a href="containerizer-internals.html">Containerizer Internals</a> for
implementation details of containerizers.</li>
</ul>
<hr />
<h2>title: Apache Mesos - Containerizer Internals
layout: documentation</h2>
<h1 id="containerizer"><a class="header" href="#containerizer">Containerizer</a></h1>
<p>Containerizers are Mesos components responsible for launching
containers. They own the containers launched for the tasks/executors,
and are responsible for their isolation, resource management, and
events (e.g., statistics).</p>
<h1 id="containerizer-internals"><a class="header" href="#containerizer-internals">Containerizer internals</a></h1>
<h3 id="containerizer-creation-and-launch"><a class="header" href="#containerizer-creation-and-launch">Containerizer creation and launch</a></h3>
<ul>
<li>Agent creates a containerizer based on the flags (using agent flag
<code>--containerizers</code>). If multiple containerizers (e.g., docker,
mesos) are specified using the <code>--containerizers</code> flag, then the
composing containerizer will be used to create a containerizer.</li>
<li>If an executor is not specified in <code>TaskInfo</code>, Mesos agent will use
the default executor for the task (depending on the Containerizer
the agent is using, it could be <code>mesos-executor</code> or
<code>mesos-docker-executor</code>). TODO: Update this after MESOS-1718 is
completed. After this change, master will be responsible for
generating executor information.</li>
</ul>
<h3 id="types-of-containerizers-1"><a class="header" href="#types-of-containerizers-1">Types of containerizers</a></h3>
<p>Mesos currently supports the following containerizers:</p>
<ul>
<li>Composing</li>
<li><a href="docker-containerizer.html">Docker</a></li>
<li><a href="mesos-containerizer.html">Mesos</a></li>
</ul>
<h4 id="composing-containerizer-1"><a class="header" href="#composing-containerizer-1">Composing Containerizer</a></h4>
<p>Composing containerizer will compose the specified containerizers
(using agent flag <code>--containerizers</code>) and act like a single
containerizer. This is an implementation of the <code>composite</code> design
pattern.</p>
<h4 id="docker-containerizer-1"><a class="header" href="#docker-containerizer-1">Docker Containerizer</a></h4>
<p>Docker containerizer manages containers using the docker engine provided
in the docker package.</p>
<h5 id="container-launch"><a class="header" href="#container-launch">Container launch</a></h5>
<ul>
<li>Docker containerizer will attempt to launch the task in docker only
if <code>ContainerInfo::type</code> is set to DOCKER.</li>
<li>Docker containerizer will first pull the image.</li>
<li>Calls pre-launch hook.</li>
<li>The executor will be launched in one of the two ways:</li>
</ul>
<p>A) Mesos agent runs in a docker container</p>
<ul>
<li>This is indicated by the presence of agent flag
<code>--docker_mesos_image</code>. In this case, the value of flag
<code>--docker_mesos_image</code> is assumed to be the docker image used to
launch the Mesos agent.</li>
<li>If the task includes an executor (custom executor), then that executor is
launched in a docker container.</li>
<li>If the task does not include an executor i.e. it defines a command, the
default executor <code>mesos-docker-executor</code> is launched in a docker container to
execute the command via Docker CLI.</li>
</ul>
<p>B) Mesos agent does not run in a docker container</p>
<ul>
<li>If the task includes an executor (custom executor), then that executor is
launched in a docker container.</li>
<li>If task does not include an executor i.e. it defines a command, a subprocess
is forked to execute the default executor <code>mesos-docker-executor</code>.
<code>mesos-docker-executor</code> then spawns a shell to execute the command via Docker
CLI.</li>
</ul>
<h4 id="mesos-containerizer-1"><a class="header" href="#mesos-containerizer-1">Mesos Containerizer</a></h4>
<p>Mesos containerizer is the native Mesos containerizer. Mesos
Containerizer will handle any executor/task that does not specify
<code>ContainerInfo::DockerInfo</code>.</p>
<h5 id="container-launch-1"><a class="header" href="#container-launch-1">Container launch</a></h5>
<ul>
<li>Calls prepare on each isolator.</li>
<li>Forks the executor using Launcher (see <a href="containerizer-internals.html#Launcher">Launcher</a>). The
forked child is blocked from executing until it is been isolated.</li>
<li>Isolate the executor. Call isolate with the pid for each isolator
(see <a href="containerizer-internals.html#Isolators">Isolators</a>).</li>
<li>Fetch the executor.</li>
<li>Exec the executor. The forked child is signalled to continue. It
will first execute any preparation commands from isolators and then
exec the executor.</li>
</ul>
<p><a name="Launcher"></a></p>
<h5 id="launcher"><a class="header" href="#launcher">Launcher</a></h5>
<p>Launcher is responsible for forking/destroying containers.</p>
<ul>
<li>Forks a new process in the containerized context. The child will
exec the binary at the given path with the given argv, flags, and
environment.</li>
<li>The I/O of the child will be redirected according to the specified
I/O descriptors.</li>
</ul>
<h6 id="linux-launcher"><a class="header" href="#linux-launcher">Linux launcher</a></h6>
<ul>
<li>Creates a &quot;freezer&quot; cgroup for the container.</li>
<li>Creates posix &quot;pipe&quot; to enable communication between host (parent
process) and container process.</li>
<li>Spawn child process (container process) using <code>clone</code> system call.</li>
<li>Moves the new container process to the freezer hierarchy.</li>
<li>Signals the child process to continue (exec'ing) by writing a
character to the write end of the pipe in the parent process.</li>
</ul>
<p>Starting from Mesos 1.1.0, <a href="nested-container-and-task-group.html">nested container</a>
is supported. The Linux Launcher is responsible to fork the subprocess
for the nested container with appropriate Linux namespaces being
cloned. The following is the table for Linux namespaces that
are supported for top level and nested containers.</p>
<h6 id="linux-namespaces"><a class="header" href="#linux-namespaces">Linux Namespaces</a></h6>
<table class="table table-striped">
  <tr>
    <th>Linux Namespaces</th>
    <th>Top Level Container</th>
    <th>Nested Container</th>
  </tr>
  <tr>
    <td>Mount</td>
    <td>Not shared</td>
    <td>Not shared</td>
  </tr>
  <tr>
    <td>PID</td>
    <td>Configurable</td>
    <td>Configurable</td>
  </tr>
  <tr>
    <td>Network & UTS</td>
    <td>Configurable</td>
    <td>Shared w/ parent</td>
  </tr>
  <tr>
    <td>IPC</td>
    <td>Not shared -> configurable (TBD)</td>
    <td>Not shared -> configurable (TBD)</td>
  </tr>
  <tr>
    <td>Cgroup</td>
    <td>Shared w/ agent -> Not shared (TBD)</td>
    <td>Shared w/ parent -> Not shared (TBD)</td>
  </tr>
  <tr>
    <td>User (not supported)</td>
    <td>Shared w/ agent</td>
    <td>Shared w/ parent</td>
  </tr>
</table>
<p>*Note: For the top level container, <code>shared</code> means that the container
shares the namespace from the agent. For the nested container, <code>shared</code>
means that the nested container shares the namespace from its parent
container.</p>
<h6 id="posix-launcher-tbd"><a class="header" href="#posix-launcher-tbd">Posix launcher (TBD)</a></h6>
<p><a name="Isolators"></a></p>
<h5 id="a-hrefmesos-containerizerhtmlisolatorsisolatorsa"><a class="header" href="#a-hrefmesos-containerizerhtmlisolatorsisolatorsa"><a href="mesos-containerizer.html#isolators">Isolators</a></a></h5>
<p><a href="mesos-containerizer.html#isolators">Isolators</a> are responsible for creating
an environment for the containers where resources like cpu, network,
storage and memory can be isolated from other containers.</p>
<h3 id="containerizer-states"><a class="header" href="#containerizer-states">Containerizer states</a></h3>
<h4 id="docker"><a class="header" href="#docker">Docker</a></h4>
<ul>
<li>FETCHING</li>
<li>PULLING</li>
<li>RUNNING</li>
<li>DESTROYING</li>
</ul>
<h4 id="mesos"><a class="header" href="#mesos">Mesos</a></h4>
<ul>
<li>PREPARING</li>
<li>ISOLATING</li>
<li>FETCHING</li>
<li>RUNNING</li>
<li>DESTROYING</li>
</ul>
<hr />
<h2>title: Apache Mesos - Docker Containerizer
layout: documentation</h2>
<h1 id="docker-containerizer-2"><a class="header" href="#docker-containerizer-2">Docker Containerizer</a></h1>
<p>Mesos 0.20.0 adds the support for launching tasks that contains Docker
images, with also a subset of Docker options supported while we plan
on adding more in the future.</p>
<p>Users can either launch a Docker image as a Task, or as an Executor.</p>
<p>The following sections will describe the API changes along with Docker
support, and also how to setup Docker.</p>
<h2 id="setup"><a class="header" href="#setup">Setup</a></h2>
<p>To run the agent to enable the Docker Containerizer, you must launch
the agent with &quot;docker&quot; as one of the containerizers option.</p>
<p>Example: <code>mesos-agent --containerizers=docker,mesos</code></p>
<p>Each agent that has the Docker containerizer should have Docker CLI
client installed (version &gt;= 1.8.0).</p>
<p>If you enable iptables on agent, make sure the iptables allow all
traffic from docker bridge interface through add below rule:</p>
<pre><code>iptables -A INPUT -s 172.17.0.0/16 -i docker0 -p tcp -j ACCEPT
</code></pre>
<h2 id="how-do-i-use-the-docker-containerizer"><a class="header" href="#how-do-i-use-the-docker-containerizer">How do I use the Docker Containerizer?</a></h2>
<p>TaskInfo before 0.20.0 used to only support either setting a
CommandInfo that launches a task running the bash command, or an
ExecutorInfo that launches a custom Executor that will launch the
task.</p>
<p>With 0.20.0 we added a ContainerInfo field to TaskInfo and
ExecutorInfo that allows a Containerizer such as Docker to be
configured to run the task or executor.</p>
<p>To run a Docker image as a task, in TaskInfo one must set both the
command and the container field as the Docker Containerizer will use
the accompanied command to launch the docker image.  The ContainerInfo
should have type Docker and a DockerInfo that has the desired docker
image.</p>
<p>To run a Docker image as an executor, in TaskInfo one must set the
ExecutorInfo that contains a ContainerInfo with type docker and the
CommandInfo that will be used to launch the executor.  Note that the
Docker image is expected to launch up as a Mesos executor that will
register with the agent once it launches.</p>
<h2 id="what-does-the-docker-containerizer-do"><a class="header" href="#what-does-the-docker-containerizer-do">What does the Docker Containerizer do?</a></h2>
<p>The Docker Containerizer is translating Task/Executor <code>Launch</code> and
<code>Destroy</code> calls to Docker CLI commands.</p>
<p>Currently the Docker Containerizer when launching as task will do the
following:</p>
<ol>
<li>
<p>Fetch all the files specified in the CommandInfo into the sandbox.</p>
</li>
<li>
<p>Pull the docker image from the remote repository.</p>
</li>
<li>
<p>Run the docker image with the Docker executor, and map the sandbox
directory into the Docker container and set the directory mapping to
the MESOS_SANDBOX environment variable. The executor will also stream
the container logs into stdout/stderr files in the sandbox.</p>
</li>
<li>
<p>On container exit or containerizer destroy, stop and remove the
docker container.</p>
</li>
</ol>
<p>The Docker Containerizer launches all containers with the <code>mesos-</code>
prefix plus the agent id (ie: <code>mesos-agent1-abcdefghji</code>), and also
assumes all containers with the <code>mesos-</code> prefix is managed by the
agent and is free to stop or kill the containers.</p>
<p>When launching the docker image as an Executor, the only difference is
that it skips launching a command executor but just reaps on the
docker container executor pid.</p>
<p>Note that we currently default to host networking when running a
docker image, to easier support running a docker image as an Executor.</p>
<p>The containerizer also supports optional force pulling of the image.
It is set disabled as default, so the docker image will only be
updated again if it's not available on the host. To enable force
pulling an image, <code>force_pull_image</code> has to be set as true.</p>
<h2 id="private-docker-repository"><a class="header" href="#private-docker-repository">Private Docker repository</a></h2>
<p>To run an image from a private repository, one can include the uri
pointing to a <code>.dockercfg</code> that contains login information.  The
<code>.dockercfg</code> file will be pulled into the sandbox the Docker
Containerizer set the HOME environment variable pointing to the
sandbox so docker cli will automatically pick up the config file.</p>
<p>Starting from 1.0, we provide an alternative way to specify docker
config file for pulling images from private registries. We allow
operators to specify a shared docker config file using an agent flag.
This docker config file will be used to pull images from private
registries for all containers. See <a href="configuration/agent.html">configuration
documentation</a> for detail. Operators can either
specify the flag as an absolute path pointing to the docker config
file (need to manually configure <code>.docker/config.json</code> or <code>.dockercfg</code>
on each agent), or specify the flag as a JSON-formatted string.  For
example:</p>
<pre><code>--docker_config=file:///home/vagrant/.docker/config.json
</code></pre>
<p>or as a JSON object,</p>
<pre><code>--docker_config=&quot;{ \
  \&quot;auths\&quot;: { \
    \&quot;https://index.docker.io/v1/\&quot;: { \
      \&quot;auth\&quot;: \&quot;xXxXxXxXxXx=\&quot;, \
      \&quot;email\&quot;: \&quot;username@example.com\&quot; \
    } \
  } \
}&quot;
</code></pre>
<h2 id="commandinfo-to-run-docker-images"><a class="header" href="#commandinfo-to-run-docker-images">CommandInfo to run Docker images</a></h2>
<p>A docker image currently supports having an entrypoint and/or a
default command.</p>
<p>To run a docker image with the default command (ie: <code>docker run image</code>), the CommandInfo's value must not be set. If the value is set
then it will override the default command.</p>
<p>To run a docker image with an entrypoint defined, the CommandInfo's
shell option must be set to false.  If shell option is set to true the
Docker Containerizer will run the user's command wrapped with <code>/bin/sh -c</code> which will also become parameters to the image entrypoint.</p>
<h2 id="recover-docker-containers-on-agent-recovery"><a class="header" href="#recover-docker-containers-on-agent-recovery">Recover Docker containers on agent recovery</a></h2>
<p>The Docker containerizer supports recovering Docker containers when
the agent restarts, which supports both when the agent is running in a
Docker container or not.</p>
<p>With the <code>--docker_mesos_image</code> flag enabled, the Docker containerizer
assumes the containerizer is running in a container itself and
modifies the mechanism it recovers and launches docker containers
accordingly.</p>
<hr />
<h2>title: Apache Mesos - Mesos Containerizer
layout: documentation</h2>
<h1 id="mesos-containerizer-2"><a class="header" href="#mesos-containerizer-2">Mesos Containerizer</a></h1>
<p>The Mesos Containerizer provides lightweight containerization and
resource isolation of executors using Linux-specific functionality
such as control cgroups and namespaces. It is composable so operators
can selectively enable different <a href="mesos-containerizer.html#isolators">isolators</a>.</p>
<p>It also provides basic support for POSIX systems (e.g., OSX) but
without any actual isolation, only resource usage reporting.</p>
<h2 id="isolators"><a class="header" href="#isolators">Isolators</a></h2>
<p>Isolators are components that each define an aspect of how a tasks
execution environment (or container) is constructed. Isolators can
control how containers are isolated from each other, how task resource
limits are enforced, how networking is configured, how security
policies are applied.</p>
<p>Since the isolator interface is <a href="modules.html">modularized</a>, operators
can write modules that implement custom isolators.</p>
<p>Mesos supports the following built-in isolators.</p>
<ul>
<li>appc/runtime</li>
<li><a href="isolators/cgroups-blkio.html">cgroups/blkio</a></li>
<li><a href="isolators/cgroups-cpu.html">cgroups/cpu</a></li>
<li>cgroups/cpuset</li>
<li><a href="isolators/cgroups-devices.html">cgroups/devices</a></li>
<li>cgroups/hugetlb</li>
<li>cgroups/mem</li>
<li><a href="isolators/cgroups-net-cls.html">cgroups/net_cls</a></li>
<li>cgroups/net_prio</li>
<li>cgroups/perf_event</li>
<li>cgroups/pids</li>
<li><a href="isolators/disk-du.html">disk/du</a></li>
<li><a href="isolators/disk-xfs.html">disk/xfs</a></li>
<li><a href="isolators/docker-runtime.html">docker/runtime</a></li>
<li><a href="isolators/docker-volume.html">docker/volume</a></li>
<li><a href="secrets.html#environment-based-secrets">environment_secret</a></li>
<li><a href="isolators/filesystems.html">filesystem/linux</a></li>
<li><a href="isolators/filesystems.html">filesystem/posix</a></li>
<li><a href="isolators/filesystem-shared.html">filesystem/shared</a></li>
<li>filesystem/windows</li>
<li><a href="gpu-support.html">gpu/nvidia</a></li>
<li><a href="isolators/linux-capabilities.html">linux/capabilities</a></li>
<li><a href="isolators/linux-devices.html">linux/devices</a></li>
<li><a href="isolators/linux-nnp.html">linux/nnp</a></li>
<li><a href="isolators/linux-seccomp.html">linux/seccomp</a></li>
<li><a href="isolators/namespaces-ipc.html">namespaces/ipc</a></li>
<li><a href="isolators/namespaces-pid.html">namespaces/pid</a></li>
<li><a href="cni.html">network/cni</a></li>
<li><a href="isolators/network-port-mapping.html">network/port_mapping</a></li>
<li><a href="isolators/network-ports.html">network/ports</a></li>
<li>posix/cpu</li>
<li>posix/mem</li>
<li><a href="isolators/posix-rlimits.html">posix/rlimits</a></li>
<li><a href="isolators/csi-volume.html">volume/csi</a></li>
<li><a href="container-volume.html#host_path-volume-source">volume/host_path</a></li>
<li>volume/image</li>
<li><a href="container-volume.html#sandbox_path-volume-source">volume/sandbox_path</a></li>
<li><a href="secrets.html#file-based-secrets">volume/secret</a></li>
<li><a href="isolators/windows.html#cpu-limits">windows/cpu</a></li>
<li><a href="isolators/windows.html#memory-limits">windows/mem</a></li>
</ul>
<hr />
<h2>title: Apache Mesos - Supporting Container Images in Mesos Containerizer
layout: documentation</h2>
<h1 id="supporting-container-images-in-a-hrefmesos-containerizerhtmlmesos-containerizera"><a class="header" href="#supporting-container-images-in-a-hrefmesos-containerizerhtmlmesos-containerizera">Supporting Container Images in <a href="mesos-containerizer.html">Mesos Containerizer</a></a></h1>
<h2 id="motivation-1"><a class="header" href="#motivation-1">Motivation</a></h2>
<p>Mesos currently supports several <a href="containerizers.html">containerizers</a>,
notably the Mesos containerizer and the Docker containerizer. Mesos
containerizer uses native OS features directly to provide isolation
between containers, while Docker containerizer delegates container
management to the Docker engine.</p>
<p>Maintaining two containerizers is hard. For instance, when we add new
features to Mesos (e.g., persistent volumes, disk isolation), it
becomes a burden to update both containerizers. Even worse, sometimes
the isolation on some resources (e.g., network handles on an agent)
requires coordination between two containerizers, which is very hard
to implement in practice. In addition, we found that extending and
customizing isolation for containers launched by Docker engine is
difficult, mainly because we do not have a way to inject logics during
the life cycle of a container.</p>
<p>Therefore, we made an effort to unify containerizers in Mesos
(<a href="https://issues.apache.org/jira/browse/MESOS-2840">MESOS-2840</a>,
a.k.a. the Universal Containerizer). We improved Mesos containerizer
so that it now supports launching containers that specify container
images (e.g., Docker/Appc images).</p>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>To support container images, we introduced a new component in Mesos
containerizer, called image provisioner. Image provisioner is
responsible for pulling, caching and preparing container root
filesystems. It also extracts runtime configurations from container
images which will then be passed to the corresponding isolators for
proper isolation.</p>
<p>There are a few container image specifications, notably
<a href="https://github.com/docker/docker/blob/master/image/spec/v1.md">Docker</a>,
<a href="https://github.com/appc/spec/blob/master/SPEC.md">Appc</a>, and
<a href="https://github.com/opencontainers/specs">OCI</a> (future). Currently, we
support Docker and Appc images. More details about what features are
supported or not can be found in the following sections.</p>
<p><strong>NOTE</strong>: container image is only supported on Linux currently.</p>
<h3 id="configure-the-agent"><a class="header" href="#configure-the-agent">Configure the agent</a></h3>
<p>To enable container image support in Mesos containerizer, the operator
will need to specify the <code>--image_providers</code> agent flag which tells
Mesos containerizer what types of container images are allowed. For
example, setting <code>--image_providers=docker</code> allow containers to use
Docker images. The operators can also specify multiple container image
types. For instance, <code>--image_providers=docker,appc</code> allows both
Docker and Appc container images.</p>
<p>A few isolators need to be turned on in order to provide proper
isolation according to the runtime configurations specified in the
container image. The operator needs to add the following isolators to
the <code>--isolation</code> flag.</p>
<ul>
<li>
<p><code>filesystem/linux</code>: This is needed because supporting container
images involves changing filesystem root, and only <code>filesystem/linux</code>
support that currently. Note that this isolator requires root
permission.</p>
</li>
<li>
<p><code>docker/runtime</code>: This is used to provide support for runtime
configurations specified in Docker images (e.g., Entrypoint/Cmd,
environment variables, etc.). See more details about this isolator in
<a href="mesos-containerizer.html">Mesos containerizer doc</a>. Note that if this
isolator is not specified and <code>--image_providers</code> contains <code>docker</code>,
the agent will refuse to start.</p>
</li>
</ul>
<p>In summary, to enable container image support in Mesos containerizer,
please specify the following agent flags:</p>
<pre><code>$ sudo mesos-agent \
  --containerizers=mesos \
  --image_providers=appc,docker \
  --isolation=filesystem/linux,docker/runtime
</code></pre>
<h3 id="framework-api"><a class="header" href="#framework-api">Framework API</a></h3>
<p>We introduced a new protobuf message <code>Image</code> which allow frameworks to
specify container images for their containers. It has two types right
now: <code>APPC</code> and <code>DOCKER</code>, representing Appc and Docker images
respectively.</p>
<p>For Appc images, the <code>name</code> and <code>labels</code> are what described in the
<a href="https://github.com/appc/spec/blob/master/spec/aci.md#image-manifest-schema">spec</a>.</p>
<p>For Docker images, the <code>name</code> is the Docker image reference in the
following form (the same format expected by <code>docker pull</code>):
<code>[REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG|@DIGEST]</code></p>
<pre><code>message Image {
  enum Type {
    APPC = 1;
    DOCKER = 2;
  }

  message Appc {
    required string name = 1;
    optional Labels labels = 3;
  }

  message Docker {
    required string name = 1;
  }

  required Type type = 1;

  // Only one of the following image messages should be set to match
  // the type.
  optional Appc appc = 2;
  optional Docker docker = 3;
}
</code></pre>
<p>The framework needs to specify <code>MesosInfo</code> in <code>ContainerInfo</code> in order
to launch containers with container images. In other words, the
framework needs to set the type to <code>ContainerInfo.MESOS</code>, indicating
that it wants to use the Mesos containerizer. If <code>MesosInfo.image</code> is
not specified, the container will use the host filesystem. If
<code>MesosInfo.image</code> is specified, it will be used as the container
image when launching the container.</p>
<pre><code>message ContainerInfo {
  enum Type {
    DOCKER = 1;
    MESOS = 2;
  }

  message MesosInfo {
    optional Image image = 1;
  }

  required Type type = 1;
  optional MesosInfo mesos = 5;
}
</code></pre>
<h3 id="test-it-out"><a class="header" href="#test-it-out">Test it out!</a></h3>
<p>First, start the Mesos master:</p>
<pre><code>$ sudo sbin/mesos-master --work_dir=/tmp/mesos/master
</code></pre>
<p>Then, start the Mesos agent:</p>
<pre><code>$ sudo GLOG_v=1 sbin/mesos-agent \
  --master=&lt;MASTER_IP&gt;:5050 \
  --isolation=docker/runtime,filesystem/linux \
  --work_dir=/tmp/mesos/agent \
  --image_providers=docker \
  --executor_environment_variables=&quot;{}&quot;
</code></pre>
<p>Now, use Mesos CLI (i.e., mesos-execute) to launch a Docker container
(e.g., redis). Note that <code>--shell=false</code> tells Mesos to use the
default entrypoint and cmd specified in the Docker image.</p>
<pre><code>$ sudo bin/mesos-execute \
  --master=&lt;MASTER_IP&gt;:5050 \
  --name=test \
  --docker_image=library/redis \
  --shell=false
</code></pre>
<p>Verify if your container is running by launching a redis client:</p>
<pre><code>$ sudo docker run -ti --net=host redis redis-cli
127.0.0.1:6379&gt; ping
PONG
127.0.0.1:6379&gt;
</code></pre>
<h2 id="docker-support-and-current-limitations"><a class="header" href="#docker-support-and-current-limitations">Docker Support and Current Limitations</a></h2>
<p>Image provisioner uses <a href="https://docs.docker.com/registry/spec/api/">Docker v2 registry
API</a> to fetch Docker
images/layers. Both docker manifest
<a href="https://docs.docker.com/registry/spec/manifest-v2-1/">v2 schema1</a>
and <a href="https://docs.docker.com/registry/spec/manifest-v2-2/">v2 schema2</a>
are supported (v2 schema2 is supported starting from 1.8.0). The
fetching is based on <code>curl</code>, therefore SSL is automatically handled.
For private registries, the operator needs to configure <code>curl</code>
with the location of required CA certificates.</p>
<p>Fetching requiring authentication is supported through the
<code>--docker_config</code> agent flag. Starting from 1.0, operators can use
this agent flag to specify a shared docker config file, which is
used for pulling private repositories with authentication. Per
container credential is not supported yet (coming soon).</p>
<p>Operators can either specify the flag as an absolute path pointing to
the docker config file (need to manually configure
<code>.docker/config.json</code> or <code>.dockercfg</code> on each agent), or specify the
flag as a JSON-formatted string. See <a href="configuration/agent.html">configuration
documentation</a> for detail. For example:</p>
<pre><code>--docker_config=file:///home/vagrant/.docker/config.json
</code></pre>
<p>or as a JSON object,</p>
<pre><code>--docker_config=&quot;{ \
  \&quot;auths\&quot;: { \
    \&quot;https://index.docker.io/v1/\&quot;: { \
      \&quot;auth\&quot;: \&quot;xXxXxXxXxXx=\&quot;, \
      \&quot;email\&quot;: \&quot;username@example.com\&quot; \
    } \
  } \
}&quot;
</code></pre>
<p>Private registry is supported either through the <code>--docker_registry</code>
agent flag, or specifying private registry for each container using
image name <code>&lt;REGISTRY&gt;/&lt;REPOSITORY&gt;</code> (e.g.,
<code>localhost:80/gilbert/inky:latest</code>). If <code>&lt;REGISTRY&gt;</code> is included as
a prefix in the image name, the registry specified through the agent
flag <code>--docker_registry</code> will be ignored.</p>
<p>If the <code>--docker_registry</code> agent flag points to a local directory
(e.g., <code>/tmp/mesos/images/docker</code>), the provisioner will pull Docker
images from local filesystem, assuming Docker archives (result of
<code>docker save</code>) are stored there based on the image name and tag.  For
example, the operator can put a <code>busybox:latest.tar</code> (the result of
<code>docker save -o busybox:latest.tar busybox</code>) under
<code>/tmp/mesos/images/docker</code> and launch the agent by specifying
<code>--docker_registry=/tmp/mesos/images/docker</code>. Then the framework can
launch a Docker container by specifying <code>busybox:latest</code> as the name
of the Docker image. This flag can also point to an HDFS URI
(<em>experimental</em> in Mesos 1.7) (e.g., <code>hdfs://localhost:8020/archives/</code>)
to fetch images from HDFS if the <code>hadoop</code> command is available on the
agent.</p>
<p>If the <code>--switch_user</code> flag is set on the agent and the framework
specifies a user (either <code>CommandInfo.user</code> or <code>FrameworkInfo.user</code>),
we expect that user exists in the container image and its uid and gids
matches that on the host. User namespace is not supported yet. If the
user is not specified, <code>root</code> will be used by default. The operator or
the framework can limit the
<a href="http://man7.org/linux/man-pages/man7/capabilities.7.html">capabilities</a>
of the container by using the
<a href="isolators/linux-capabilities.html">linux/capabilities</a> isolator.</p>
<p>Currently, we support <code>host</code>, <code>bridge</code> and user defined networks
(<a href="https://docs.docker.com/engine/userguide/networking/">reference</a>).
<code>none</code> is not supported yet. We support the above networking modes in
<a href="mesos-containerizer.html">Mesos Containerizer</a> using the
<a href="https://github.com/containernetworking/cni">CNI</a> (Container Network
Interface) standard. Please refer to the <a href="cni.html">network/cni</a>
isolator document for more details about how to configure the network
for the container.</p>
<h3 id="more-agent-flags"><a class="header" href="#more-agent-flags">More agent flags</a></h3>
<p><code>--docker_registry</code>: The default URL for pulling Docker images. It
could either be a Docker registry server URL (i.e:
<code>https://registry.docker.io</code>), or a local path (i.e:
<code>/tmp/docker/images</code>) in which Docker image archives (result of
<code>docker save</code>) are stored. The default value is
<code>https://registry-1.docker.io</code>.</p>
<p><code>--docker_store_dir</code>: Directory the Docker provisioner will store
images in. All the Docker images are cached under this directory. The
default value is <code>/tmp/mesos/store/docker</code>.</p>
<p><code>--docker_config</code>: The default docker config file for agent. Can
be provided either as an absolute path pointing to the agent local
docker config file, or as a JSON-formatted string. The format of
the docker config file should be identical to docker's default one
(e.g., either <code>$HOME/.docker/config.json</code> or <code>$HOME/.dockercfg</code>).</p>
<h2 id="appc-support-and-current-limitations"><a class="header" href="#appc-support-and-current-limitations">Appc Support and Current Limitations</a></h2>
<p>Currently, only the root filesystem specified in the Appc image is
supported. Other runtime configurations like environment variables,
exec, working directory are not supported yet (coming soon).</p>
<p>For image discovery, we current support a simple discovery mechanism.
We allow operators to specify a URI prefix which will be prepend to
the URI template <code>{name}-{version}-{os}-{arch}.{ext}</code>. For example, if
the URI prefix is <code>file:///tmp/appc/</code> and the Appc image name is
<code>example.com/reduce-worker</code> with <code>version:1.0.0</code>, we will fetch the
image at <code>file:///tmp/appc/example.com/reduce-worker-1.0.0.aci</code>.</p>
<h3 id="more-agent-flags-1"><a class="header" href="#more-agent-flags-1">More agent flags</a></h3>
<p><code>appc_simple_discovery_uri_prefix</code>: URI prefix to be used for simple
discovery of appc images, e.g., <code>http://</code>, <code>https://</code>,
<code>hdfs://&lt;hostname&gt;:9000/user/abc/cde</code>. The default value is <code>http://</code>.</p>
<p><code>appc_store_dir</code>: Directory the appc provisioner will store images in.
All the Appc images are cached under this directory. The default value
is <code>/tmp/mesos/store/appc</code>.</p>
<h2 id="provisioner-backends"><a class="header" href="#provisioner-backends">Provisioner Backends</a></h2>
<p>A provisioner backend takes a set of filesystem layers and stacks them
into a root filesystem. Currently, we support the following backends:
<code>copy</code>, <code>bind</code>, <code>overlay</code> and <code>aufs</code>. Mesos will validate if the
selected backend works with the underlying filesystem (the filesystem
used by the image store <code>--docker_store_dir</code> or <code>--appc_store_dir</code>)
using the following logic table:</p>
<pre><code>+---------+--------------+------------------------------------------+
| Backend | Suggested on | Disabled on                              |
+---------+--------------+------------------------------------------+
| aufs    | ext4 xfs     | btrfs aufs eCryptfs                      |
| overlay | ext4 xfs*    | btrfs aufs overlay overlay2 zfs eCryptfs |
| bind    |              | N/A(`--sandbox_directory' must exist)    |
| copy    |              | N/A                                      |
+---------+--------------+------------------------------------------+
</code></pre>
<p>NOTE: <code>xfs</code> support on <code>overlay</code> is enabled only when <code>d_type=true</code>. Use
<code>xfs_info</code> to verify that the <code>xfs</code> ftype option is set to 1. To format
an xfs filesystem for <code>overlay</code>, use the flag <code>-n ftype=1</code> with <code>mkfs.xfs</code>.</p>
<p>The provisioner backend can be specified through the agent flag
<code>--image_provisioner_backend</code>. If not set, Mesos will select the best
backend automatically for the users/operators. The selection logic is
as following:</p>
<pre><code>1. Use `overlay` backend if the overlayfs is available.
2. Use `aufs` backend if the aufs is available and overlayfs is not supported.
3. Use `copy` backend if none of above is selected.
</code></pre>
<h3 id="copy"><a class="header" href="#copy">Copy</a></h3>
<p>The Copy backend simply copies all the layers into a target root
directory to create a root filesystem.</p>
<h3 id="bind"><a class="header" href="#bind">Bind</a></h3>
<p>This is a specialized backend that may be useful for deployments using
large (multi-GB) single-layer images <em>and</em> where more recent kernel
features such as overlayfs are not available. For small images (10's
to 100's of MB) the copy backend may be sufficient. Bind backend is
faster than Copy as it requires nearly zero IO.</p>
<p>The bind backend currently has these two limitations:</p>
<ol>
<li>
<p>The bind backend supports only a single layer. Multi-layer images will
fail to provision and the container will fail to launch!</p>
</li>
<li>
<p>The filesystem is read-only because all containers using this image
share the source. Select writable areas can be achieved by mounting
read-write volumes to places like <code>/tmp</code>, <code>/var/tmp</code>, <code>/home</code>, etc.
using the <code>ContainerInfo</code>. These can be relative to the executor work
directory. Since the filesystem is read-only, <code>--sandbox_directory</code>
and <code>/tmp</code> must already exist within the filesystem because the
filesystem isolator is unable to create it (e.g., either the image
writer needs to create the mount point in the image, or the operator
needs to set agent flag <code>--sandbox_directory</code> properly).</p>
</li>
</ol>
<h3 id="overlay"><a class="header" href="#overlay">Overlay</a></h3>
<p>The reason overlay backend was introduced is because the copy backend
will waste IO and space while the bind backend can only deal with one
layer. The overlay backend allows containizer to utilize the
filesystem to merge multiple filesystems into one efficiently.</p>
<p>The overlay backend depends on support for multiple lower layers,
which requires Linux kernel version 4.0 or later. For more information
of overlayfs, please refer to
<a href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt">here</a>.</p>
<h3 id="aufs"><a class="header" href="#aufs">AUFS</a></h3>
<p>The reason AUFS is introduced is because overlayfs support hasn't been
merged until kernel 3.18 and Docker's default storage backend for
ubuntu 14.04 is AUFS.</p>
<p>Like overlayfs, AUFS is also a unioned file system, which is very
stable, has a lot of real-world deployments, and has strong community
support.</p>
<p>Some Linux distributions do not support AUFS. This is usually because
AUFS is not included in the mainline (upstream) Linux kernel.</p>
<p>For more information of AUFS, please refer to
<a href="http://aufs.sourceforge.net/aufs2/man.html">here</a>.</p>
<h2 id="executor-dependencies-in-a-container-image"><a class="header" href="#executor-dependencies-in-a-container-image">Executor Dependencies in a Container Image</a></h2>
<p>Mesos has this concept of executors. All tasks are launched by an
executor. For a general purpose executor (e.g., thermos) of a
framework (e.g., Aurora), requiring it and all its dependencies to be
present in all possible container images that a user might use is
not trivial.</p>
<p>In order to solve this issue, we propose a solution where we allow the
executor to run on the host filesystem (without a container image).
Instead, it can specify a <code>volume</code> whose source is an <code>Image</code>. Mesos
containerizer will provision the <code>image</code> specified in the <code>volume</code>,
and mount it under the sandbox directory. The executor can perform
<code>pivot_root</code> or <code>chroot</code> itself to enter the container root
filesystem.</p>
<h2 id="garbage-collect-unused-container-images"><a class="header" href="#garbage-collect-unused-container-images">Garbage Collect Unused Container Images</a></h2>
<p>Experimental support of garbage-collecting unused container images was added at
Mesos 1.5. This can be either configured automatically via a new agent flag
<code>--image_gc_config</code>, or manually invoked through agent's
<a href="operator-http-api.html#prune_images">v1 Operator HTTP API</a>. This can be used
to avoid unbounded disk space usage of image stores.</p>
<p>This is implemented with a simple mark-and-sweep logic. When image GC happens,
we check all layers and images referenced by active running containers and avoid
removing them from the image store. As a pre-requisite, if there are active
containers launched before Mesos 1.5.0, we cannot determine what images can be
safely garbage collected, so agent will refuse to invoke image GC. To garbage
collect container images, users are expected to drain all containers launched
before Mesos 1.5.0.</p>
<p><strong>NOTE</strong>: currently, the image GC is only supported for docker store in Mesos
Containerizer.</p>
<h3 id="automatic-image-gc-through-agent-flag"><a class="header" href="#automatic-image-gc-through-agent-flag">Automatic Image GC through Agent Flag</a></h3>
<p>To enable automatic image GC, use the new agent flag <code>--image_gc_config</code>:</p>
<pre><code>--image_gc_config=file:///home/vagrant/image-gc-config.json
</code></pre>
<p>or as a JSON object,</p>
<pre><code>--image_gc_config=&quot;{ \
  \&quot;image_disk_headroom\&quot;: 0.1, \
  \&quot;image_disk_watch_interval\&quot;: { \
    \&quot;nanoseconds\&quot;: 3600000000000 \
    }, \
  \&quot;excluded_images\&quot;: \[ \] \
}&quot;
</code></pre>
<h3 id="manual-image-gc-through-http-api"><a class="header" href="#manual-image-gc-through-http-api">Manual Image GC through HTTP API</a></h3>
<p>See <code>PRUNE_IMAGES</code> section in
<a href="operator-http-api.html#prune_images">v1 Operator HTTP API</a> for manual image GC
through the agent HTTP API.</p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<p>For more information on the Mesos containerizer filesystem, namespace,
and isolator features, visit <a href="mesos-containerizer.html">Mesos
Containerizer</a>.  For more information on
launching Docker containers through the Docker containerizer, visit
<a href="docker-containerizer.html">Docker Containerizer</a>.</p>
<hr />
<h2>title: Apache Mesos - Docker Volume Support in Mesos Containerizer
layout: documentation</h2>
<h1 id="docker-volume-support-in-mesos-containerizer"><a class="header" href="#docker-volume-support-in-mesos-containerizer">Docker Volume Support in Mesos Containerizer</a></h1>
<p>Mesos 1.0 adds Docker volume support to the
<a href="isolators/../mesos-containerizer.html">MesosContainerizer</a> (a.k.a., the universal
containerizer) by introducing the new <code>docker/volume</code> isolator.</p>
<p>This document describes the motivation, overall architecture, configuration
steps for enabling Docker volume isolator, and required framework changes.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="isolators/docker-volume.html#motivation">Motivation</a></li>
<li><a href="isolators/docker-volume.html#how-does-it-work">How does it work?</a></li>
<li><a href="isolators/docker-volume.html#configuration">Configuration</a>
<ul>
<li><a href="isolators/docker-volume.html#pre-conditions">Pre-conditions</a></li>
<li><a href="isolators/docker-volume.html#configure-Docker-volume-isolator">Configuring Docker Volume Isolator</a></li>
<li><a href="isolators/docker-volume.html#enable-frameworks">Enabling frameworks to use Docker volumes</a>
<ul>
<li><a href="isolators/docker-volume.html#volume-protobuf">Volume Protobuf</a></li>
<li><a href="isolators/docker-volume.html#examples">Examples</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="isolators/docker-volume.html#limitations">Limitations</a></li>
<li><a href="isolators/docker-volume.html#test-it-out">Test it out!</a></li>
</ul>
<h2 id="a-namemotivationamotivation"><a class="header" href="#a-namemotivationamotivation"><a name="motivation"></a>Motivation</a></h2>
<p>The integration of external storage in Mesos is an attractive feature.  The
Mesos <a href="isolators/../persistent-volume.html">persistent volume</a> primitives allow stateful
services to persist data on an agent's local storage. However, the amount of
storage capacity that can be directly attached to a single agent is
limited---certain applications (e.g., databases) would like to access more data
than can easily be attached to a single node. Using external storage can
also simplify data migration between agents/containers, and can make backups and
disaster recovery easier.</p>
<p>The <a href="https://github.com/Docker/Docker/blob/master/docs/extend/plugins_volume.md">Docker Volume Driver
API</a>
defines an interface between the container runtime and external storage systems.
It has been widely adopted. There are Docker volume plugins for a variety of
storage drivers, such as <a href="https://github.com/rancher/convoy">Convoy</a>,
<a href="https://docs.clusterhq.com/en/latest/Docker-integration/">Flocker</a>,
<a href="https://github.com/calavera/Docker-volume-glusterfs">GlusterFS</a>, and
<a href="https://github.com/emccode/rexray">REX-Ray</a>. Each plugin typically supports a
variety of external storage systems, such as Amazon EBS, OpenStack Cinder, etc.</p>
<p>Therefore, introducing support for external storage in Mesos through the
<code>docker/volume</code> isolator provides Mesos with tremendous flexibility to
orchestrate containers on a wide variety of external storage technologies.</p>
<h2 id="a-namehow-does-it-workahow-does-it-work"><a class="header" href="#a-namehow-does-it-workahow-does-it-work"><a name="how-does-it-work"></a>How does it work?</a></h2>
<p><img src="isolators/images/docker-volume-isolator.png" alt="Docker Volume Isolator Architecture" /></p>
<p>The <code>docker/volume</code> isolator interacts with Docker volume plugins using
<a href="https://github.com/emccode/dvdcli">dvdcli</a>, an open-source command line tool
from EMC.</p>
<p>When a new task with Docker volumes is launched, the <code>docker/volume</code> isolator
will invoke <a href="https://github.com/emccode/dvdcli">dvdcli</a> to mount the
corresponding Docker volume onto the host and then onto the container.</p>
<p>When the task finishes or is killed, the <code>docker/volume</code> isolator will invoke
<a href="https://github.com/emccode/dvdcli">dvdcli</a> to unmount the corresponding Docker
volume.</p>
<p>The detailed workflow for the <code>docker/volume</code> isolator is as follows:</p>
<ol>
<li>
<p>A framework specifies external volumes in <code>ContainerInfo</code> when launching a
task.</p>
</li>
<li>
<p>The master sends the launch task message to the agent.</p>
</li>
<li>
<p>The agent receives the message and asks all isolators (including the
<code>docker/volume</code> isolator) to prepare for the container with the
<code>ContainerInfo</code>.</p>
</li>
<li>
<p>The isolator invokes <a href="https://github.com/emccode/dvdcli">dvdcli</a> to mount the
corresponding external volume to a mount point on the host.</p>
</li>
<li>
<p>The agent launches the container and bind-mounts the volume into the
container.</p>
</li>
<li>
<p>The bind-mounted volume inside the container will be unmounted from the
container automatically when the container finishes, as the container is in
its own mount namespace.</p>
</li>
<li>
<p>The agent invokes isolator cleanup which invokes
<a href="https://github.com/emccode/dvdcli">dvdcli</a> to unmount all mount points for
the container.</p>
</li>
</ol>
<h2 id="a-nameconfigurationaconfiguration"><a class="header" href="#a-nameconfigurationaconfiguration"><a name="configuration"></a>Configuration</a></h2>
<p>To use the <code>docker/volume</code> isolator, there are certain actions required by
operators and framework developers. In this section we list the steps required
by the operator to configure <code>docker/volume</code> isolator and the steps required by
framework developers to specify the Docker volumes.</p>
<h3 id="a-namepre-conditionsapre-conditions"><a class="header" href="#a-namepre-conditionsapre-conditions"><a name="pre-conditions"></a>Pre-conditions</a></h3>
<ul>
<li>
<p>Install <code>dvdcli</code> version
<a href="https://github.com/emccode/dvdcli/releases/tag/v0.1.0">0.1.0</a> on each agent.</p>
</li>
<li>
<p>Install the <a href="https://github.com/Docker/Docker/blob/master/docs/extend/plugins.md#volume-plugins">Docker volume
plugin</a>
on each agent.</p>
</li>
<li>
<p>Explicitly create the Docker volumes that are going to be accessed by Mesos
tasks. If this is not done, volumes will be implicitly created by
<a href="https://github.com/emccode/dvdcli">dvdcli</a> but the volumes may not fit into
framework resource requirement well.</p>
</li>
</ul>
<h3 id="a-nameconfigure-docker-volume-isolatoraconfiguring-docker-volume-isolator"><a class="header" href="#a-nameconfigure-docker-volume-isolatoraconfiguring-docker-volume-isolator"><a name="configure-Docker-volume-isolator"></a>Configuring Docker Volume Isolator</a></h3>
<p>In order to configure the <code>docker/volume</code> isolator, the operator needs to
configure two flags at agent startup as follows:</p>
<pre><code class="language-{.console}">  sudo mesos-agent \
    --master=&lt;master IP&gt; \
    --ip=&lt;agent IP&gt; \
    --work_dir=/var/lib/mesos \
    --isolation=filesystem/linux,docker/volume \
    --docker_volume_checkpoint_dir=&lt;mount info checkpoint path&gt;
</code></pre>
<p>The <code>docker/volume</code> isolator must be specified in the <code>--isolation</code> flag at
agent startup; the <code>docker/volume</code> isolator has a dependency on the
<code>filesystem/linux</code> isolator.</p>
<p>The <code>--docker_volume_checkpoint_dir</code> is an optional flag with a default value of
<code>/var/run/mesos/isolators/docker/volume</code>. The <code>docker/volume</code> isolator will
checkpoint all Docker volume mount point information under
<code>--docker_volume_checkpoint_dir</code> for recovery. The checkpoint information under
the default <code>--docker_volume_checkpoint_dir</code> will be cleaned up after agent
restart. Therefore, it is recommended to set <code>--docker_volume_checkpoint_dir</code> to
a directory which will survive agent restart.</p>
<h3 id="a-nameenable-frameworksaenabling-frameworks-to-use-docker-volumes"><a class="header" href="#a-nameenable-frameworksaenabling-frameworks-to-use-docker-volumes"><a name="enable-frameworks"></a>Enabling frameworks to use Docker volumes</a></h3>
<h4 id="a-namevolume-protobufavolume-protobuf"><a class="header" href="#a-namevolume-protobufavolume-protobuf"><a name="volume-protobuf"></a>Volume Protobuf</a></h4>
<p>The <code>Volume</code> protobuf message has been updated to support Docker volumes.</p>
<pre><code class="language-{.proto}">message Volume {
  ...

  required string container_path = 1;

  message Source {
    enum Type {
      UNKNOWN = 0;
      DOCKER_VOLUME = 1;
    }

    message DockerVolume {
      optional string driver = 1;
      required string name = 2;
      optional Parameters driver_options = 3;
    }

    optional Type type = 1;
    optional DockerVolume docker_volume = 2;
  }

  optional Source source = 5;
}
</code></pre>
<p>When requesting a Docker volume for a container, the framework developer needs to
set <code>Volume</code> for the container, which includes <code>mode</code>, <code>container_path</code> and
<code>source</code>.</p>
<p>The <code>source</code> field specifies where the volume comes from. Framework developers need to
specify the <code>type</code>, Docker volume <code>driver</code>, <code>name</code> and <code>options</code>. At present,
only the <code>DOCKER_VOLUME</code> type is supported; we plan to add support for more
types of volumes in the future.</p>
<p>How to specify <code>container_path</code>:</p>
<ol>
<li>
<p>If you are launching a Mesos container <code>without rootfs</code>. If <code>container_path</code>
is an absolute path, you need to make sure the absolute path exists on your
host root file system as the container shares the host root file system;
otherwise, the task will fail.</p>
</li>
<li>
<p>For other cases like launching a Mesos container <code>without rootfs</code> and
<code>container_path</code> is a relative path, or launching a task <code>with rootfs</code> and
<code>container_path</code> is an absolute path, or launching a task <code>with rootfs</code> and
<code>container_path</code> as a relative path, the isolator will help create the
<code>container_path</code> as the mount point.</p>
</li>
</ol>
<p>The following table summarizes the above rules for <code>container_path</code>:</p>
<table class="table table-striped">
  <tr>
    <th></th>
    <th>Container with rootfs</th>
    <th>Container without rootfs</th>
  </tr>
  <tr>
    <td>Absolute container_path</td>
    <td>No need to exist</td>
    <td>Must exist</td>
  </tr>
  <tr>
    <td>Relative container_path</td>
    <td>No need to exist</td>
    <td>No need to exist</td>
  </tr>
</table>
<h4 id="a-nameexamplesaexamples"><a class="header" href="#a-nameexamplesaexamples"><a name="examples"></a>Examples</a></h4>
<ol>
<li>
<p>Launch a task with one Docker volume using the default command executor.</p>
<pre><code class="language-{.json}">TaskInfo {
  ...
  &quot;command&quot; : ...,
  &quot;container&quot; : {
    &quot;volumes&quot; : [
      {
        &quot;container_path&quot; : &quot;/mnt/volume&quot;,
        &quot;mode&quot; : &quot;RW&quot;,
        &quot;source&quot; : {
          &quot;type&quot; : &quot;DOCKER_VOLUME&quot;,
          &quot;docker_volume&quot; : {
            &quot;driver&quot; : &quot;rexray&quot;,
            &quot;name&quot; : &quot;myvolume&quot;
          }
        }
      }
    ]
  }
}
</code></pre>
</li>
<li>
<p>Launch a task with two Docker volumes using the default command executor.</p>
<pre><code class="language-{.json}">TaskInfo {
  ...
  &quot;command&quot; : ...,
  &quot;container&quot; : {
    &quot;volumes&quot; : [
      {
        &quot;container_path&quot; : &quot;volume1&quot;,
        &quot;mode&quot; : &quot;RW&quot;,
        &quot;source&quot; : {
          &quot;type&quot; : &quot;DOCKER_VOLUME&quot;,
          &quot;docker_volume&quot; : {
            &quot;driver&quot; : &quot;rexray&quot;,
            &quot;name&quot; : &quot;volume1&quot;
          }
        }
      },
      {
        &quot;container_path&quot; : &quot;volume2&quot;,
        &quot;mode&quot; : &quot;RW&quot;,
        &quot;source&quot; : {
          &quot;type&quot; : &quot;DOCKER_VOLUME&quot;,
          &quot;docker_volume&quot; : {
            &quot;driver&quot; : &quot;rexray&quot;,
            &quot;name&quot; : &quot;volume2&quot;,
            &quot;driver_options&quot; : {
              &quot;parameter&quot; : [{
                &quot;key&quot; : &lt;key&gt;,
                &quot;value&quot; : &lt;value&gt;
              }, {
                &quot;key&quot; : &lt;key&gt;,
                &quot;value&quot; : &lt;value&gt;
              }]
            }
          }
        }
      }
    ]
  }
}
</code></pre>
</li>
</ol>
<p><strong>NOTE</strong>: The task launch will be failed if one container uses multiple Docker
volumes with the same <code>driver</code> and <code>name</code>.</p>
<h2 id="a-namelimitationsalimitations"><a class="header" href="#a-namelimitationsalimitations"><a name="limitations"></a>Limitations</a></h2>
<p>Using the same Docker volume in both the <a href="isolators/../docker-containerizer.html">Docker
Containerizer</a> and the <a href="isolators/../mesos-containerizer.html">Mesos
Containerizer</a> simultaneously is <strong>strongly
discouraged</strong>, because the MesosContainerizer has its own reference
counting to decide when to unmount a Docker volume. Otherwise, it
would be problematic if a Docker volume is unmounted by
MesosContainerizer but the DockerContainerizer is still using it.</p>
<h2 id="a-nametest-it-outatest-it-out"><a class="header" href="#a-nametest-it-outatest-it-out"><a name="test-it-out"></a>Test it out!</a></h2>
<p>This section presents examples for launching containers with Docker volumes.
The following example is using <a href="https://github.com/rancher/convoy/">convoy</a>
as the Docker volume driver.</p>
<p>Start the Mesos master.</p>
<pre><code class="language-{.console}">  $ sudo mesos-master --work_dir=/tmp/mesos/master
</code></pre>
<p>Start the Mesos agent.</p>
<pre><code class="language-{.console}">  $ sudo mesos-agent \
    --master=&lt;MASTER_IP&gt;:5050 \
    --isolation=docker/volume,docker/runtime,filesystem/linux \
    --work_dir=/tmp/mesos/agent \
    --image_providers=docker \
    --executor_environment_variables=&quot;{}&quot;
</code></pre>
<p>Create a volume named as <code>myvolume</code> with
<a href="https://github.com/rancher/convoy/">convoy</a>.</p>
<pre><code class="language-{.console}">  $ convoy create myvolume
</code></pre>
<p>Prepare a volume json file named as <code>myvolume.json</code> with following content.</p>
<pre><code>  [{
    &quot;container_path&quot;:&quot;\/tmp\/myvolume&quot;,
    &quot;mode&quot;:&quot;RW&quot;,
    &quot;source&quot;:
    {
      &quot;docker_volume&quot;:
        {
          &quot;driver&quot;:&quot;convoy&quot;,
          &quot;name&quot;:&quot;myvolume&quot;
        },
        &quot;type&quot;:&quot;DOCKER_VOLUME&quot;
    }
  }]
</code></pre>
<p>Now, use Mesos CLI (i.e., mesos-execute) to launch a Docker container with
<code>--volumes=&lt;path&gt;/myvolume.json</code> option.</p>
<pre><code class="language-{.console}">  $ sudo mesos-execute \
    --master=&lt;MASTER_IP&gt;:5050 \
    --name=test \
    --docker_image=ubuntu:14.04 \
    --command=&quot;touch /tmp/myvolume/myfile&quot; \
    --volumes=&lt;path&gt;/myvolume.json
</code></pre>
<p>Create another task to verify the file <code>myfile</code> was created successfully.</p>
<pre><code class="language-{.console}">  $ sudo mesos-execute \
    --master=&lt;MASTER_IP&gt;:5050 \
    --name=test \
    --docker_image=ubuntu:14.04 \
    --command=&quot;ls /tmp/myvolume&quot; \
    --volumes=&lt;path&gt;/myvolume.json
</code></pre>
<p>Check the <a href="isolators/../sandbox.html#where-is-it">sandbox</a>
for the second task to check the file <code>myfile</code> was created successfully.</p>
<pre><code class="language-{.console}">  $ cat stdout
    Received SUBSCRIBED event
    Subscribed executor on mesos002
    Received LAUNCH event
    Starting task test
    Forked command at 27288
    sh -c 'ls /tmp/myvolume/'
    lost+found
    myfile
    Command exited with status 0 (pid: 27288)
</code></pre>
<hr />
<h2>title: Apache Mesos - Nvidia GPU Support
layout: documentation</h2>
<h1 id="nvidia-gpu-support"><a class="header" href="#nvidia-gpu-support">Nvidia GPU Support</a></h1>
<p>Mesos 1.0.0 added first-class support for Nvidia GPUs.
The minimum required Nvidia driver version is <code>340.29</code>.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Getting up and running with GPU support in Mesos is fairly
straightforward once you know the steps necessary to make it work as
expected. On one side, this includes setting the necessary agent flags
to enumerate GPUs and advertise them to the Mesos master. On the other
side, this includes setting the proper framework capabilities so that
the Mesos master will actually include GPUs in the resource offers it
sends to a framework. So long as all of these constraints are met,
accepting offers that contain GPUs and launching tasks that consume
them should be just as straightforward as launching a traditional task
that only consumes CPUs, memory, and disk.</p>
<p>Mesos exposes GPUs as a simple <code>SCALAR</code> resource in the same
way it always has for CPUs, memory, and disk. That is, a resource
offer such as the following is now possible:</p>
<pre><code>cpus:8; mem:1024; disk:65536; gpus:4;
</code></pre>
<p>However, unlike CPUs, memory, and disk, <em>only</em> whole numbers of GPUs
can be selected. If a fractional amount is selected, launching the
task will result in a <code>TASK_ERROR</code>.</p>
<p>At the time of this writing, Nvidia GPU support is only available for
tasks launched through the Mesos containerizer (i.e., no support exists
for launching GPU capable tasks through the Docker containerizer).
That said, the Mesos containerizer now supports running docker
images natively, so this limitation should not affect most users.</p>
<p>Moreover, we mimic the support provided by <a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-driver">nvidia-docker</a> to
automatically mount the proper Nvidia drivers and tools directly into
your docker container. This means you can easily test your GPU-enabled
docker containers locally and deploy them to Mesos with the assurance
that they will work without modification.</p>
<p>In the following sections we walk through all of the flags and
framework capabilities necessary to enable Nvidia GPU support in
Mesos. We then show an example of setting up and running an example
test cluster that launches tasks both with and without docker
containers. Finally, we conclude with a step-by-step guide of how to
install any necessary Nvidia GPU drivers on your machine.</p>
<h2 id="agent-flags-1"><a class="header" href="#agent-flags-1">Agent Flags</a></h2>
<p>The following isolation flags are required to enable Nvidia GPU
support on an agent.</p>
<pre><code>--isolation=&quot;filesystem/linux,cgroups/devices,gpu/nvidia&quot;
</code></pre>
<p>The <code>filesystem/linux</code> flag tells the agent to use Linux-specific
commands to prepare the root filesystem and volumes (e.g., persistent
volumes) for containers that require them. Specifically, it relies on
Linux mount namespaces to prevent the mounts of a container from being
propagated to the host mount table. In the case of GPUs, we require
this flag to properly mount certain Nvidia binaries (e.g.,
<code>nvidia-smi</code>) and libraries (e.g., <code>libnvidia-ml.so</code>) into a container
when necessary.</p>
<p>The <code>cgroups/devices</code> flag tells the agent to restrict access to a
specific set of devices for each task that it launches (i.e., a subset
of all devices listed in <code>/dev</code>). When used in conjunction with the
<code>gpu/nvidia</code> flag, the <code>cgroups/devices</code> flag allows us to grant /
revoke access to specific GPUs on a per-task basis.</p>
<p>By default, all GPUs on an agent are automatically discovered and sent
to the Mesos master as part of its resource offer. However, it may
sometimes be necessary to restrict access to only a subset of the GPUs
available on an agent. This is useful, for example, if you want to
exclude a specific GPU device because an unwanted Nvidia graphics card
is listed alongside a more powerful set of GPUs. When this is
required, the following additional agent flags can be used to
accomplish this:</p>
<pre><code>--nvidia_gpu_devices=&quot;&lt;list_of_gpu_ids&gt;&quot;

--resources=&quot;gpus:&lt;num_gpus&gt;&quot;
</code></pre>
<p>For the <code>--nvidia_gpu_devices</code> flag, you need to provide a comma
separated list of GPUs, as determined by running <code>nvidia-smi</code> on the
host where the agent is to be launched (<a href="gpu-support.html#external-dependencies">see
below</a> for instructions on what external
dependencies must be installed on these hosts to run this command).
Example output from running <code>nvidia-smi</code> on a machine with four GPUs
can be seen below:</p>
<pre><code>+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla M60           Off  | 0000:05:00.0     Off |                    0 |
| N/A   35C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla M60           Off  | 0000:83:00.0     Off |                    0 |
| N/A   38C    P0    40W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla M60           Off  | 0000:84:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |     97%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre>
<p>The GPU <code>id</code> to choose can be seen in the far left of each row. Any
subset of these <code>ids</code> can be listed in the <code>--nvidia_gpu_devices</code>
flag (i.e., all of the following values of this flag are valid):</p>
<pre><code>--nvidia_gpu_devices=&quot;0&quot;
--nvidia_gpu_devices=&quot;0,1&quot;
--nvidia_gpu_devices=&quot;0,1,2&quot;
--nvidia_gpu_devices=&quot;0,1,2,3&quot;
--nvidia_gpu_devices=&quot;0,2,3&quot;
--nvidia_gpu_devices=&quot;3,1&quot;
etc...
</code></pre>
<p>For the <code>--resources=gpus:&lt;num_gpus&gt;</code> flag, the value passed to
<code>&lt;num_gpus&gt;</code> must equal the number of GPUs listed in
<code>--nvidia_gpu_devices</code>. If these numbers do not match, launching the
agent will fail. This can sometimes be a source of confusion, so it
is important to emphasize it here for clarity.</p>
<h2 id="framework-capabilities"><a class="header" href="#framework-capabilities">Framework Capabilities</a></h2>
<p>Once you launch an agent with the flags above, GPU resources will be
advertised to the Mesos master along side all of the traditional
resources such as CPUs, memory, and disk. However, the master will
only forward offers that contain GPUs to frameworks that have
explicitly enabled the <code>GPU_RESOURCES</code> framework capability.</p>
<p>The choice to make frameworks explicitly opt-in to this <code>GPU_RESOURCES</code>
capability was to keep legacy frameworks from accidentally consuming
non-GPU resources on GPU-capable machines (and thus preventing your GPU
jobs from running). It's not that big a deal if all of your nodes have
GPUs, but in a mixed-node environment, it can be a big problem.</p>
<p>An example of setting this capability in a C++-based framework can be
seen below:</p>
<pre><code>FrameworkInfo framework;
framework.add_capabilities()-&gt;set_type(
      FrameworkInfo::Capability::GPU_RESOURCES);

GpuScheduler scheduler;

driver = new MesosSchedulerDriver(
    &amp;scheduler,
    framework,
    127.0.0.1:5050);

driver-&gt;run();
</code></pre>
<h2 id="minimal-gpu-capable-cluster"><a class="header" href="#minimal-gpu-capable-cluster">Minimal GPU Capable Cluster</a></h2>
<p>In this section we walk through two examples of configuring GPU-capable
clusters and running tasks on them. The first example demonstrates the
minimal setup required to run a command that consumes GPUs on a GPU-capable
agent. The second example demonstrates the setup necessary to
launch a docker container that does the same.</p>
<p><strong>Note</strong>: Both of these examples assume you have installed the
external dependencies required for Nvidia GPU support on Mesos. Please
see <a href="gpu-support.html#external-dependencies">below</a> for more information.</p>
<h3 id="minimal-setup-without-support-for-docker-containers"><a class="header" href="#minimal-setup-without-support-for-docker-containers">Minimal Setup Without Support for Docker Containers</a></h3>
<p>The commands below show a minimal example of bringing up a GPU-capable
Mesos cluster on <code>localhost</code> and executing a task on it. The required
agent flags are set as described above, and the <code>mesos-execute</code>
command has been told to enable the <code>GPU_RESOURCES</code> framework
capability so it can receive offers containing GPU resources.</p>
<pre><code>$ mesos-master \
      --ip=127.0.0.1 \
      --work_dir=/var/lib/mesos

$ mesos-agent \
      --master=127.0.0.1:5050 \
      --work_dir=/var/lib/mesos \
      --isolation=&quot;cgroups/devices,gpu/nvidia&quot;

$ mesos-execute \
      --master=127.0.0.1:5050 \
      --name=gpu-test \
      --command=&quot;nvidia-smi&quot; \
      --framework_capabilities=&quot;GPU_RESOURCES&quot; \
      --resources=&quot;gpus:1&quot;
</code></pre>
<p>If all goes well, you should see something like the following in the
<code>stdout</code> out of your task:</p>
<pre><code>+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre>
<h3 id="minimal-setup-with-support-for-docker-containers"><a class="header" href="#minimal-setup-with-support-for-docker-containers">Minimal Setup With Support for Docker Containers</a></h3>
<p>The commands below show a minimal example of bringing up a GPU-capable
Mesos cluster on <code>localhost</code> and running a docker container on it. The
required agent flags are set as described above, and the
<code>mesos-execute</code> command has been told to enable the <code>GPU_RESOURCES</code>
framework capability so it can receive offers containing GPU
resources.  Additionally, the required flags to enable support for
docker containers (as described <a href="container-image.html">here</a>) have been
set up as well.</p>
<pre><code>$ mesos-master \
      --ip=127.0.0.1 \
      --work_dir=/var/lib/mesos

$ mesos-agent \
      --master=127.0.0.1:5050 \
      --work_dir=/var/lib/mesos \
      --image_providers=docker \
      --executor_environment_variables=&quot;{}&quot; \
      --isolation=&quot;docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia&quot;

$ mesos-execute \
      --master=127.0.0.1:5050 \
      --name=gpu-test \
      --docker_image=nvidia/cuda \
      --command=&quot;nvidia-smi&quot; \
      --framework_capabilities=&quot;GPU_RESOURCES&quot; \
      --resources=&quot;gpus:1&quot;
</code></pre>
<p>If all goes well, you should see something like the following in the
<code>stdout</code> out of your task.</p>
<pre><code>+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre>
<p><a name="external-dependencies"></a></p>
<h2 id="external-dependencies"><a class="header" href="#external-dependencies">External Dependencies</a></h2>
<p>Any host running a Mesos agent with Nvidia GPU support <strong>MUST</strong> have a
valid Nvidia kernel driver installed. It is also <em>highly</em> recommended to
install the corresponding user-level libraries and tools available as
part of the Nvidia CUDA toolkit. Many jobs that use Nvidia GPUs rely
on CUDA and not including it will severely limit the type of
GPU-aware jobs you can run on Mesos.</p>
<p><strong>Note:</strong> The minimum supported version of CUDA is <code>6.5</code>.</p>
<h3 id="installing-the-required-tools"><a class="header" href="#installing-the-required-tools">Installing the Required Tools</a></h3>
<p>The Nvidia kernel driver can be downloaded at the link below. Make
sure to choose the proper model of GPU, operating system, and CUDA
toolkit you plan to install on your host:</p>
<pre><code>http://www.nvidia.com/Download/index.aspx
</code></pre>
<p>Unfortunately, most Linux distributions come preinstalled with an open
source video driver called <code>Nouveau</code>. This driver conflicts with the
Nvidia driver we are trying to install. The following guides may prove
useful to help guide you through the process of uninstalling <code>Nouveau</code>
before installing the Nvidia driver on CentOS or Ubuntu.</p>
<pre><code>http://www.dedoimedo.com/computers/centos-7-nvidia.html
http://www.allaboutlinux.eu/remove-nouveau-and-install-nvidia-driver-in-ubuntu-15-04/
</code></pre>
<p>After installing the Nvidia kernel driver, you can follow the
instructions in the link below to install the Nvidia CUDA toolkit:</p>
<pre><code>http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/
</code></pre>
<p>In addition to the steps listed in the link above, it is <em>highly</em>
recommended to add CUDA's <code>lib</code> directory into your <code>ldcache</code> so that
tasks launched by Mesos will know where these libraries exist and link
with them properly.</p>
<pre><code>sudo bash -c &quot;cat &gt; /etc/ld.so.conf.d/cuda-lib64.conf &lt;&lt; EOF
/usr/local/cuda/lib64
EOF&quot;

sudo ldconfig
</code></pre>
<p>If you choose <strong>not</strong> to add CUDAs <code>lib</code> directory to your <code>ldcache</code>,
you <strong>MUST</strong> add it to every task's <code>LD_LIBRARY_PATH</code> that requires
it.</p>
<p><strong>Note:</strong> This is <em>not</em> the recommended method. You have been warned.</p>
<h3 id="verifying-the-installation"><a class="header" href="#verifying-the-installation">Verifying the Installation</a></h3>
<p>Once the kernel driver has been installed, you can make sure
everything is working by trying to run the bundled <code>nvidia-smi</code> tool.</p>
<pre><code>nvidia-smi
</code></pre>
<p>You should see output similar to the following:</p>
<pre><code>Thu Apr 14 11:58:17 2016
+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla M60           Off  | 0000:05:00.0     Off |                    0 |
| N/A   35C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla M60           Off  | 0000:83:00.0     Off |                    0 |
| N/A   38C    P0    38W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla M60           Off  | 0000:84:00.0     Off |                    0 |
| N/A   34C    P0    38W / 150W |     34MiB /  7679MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>To verify your CUDA installation, it is recommended to go through the instructions at the link below:</p>
<pre><code>http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#install-samples
</code></pre>
<p>Finally, you should get a developer to run Mesos's Nvidia GPU-related
unit tests on your machine to ensure that everything passes (as
described below).</p>
<h3 id="running-mesos-unit-tests"><a class="header" href="#running-mesos-unit-tests">Running Mesos Unit Tests</a></h3>
<p>At the time of this writing, the following Nvidia GPU specific unit
tests exist on Mesos:</p>
<pre><code>DockerTest.ROOT_DOCKER_NVIDIA_GPU_DeviceAllow
DockerTest.ROOT_DOCKER_NVIDIA_GPU_InspectDevices
NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_VerifyDeviceAccess
NvidiaGpuTest.ROOT_INTERNET_CURL_CGROUPS_NVIDIA_GPU_NvidiaDockerImage
NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_FractionalResources
NvidiaGpuTest.NVIDIA_GPU_Discovery
NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_FlagValidation
NvidiaGpuTest.NVIDIA_GPU_Allocator
NvidiaGpuTest.ROOT_NVIDIA_GPU_VolumeCreation
NvidiaGpuTest.ROOT_NVIDIA_GPU_VolumeShouldInject)
</code></pre>
<p>The capitalized words following the <code>'.'</code> specify test filters to
apply when running the unit tests. In our case the filters that apply
are <code>ROOT</code>, <code>CGROUPS</code>, and <code>NVIDIA_GPU</code>. This means that these tests
must be run as <code>root</code> on Linux machines with <code>cgroups</code> support that
have Nvidia GPUs installed on them. The check to verify that Nvidia
GPUs exist is to look for the existence of the Nvidia System
Management Interface (<code>nvidia-smi</code>) on the machine where the tests are
being run. This binary should already be installed if the instructions
above have been followed correctly.</p>
<p>So long as these filters are satisfied, you can run the following to
execute these unit tests:</p>
<pre><code>[mesos]$ GTEST_FILTER=&quot;&quot; make -j check
[mesos]$ sudo bin/mesos-tests.sh --gtest_filter=&quot;*NVIDIA_GPU*&quot;
</code></pre>
<hr />
<h2>title: Apache Mesos - Sandbox
layout: documentation</h2>
<h1 id="mesos-sandbox"><a class="header" href="#mesos-sandbox">Mesos &quot;Sandbox&quot;</a></h1>
<p>Mesos refers to the &quot;sandbox&quot; as a temporary directory that holds files specific
to a single executor.  Each time an executor is run, the executor is given its
own sandbox and the executor's working directory is set to the sandbox.</p>
<h2 id="sandbox-files"><a class="header" href="#sandbox-files">Sandbox files</a></h2>
<p>The sandbox holds:</p>
<ul>
<li>Files <a href="fetcher.html">fetched by Mesos</a>, prior to starting
the executor's tasks.</li>
<li>The output of the executor and tasks (as files &quot;stdout&quot; and &quot;stderr&quot;).</li>
<li>Files created by the executor and tasks, with some exceptions.</li>
</ul>
<p><strong>NOTE:</strong> With the introduction of <a href="persistent-volume.html">persistent volumes</a>,
executors and tasks should never create files outside of the sandbox.  However,
some containerizers do not enforce this sandboxing.</p>
<h2 id="a-namewhere-is-itawhere-is-the-sandbox"><a class="header" href="#a-namewhere-is-itawhere-is-the-sandbox"><a name="where-is-it"></a>Where is the sandbox?</a></h2>
<p>The sandbox is located within the agent's working directory (which is specified
via the <code>--work_dir</code> flag).  To find a particular executor's sandbox, you must
know the agent's ID, the executor's framework's ID, and the executor's ID.
Each run of the executor will have a corresponding sandbox, denoted by a
container ID.</p>
<p>The sandbox is located on the agent, inside a directory tree like the following:</p>
<pre><code>root ('--work_dir')
|-- slaves
|   |-- latest (symlink)
|   |-- &lt;agent ID&gt;
|       |-- frameworks
|           |-- &lt;framework ID&gt;
|               |-- executors
|                   |-- &lt;executor ID&gt;
|                       |-- runs
|                           |-- latest (symlink)
|                           |-- &lt;container ID&gt; (Sandbox!)
</code></pre>
<h2 id="using-the-sandbox"><a class="header" href="#using-the-sandbox">Using the sandbox</a></h2>
<p><strong>NOTE:</strong> For anything other than Mesos, the executor, or the task(s), the
sandbox should be considered a read-only directory.  This is not enforced via
permissions, but the executor/tasks may malfunction if the sandbox is mutated
unexpectedly.</p>
<h3 id="via-a-file-browser"><a class="header" href="#via-a-file-browser">Via a file browser</a></h3>
<p>If you have access to the machine running the agent, you can <a href="sandbox.html#where-is-it">navigate to the
sandbox directory directly</a>.</p>
<h3 id="via-the-mesos-web-ui"><a class="header" href="#via-the-mesos-web-ui">Via the Mesos web UI</a></h3>
<p>Sandboxes can be browsed and downloaded via the Mesos web UI.  Tasks and
executors will be shown with a &quot;Sandbox&quot; link.  Any files that live in the
sandbox will appear in the web UI.</p>
<h3 id="via-the-files-endpoint"><a class="header" href="#via-the-files-endpoint">Via the <code>/files</code> endpoint</a></h3>
<p>Underneath the web UI, the files are fetched from the agent via the <code>/files</code>
endpoint running on the agent.</p>
<table class="table table-striped">
  <thead>
    <tr>
      <th width="30%">
        Endpoint
      </th>
      <th>
        Description
      </th>
    </tr>
  </thead>
<tr>
    <td>
       <code>/files/browse?path=...</code>
    </td>
    <td>
      Returns a JSON list of files and directories contained in the path.
      Each list is a JSON object containing all the fields normally found in
      <code>ls -l</code>.
    </td>
  </tr>
  <tr>
    <td>
       <code>/files/debug</code>
    </td>
    <td>
      Returns a JSON object holding the internal mapping of files managed by
      this endpoint.  This endpoint can be used to quickly fetch the paths
      of all files exposed on the agent.
    </td>
  </tr>
  <tr>
    <td>
       <code>/files/download?path=...</code>
    </td>
    <td>
      Returns the raw contents of the file located at the given path.
      Where the file extension is understood, the <code>Content-Type</code>
      header will be set appropriately.
    </td>
  </tr>
  <tr>
    <td>
       <code>/files/read?path=...</code>
    </td>
    <td>
      Reads a chunk of the file located at the given path and returns a JSON
      object containing the read <code>"data"</code> and the
      <code>"offset"</code> in bytes.
      <blockquote>
        <p>
          <strong>NOTE:</strong> This endpoint is not designed to read
          arbitrary binary files. Binary files may be returned as
          invalid/un-parseable JSON.
          Use <code>/files/download</code> instead.
        </p>
      </blockquote>
      Optional query parameters:
      <ul>
        <li><code>offset</code> - can be used to page through the file.</li>
        <li><code>length</code> - maximum size of the chunk to read.</li>
      </ul>
    </td>
  </tr>
</table>
<h2 id="sandbox-size"><a class="header" href="#sandbox-size">Sandbox size</a></h2>
<p>The maximum size of the sandbox is dependent on the containerization of the
executor and isolators:</p>
<ul>
<li>Mesos containerizer - For backwards compatibility, the Mesos containerizer
does not enforce a container's disk quota by default.  However, if the
<code>--enforce_container_disk_quota</code> flag is enabled on the agent, and
<code>disk/du</code> is specified in the <code>--isolation</code> flag, the executor
will be killed if the sandbox size exceeds the executor's <code>disk</code> resource.</li>
<li>Docker containerizer - As of Docker <code>1.9.1</code>, the Docker containerizer
does not enforce nor support a disk quota.  See the
<a href="https://github.com/docker/docker/issues/3804">Docker issue</a>.</li>
</ul>
<h2 id="sandbox-lifecycle"><a class="header" href="#sandbox-lifecycle">Sandbox lifecycle</a></h2>
<p>Sandbox files are scheduled for garbage collection when:</p>
<ul>
<li>An executor is removed or terminated.</li>
<li>A framework is removed.</li>
<li>An executor is recovered unsuccessfully during agent recovery.</li>
<li>If the <code>--gc_non_executor_container_sandboxes</code> agent flag is enabled,
nested container sandboxes will also be garbage collected when the
container exits.</li>
</ul>
<p><strong>NOTE:</strong> During agent recovery, all of the executor's runs, except for the
latest run, are scheduled for garbage collection as well.</p>
<p>Garbage collection is scheduled based on the <code>--gc_delay</code> agent flag.  By
default, this is one week since the sandbox was last modified.
After the delay, the files are deleted.</p>
<p>Additionally, according to the <code>--disk_watch_interval</code> agent flag, files
scheduled for garbage collection are pruned based on the available disk and
the <code>--gc_disk_headroom</code> agent flag.
See <a href="configuration/agent.html#gc_disk_headroom">the formula here</a>.</p>
<hr />
<h2>title: Apache Mesos - Container Volumes
layout: documentation</h2>
<h1 id="container-volumes"><a class="header" href="#container-volumes">Container Volumes</a></h1>
<p>For each volume a container specifies (i.e., <code>ContainerInfo.volumes</code>),
the following fields must be specified:</p>
<ul>
<li>
<p><code>container_path</code>: Path in the container filesystem at which the
volume will be mounted. If the path is a relative path, it is
relative to the container's sandbox.</p>
</li>
<li>
<p><code>mode</code>: If the volume is read-only or read-write.</p>
</li>
<li>
<p><code>source</code>: Describe where the volume originates from. See more
details in the following section.</p>
</li>
</ul>
<h2 id="volume-source-types"><a class="header" href="#volume-source-types">Volume Source Types</a></h2>
<ul>
<li><a href="container-volume.html#host_path-volume-source">HOST_PATH</a></li>
<li><a href="container-volume.html#sandbox_path-volume-source">SANDBOX_PATH</a></li>
<li><a href="container-volume.html#docker_volume-volume-source">DOCKER_VOLUME</a></li>
<li><a href="container-volume.html#secret-volume-source">SECRET</a></li>
</ul>
<h3 id="host_path-volume-source"><a class="header" href="#host_path-volume-source">HOST_PATH Volume Source</a></h3>
<p>This volume source represents a path on the host filesystem. The path
can either point to a directory or a file (either a regular file or a
device file).</p>
<p>The following example shows a <code>HOST_PATH</code> volume that mounts
<code>/var/lib/mysql</code> on the host filesystem to the same location in the
container.</p>
<pre><code class="language-json">{
  &quot;container_path&quot;: &quot;/var/lib/mysql&quot;,
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;source&quot;: {
    &quot;type&quot;: &quot;HOST_PATH&quot;,
    &quot;host_path&quot;: {
      &quot;path&quot;: &quot;/var/lib/mysql&quot;
    }
  }
}
</code></pre>
<p>The mode and ownership of the volume will be the same as that on the
host filesystem.</p>
<p>If you are using the <a href="mesos-containerizer.html">Mesos Containerizer</a>,
<code>HOST_PATH</code> volumes are handled by the <code>volume/host_path</code> isolator. To
enable this isolator, append <code>volume/host_path</code> to the <code>--isolation</code>
flag when starting the agent. This isolator depends on the
<a href="isolators/filesystems.html#filesystemlinux-isolator"><code>filesystem/linux</code></a>
isolator.</p>
<p><a href="docker-containerizer.html">Docker Containerizer</a> supports <code>HOST_PATH</code>
volume as well.</p>
<h3 id="sandbox_path-volume-source"><a class="header" href="#sandbox_path-volume-source">SANDBOX_PATH Volume Source</a></h3>
<p>There are currently two types of <code>SANDBOX_PATH</code> volume sources:
<a href="container-volume.html#self-type"><code>SELF</code></a> and <a href="container-volume.html#parent-type"><code>PARENT</code></a>.</p>
<p>If you are using <a href="mesos-containerizer.html">Mesos Containerizer</a>,
<code>SANDBOX_PATH</code> volumes are handled by the <code>volume/sandbox_path</code>
isolator.  To enable this isolator, append <code>volume/sandbox_path</code> to
the <code>--isolation</code> flag when starting the agent.</p>
<p>The <a href="docker-containerizer.html">Docker Containerizer</a> only supports
<code>SELF</code> type <code>SANDBOX_PATH</code> volumes currently.</p>
<h4 id="self-type"><a class="header" href="#self-type"><code>SELF</code> Type</a></h4>
<p>This represents a path in the container's own sandbox. The path can
point to either a directory or a file in the sandbox of the container.</p>
<p>The following example shows a <code>SANDBOX_PATH</code> volume from the
container's own sandbox that mount the subdirectory <code>tmp</code> in the
sandbox to <code>/tmp</code> in the container root filesystem. This will be
useful to cap the <code>/tmp</code> usage in the container (if disk isolator is
used and <code>--enforce_container_disk_quota</code> is turned on).</p>
<pre><code class="language-json">{
  &quot;container_path&quot;: &quot;/tmp&quot;,
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;source&quot;: {
    &quot;type&quot;: &quot;SANDBOX_PATH&quot;,
    &quot;sandbox_path&quot;: {
      &quot;type&quot;: &quot;SELF&quot;,
      &quot;path&quot;: &quot;tmp&quot;
    }
  }
}
</code></pre>
<p>The ownership of the volume will be the same as that of the sandbox of
the container.</p>
<p>Note that <code>container_path</code> has to be an absolute path in this case. If
<code>container_path</code> is relative, that means it's a volume from a
subdirectory in the container sandbox to another subdirectory in the
container sandbox. In that case, the user can just create a symlink,
instead of using a volume.</p>
<h4 id="parent-type"><a class="header" href="#parent-type">PARENT Type</a></h4>
<p>This represents a path in the sandbox of the parent container. The
path can point to either a directory or a file in the sandbox of the
parent container. See the <a href="nested-container-and-task-group.html">nested container
doc</a> for more details about what a
parent container is.</p>
<p>The following example shows a <code>SANDBOX_PATH</code> volume from the sandbox
of the parent container that mounts the subdirectory <code>shared_volume</code> in
the sandbox of the parent container to subdirectory <code>volume</code> in the
sandbox of the container.</p>
<pre><code class="language-json">{
  &quot;container_path&quot;: &quot;volume&quot;,
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;source&quot;: {
    &quot;type&quot;: &quot;SANDBOX_PATH&quot;,
    &quot;sandbox_path&quot;: {
      &quot;type&quot;: &quot;PARENT&quot;,
      &quot;path&quot;: &quot;shared_volume&quot;
    }
  }
}
</code></pre>
<p>The ownership of the volume will be the same as that of the sandbox of
the parent container.</p>
<h3 id="docker_volume-volume-source"><a class="header" href="#docker_volume-volume-source">DOCKER_VOLUME Volume Source</a></h3>
<p>See more details in this <a href="isolators/docker-volume.html">doc</a>.</p>
<h3 id="secret-volume-source"><a class="header" href="#secret-volume-source">SECRET Volume Source</a></h3>
<p>See more details in this <a href="secrets.html#file-based-secrets">doc</a>.</p>
<hr />
<h2>title: Apache Mesos - Mesos Nested Container and Task Group
layout: documentation</h2>
<h1 id="overview-5"><a class="header" href="#overview-5">Overview</a></h1>
<h2 id="motivation-2"><a class="header" href="#motivation-2">Motivation</a></h2>
<p>A <a href="http://kubernetes.io/docs/user-guide/pods">pod</a> can be defined as
a set of containers co-located and co-managed on an agent that share
some resources (e.g., network namespace, volumes) but not others
(e.g., container image, resource limits). Here are the use cases for
pod:</p>
<ul>
<li>Run a side-car container (e.g., logger, backup) next to the main
application controller.</li>
<li>Run an adapter container (e.g., metrics endpoint, queue consumer)
next to the main container.</li>
<li>Run transient tasks inside a pod for operations which are
short-lived and whose exit does not imply that a pod should
exit (e.g., a task which backs up data in a persistent volume).</li>
<li>Provide performance isolation between latency-critical application
and supporting processes.</li>
<li>Run a group of containers sharing volumes and network namespace
while some of them can have their own mount namespace.</li>
<li>Run a group of containers with the same life cycle, e.g, one
container's failure would cause all other containers being
cleaned up.</li>
</ul>
<p>In order to have first class support for running &quot;pods&quot;, two new
primitives are introduced in Mesos: <code>Task Group</code> and <code>Nested Container</code>.</p>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>Mesos has the concept of Executors and Tasks. An executor can launch
multiple tasks while the executor runs in a container. An agent can
run multiple executors. The pod can be implemented by leveraging the
executor and task abstractions. More specifically, the executor runs
in the top level container (called executor container) and its tasks
run in separate nested containers inside this top level container,
while the container image can be specified for each container.</p>
<h2 id="task-groups"><a class="header" href="#task-groups">Task Groups</a></h2>
<p>The concept of a &quot;task group&quot; addresses a previous limitation of the scheduler
and executor APIs, which could not send a group of tasks to an executor
atomically. Even though a scheduler can launch multiple tasks for the same
executor in a LAUNCH operation, these tasks are delivered to the executor one at
a time via separate LAUNCH events. It cannot guarantee atomicity since any
individual task might be dropped due to different reasons (e.g., network
partition). Therefore, the task group provides all-or-nothing semantics to
ensure a group of tasks are delivered atomically to an executor.</p>
<h2 id="nested-containers"><a class="header" href="#nested-containers">Nested Containers</a></h2>
<p>The concept of a &quot;nested container&quot; describes containers nested under an
executor container. In the typical case of a Linux agent, they share a network
namespace and volumes so that they can communicate using the network and access
the same data, though they may have their own container images and resource
limits. On Linux, they may share cgroups or have their own - see the section
below on resource limits for more information.</p>
<p>With the agent nested container API, executors can utilize the
containerizer in the agent to launch nested containers. Both authorized
operators or executors will be allowed to create nested containers. The Mesos
default executor makes use of this API when launching tasks, and custom
executors may consume it as well.</p>
<h2 id="resource-requests-and-limits"><a class="header" href="#resource-requests-and-limits">Resource Requests and Limits</a></h2>
<p>In each task, the resources required by that task can be specified. Common
resource types are <code>cpus</code>, <code>mem</code>, and <code>disk</code>. The resources listed in the
<code>resources</code> field are known as resource &quot;requests&quot; and represent the minimum
resource guarantee required by the task; these resources are used to set the
cgroups of the nested container associated with the task and will always be
available to the task process if they are needed. The quantities specified in
the <code>limits</code> field are the resource &quot;limits&quot;, which represent the maximum amount
of <code>cpus</code> and/or <code>mem</code> that the task may use. Setting a CPU or memory limit
higher than the corresponding request allows the task to consume more than its
allocated amount of CPU or memory when there are unused resources available on
the agent.</p>
<p>When multiple nested containers run under a single executor, the enforcement
of resource constraints depends on the value of the
<code>container.linux_info.share_cgroups</code> field. When this boolean field is <code>true</code>
(this is the default), each container is constrained by the cgroups of its
parent container. This means that if multiple tasks run underneath one executor,
their resource constraints will be enforced as a sum of all the task resource
constraints, applied collectively to those task processes. In this case, nested
container resource consumption is collectively managed via one set of cgroup
subsystem control files associated with the parent executor container.</p>
<p>When the <code>share_cgroups</code> field is set to <code>false</code>, the resource consumption of
each task is managed via a unique set of cgroups associated with that task's
nested container, which means that each task process is subject to its own
resource requests and limits. Note that if you want to specify <code>limits</code> on a
task, the task's container MUST set <code>share_cgroups</code> to <code>false</code>. Also note that
all nested containers under a single executor container must share the same
value of <code>share_cgroups</code>.</p>
<p>Note that when a task sets a memory limit higher than its memory request, the
Mesos agent will change the OOM score adjustment of the task process using a
heuristic based on the task's memory request and the agent's memory capacity.
This means that if the agent's memory becomes exhausted and processes must be
OOM-killed to reclaim memory at a time when the task is consuming more than its
memory request, the task process will be killed preferentially.</p>
<h1 id="task-group-api"><a class="header" href="#task-group-api">Task Group API</a></h1>
<h2 id="framework-api-1"><a class="header" href="#framework-api-1">Framework API</a></h2>
<pre><code>message TaskGroupInfo {
  repeated TaskInfo tasks = 1;
}

message Offer {
  ...

  message Operation {
    enum Type {
      ...
      LAUNCH_GROUP = 6;
      ...
    }
    ...

    message LaunchGroup {
      required ExecutorInfo executor = 1;
      required TaskGroupInfo task_group = 2;
    }
    ...

    optional LaunchGroup launch_group = 7;
  }
}
</code></pre>
<p>By using the TaskGroup Framework API, frameworks can launch a task
group with the <a href="app-framework-development-guide.html">default executor</a>
or a custom executor. The group of tasks can be specified through an
offer operation <code>LaunchGroup</code> when accepting an offer. The
<code>ExecutorInfo</code> indicates the executor to launch the task group, while
the <code>TaskGroupInfo</code> includes the group of tasks to be launched
atomically.</p>
<p>To use the default executor for launching the task group, the framework should:</p>
<ul>
<li>Set <code>ExecutorInfo.type</code> as <code>DEFAULT</code>.</li>
<li>Set <code>ExecutorInfo.resources</code> for the resources needed for the executor.</li>
</ul>
<p>Please note that the following fields in the <code>ExecutorInfo</code> are not allowed to set when using the default executor:</p>
<ul>
<li><code>ExecutorInfo.command</code>.</li>
<li><code>ExecutorInfo.container.type</code>, <code>ExecutorInfo.container.docker</code> and <code>ExecutorInfo.container.mesos</code>.</li>
</ul>
<p>To allow containers to share a network namespace:</p>
<ul>
<li>Set <code>ExecutorInfo.container.network</code>.</li>
</ul>
<p>To allow containers to share an ephemeral volume:</p>
<ul>
<li>Specify the <code>volume/sandbox_path</code> isolator.</li>
<li>Set <code>TaskGroupInfo.tasks.container.volumes.source.type</code> as <code>SANDBOX_PATH</code>.</li>
<li>Set <code>TaskGroupInfo.tasks.container.volumes.source.sandbox_path.type</code> as <code>PARENT</code> and the path relative to the parent container's sandbox.</li>
</ul>
<h2 id="executor-api"><a class="header" href="#executor-api">Executor API</a></h2>
<pre><code>message Event {
  enum Type {
    ...
    LAUNCH_GROUP = 8;
    ...
  }
  ...

  message LaunchGroup {
    required TaskGroupInfo task_group = 1;
  }
  ...

  optional LaunchGroup launch_group = 8;
}
</code></pre>
<p>A new event <code>LAUNCH_GROUP</code> is added to Executor API. Similar to the
Framework API, the <code>LAUNCH_GROUP</code> event guarantees a group of tasks
are delivered to the executor atomically.</p>
<h1 id="nested-container-api"><a class="header" href="#nested-container-api">Nested Container API</a></h1>
<h2 id="new-agent-api"><a class="header" href="#new-agent-api">New Agent API</a></h2>
<pre><code>package mesos.agent;

message Call {
  enum Type {
    ...
    // Calls for managing nested containers underneath an executor's container.
    NESTED_CONTAINER_LAUNCH = 14;  // See 'NestedContainerLaunch' below.
    NESTED_CONTAINER_WAIT = 15;    // See 'NestedContainerWait' below.
    NESTED_CONTAINER_KILL = 16;    // See 'NestedContainerKill' below.
  }

  // Launches a nested container within an executor's tree of containers.
  message LaunchNestedContainer {
    required ContainerID container_id = 1;
    optional CommandInfo command = 2;
    optional ContainerInfo container = 3;
  }

  // Waits for the nested container to terminate and receives the exit status.
  message WaitNestedContainer {
    required ContainerID container_id = 1;
  }

  // Kills the nested container. Currently only supports SIGKILL.
  message KillNestedContainer {
    required ContainerID container_id = 1;
  }

  optional Type type = 1;
  ...
  optional NestedContainerLaunch nested_container_launch = 6;
  optional NestedContainerWait nested_container_wait = 7;
  optional NestedContainerKill nested_container_kill = 8;
}

message Response {
  enum Type {
    ...
    NESTED_CONTAINER_WAIT = 13;    // See 'NestedContainerWait' below.
  }

  // Returns termination information about the nested container.
  message NestedContainerWait {
    optional int32 exit_status = 1;
  }

  optional Type type = 1;
  ...
  optional NestedContainerWait nested_container_wait = 14;
}
</code></pre>
<p>By adding the new Agent API, any authorized entity, including the
executor itself, its tasks, or the operator can use this API to
launch/wait/kill nested containers. Multi-level nesting is supported
by using this API. Technically, the nested level is up to 32 since
it is limited by the maximum depth of <a href="https://github.com/torvalds/linux/commit/f2302505775fd13ba93f034206f1e2a587017929">pid namespace</a> and
<a href="http://man7.org/linux/man-pages/man7/user_namespaces.7.html">user namespace</a> from the Linux Kernel.</p>
<p>The following is the workflow of how the new Agent API works:</p>
<ol>
<li>
<p>The executor sends a <code>NESTED_CONTAINER_LAUNCH</code> call to the agent.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+                | +-----------------+ |
 |             |     LAUNCH     | |                 | |
 |             | &lt;------------+ | |    Executor     | |
 |             |                | |                 | |
 | Mesos Agent |                | +-----------------+ |
 |             |                |                     |
 |             |                |                     |
 |             |                |                     |
 +-------------+                |                     |
                                +---------------------+
</code></pre>
</li>
<li>
<p>Depending on the <code>LaunchNestedContainer</code> from the executor, the
agent launches a nested container inside of the executor container
by calling <code>containerizer::launch()</code>.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+                | +-----------------+ |
 |             |     LAUNCH     | |                 | |
 |             | &lt;------------+ | |    Executor     | |
 |             |                | |                 | |
 | Mesos Agent |                | +-----------------+ |
 |             |                |                     |
 |             |                | +---------+         |
 |             | +------------&gt; | |Nested   |         |
 +-------------+                | |Container|         |
                                | +---------+         |
                                +---------------------+
</code></pre>
</li>
<li>
<p>The executor sends a <code>NESTED_CONTAINER_WAIT</code> call to the agent.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+                | +-----------------+ |
 |             |      WAIT      | |                 | |
 |             | &lt;------------+ | |    Executor     | |
 |             |                | |                 | |
 | Mesos Agent |                | +-----------------+ |
 |             |                |                     |
 |             |                | +---------+         |
 |             |                | |Nested   |         |
 +-------------+                | |Container|         |
                                | +---------+         |
                                +---------------------+
</code></pre>
</li>
<li>
<p>Depending on the <code>ContainerID</code>, the agent calls <code>containerizer::wait()</code>
to wait for the nested container to terminate or exit. Once the container
terminates or exits, the agent returns the container exit status to the
executor.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+      WAIT      | +-----------------+ |
 |             | &lt;------------+ | |                 | |
 |             |                | |    Executor     | |
 |             | +------------&gt; | |                 | |
 | Mesos Agent |  Exited with   | +-----------------+ |
 |             |  status 0      |                     |
 |             |                | +--XX-XX--+         |
 |             |                | |   XXX   |         |
 +-------------+                | |   XXX   |         |
                                | +--XX-XX--+         |
                                +---------------------+
</code></pre>
</li>
</ol>
<h1 id="future-work-1"><a class="header" href="#future-work-1">Future Work</a></h1>
<ul>
<li>Authentication and authorization on the new Agent API.</li>
<li>Command health checks inside of the container's mount namespace.</li>
<li>Resource isolation for nested containers.</li>
<li>Resource statistics reporting for nested containers.</li>
<li>Multiple task groups.</li>
</ul>
<h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<ul>
<li><a href="http://kubernetes.io/docs/user-guide/pods/">Kubernetes Pods</a></li>
<li><a href="https://github.com/docker/docker/issues/8781">Docker Pods</a></li>
</ul>
<hr />
<h2>title: Apache Mesos - Standalone Containers
layout: documentation</h2>
<h1 id="standalone-containers"><a class="header" href="#standalone-containers">Standalone Containers</a></h1>
<p>Traditionally, launching a container in a Mesos cluster involves
communication between multiple components:</p>
<pre><code>                                                 Container(s)
  +-----------+     +--------+     +-------+     +----------+
  | Framework | &lt;-&gt; | Master | &lt;-&gt; | Agent | &lt;-&gt; | Executor |
  +-----------+     +--------+     +-------+     |  `-&gt;Task |
                         ^                       +----------+
                         |         +-------+     +----------+
                         +------&gt;  | Agent | &lt;-&gt; | Executor |
                         |         +-------+     |  `-&gt;Task |
                        ...                      +----------+
</code></pre>
<p>Mesos 1.5 introduced &quot;Standalone Containers&quot;, which provide an alternate
path for launching containers with a reduced scope and feature set:</p>
<pre><code>                   +-------+    +----------------------+
  Operator API &lt;-&gt; | Agent | -&gt; | Standalone Container |
                   +-------+    +----------------------+
</code></pre>
<p><strong>NOTE:</strong> Agents currently require a connection to a Mesos master in
order to accept any Operator API calls.  This limitation is not necessary
and may be fixed in future.</p>
<p><strong>NOTE:</strong> Standalone containers only apply to the Mesos containerizer.
For standalone docker containers, use docker directly.</p>
<p>As hinted by the diagrams, standalone containers are launched on single
Agents, rather than cluster-wide.  This document describes the major
differences between normal containers and standalone containers; and
provides some examples of how to use the new Operator APIs.</p>
<h2 id="launching-a-standalone-container"><a class="header" href="#launching-a-standalone-container">Launching a Standalone Container</a></h2>
<p>Because standalone containers are launched directly on Mesos Agents,
these containers do not participate in the Mesos Master's offer cycle.
This means standalone containers can be launched regardless of resource
allocation and can potentially overcommit the Mesos Agent, but cannot
use reserved resources.</p>
<p>An Operator API might look like this:</p>
<pre><code>LAUNCH_CONTAINER HTTP Request (JSON):

POST /api/v1  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json

{
  &quot;type&quot;: &quot;LAUNCH_CONTAINER&quot;,
  &quot;launch_container&quot;: {
    &quot;container_id&quot;: {
      &quot;value&quot;: &quot;my-standalone-container-id&quot;
    },
    &quot;command&quot;: {
      &quot;value&quot;: &quot;sleep 100&quot;
    },
    &quot;resources&quot;: [
      {
        &quot;name&quot;: &quot;cpus&quot;,
        &quot;scalar&quot;: { &quot;value&quot;: 2.0 },
        &quot;type&quot;: &quot;SCALAR&quot;
      },
      {
        &quot;name&quot;: &quot;mem&quot;,
        &quot;scalar&quot;: { &quot;value&quot;: 1024.0 },
        &quot;type&quot;: &quot;SCALAR&quot;
      },
      {
        &quot;name&quot;: &quot;disk&quot;,
        &quot;scalar&quot;: { &quot;value&quot;: 1024.0 },
        &quot;type&quot;: &quot;SCALAR&quot;
      }
    ],
    &quot;container&quot;: {
      &quot;type&quot;: &quot;MESOS&quot;,
      &quot;mesos&quot;: {
        &quot;image&quot;: {
          &quot;type&quot;: &quot;DOCKER&quot;,
          &quot;docker&quot;: {
            &quot;name&quot;: &quot;alpine&quot;
          }
        }
      }
    }
  }
}
</code></pre>
<p>The Agent will return:</p>
<ul>
<li>200 OK if the launch succeeds, including fetching any container images
or URIs specified in the launch command.</li>
<li>202 Accepted if the specified ContainerID is already in use by a running
container.</li>
<li>400 Bad Request if the launch fails for any reason.</li>
</ul>
<p><strong>NOTE:</strong> Nested containers share the same Operator API.  To launch a nested
container, the ContainerID needs to have a parent; and no resources may be
specified in the request.</p>
<h2 id="monitoring-a-standalone-container"><a class="header" href="#monitoring-a-standalone-container">Monitoring a Standalone Container</a></h2>
<p>Standalone containers are not managed by a framework, do not use executors,
and therefore do not have status updates.  They are not automatically
relaunched upon completion/failure.</p>
<p>After launching a standalone container, the operator should monitor the
container via the <code>WAIT_CONTAINER</code> call:</p>
<pre><code>WAIT_CONTAINER HTTP Request (JSON):

POST /api/v1  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json
Accept: application/json

{
  &quot;type&quot;: &quot;WAIT_CONTAINER&quot;,
  &quot;wait_container&quot;: {
    &quot;container_id&quot;: {
      &quot;value&quot;: &quot;my-standalone-container-id&quot;
    }
  }
}

WAIT_CONTAINER HTTP Response (JSON):

HTTP/1.1 200 OK

Content-Type: application/json

{
  &quot;type&quot;: &quot;WAIT_CONTAINER&quot;,
  &quot;wait_container&quot;: {
    &quot;exit_status&quot;: 0
  }
}
</code></pre>
<p>This is a blocking HTTP call that only returns after the container has
exited.</p>
<p>If the specified ContainerID does not exist, the call returns a 404.</p>
<h2 id="killing-a-standalone-container"><a class="header" href="#killing-a-standalone-container">Killing a Standalone Container</a></h2>
<p>A standalone container can be signalled (usually to kill it) via this API:</p>
<pre><code>KILL_CONTAINER HTTP Request (JSON):

POST /api/v1  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json

{
  &quot;type&quot;: &quot;KILL_CONTAINER&quot;,
  &quot;kill_container&quot;: {
    &quot;container_id&quot;: {
      &quot;value&quot;: &quot;my-standalone-container-id&quot;
    }
  }
}

KILL_CONTAINER HTTP Response (JSON):

HTTP/1.1 200 OK
</code></pre>
<p>If the specified ContainerID does not exist, the call returns a 404.</p>
<h2 id="cleaning-up-a-standalone-container"><a class="header" href="#cleaning-up-a-standalone-container">Cleaning up a Standalone Container</a></h2>
<p>Unlike other containers, a standalone container's sandbox is not garbage
collected by the Agent after some time (like other sandbox directories).
The Agent is unable to garbage collect these containers because there is
no status update mechanism to report the exit status of the container.</p>
<p>Standalone container sandboxes must be manually cleaned up by the operator and
are located in the agent's work directory under
<code>/containers/&lt;my-standalone-container-id&gt;</code>.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
